
# 开源的AI部署框架nndeploy

nndeploy解决的是AI算法在端侧部署的问题，包括桌面端（Windows、macOS）、移动端（Android、iOS）、边缘计算设备（昇腾NPU、NVIDIA Jetson、瑞芯微）以及服务器端（Linux + RTX系列显卡），让AI算法可以更高效以及更高性能的在上述平台落地。

> 我们不开发推理框架，背后依旧使用的是芯片公司提供的推理框架
> 针对大模型，比如几十B语言模型和AIGC生成模型，nndeploy适合当一个可视化工具给他们使用

## 1 为什么要做nndeploy

我们将从实际案例出发，然后介绍一个AI算法在端侧落地的流程，并分析在AI算法落地时的痛点。

### 1.1 从实际案例出发

在很多应用中，比如视频图像编辑工具、直播、文档编辑工具，里面有很多小模型，比如人脸检测、人体关键点、OCR等等，在AI box（工业质检、智慧城市），那就基本上都是检测、分割等等小模型，这些模型需要在各种各样的环境下落地，图像、视频编辑、直播等这类应用中，本身就包含大量模型，处于延时和价格的原因，他们需要在桌面端和移动端本地部署。对于工业质检、智慧城市等等，他们的环境目前还是已边缘计算设备和单机的服务器(Linux + RTX系列显卡、或者是T4的推理卡)为主

<!-- 图片案例，最好是一张图片可以表达编辑、OCR、AI box的场景 -->

更多场景：桌面端应用、手机侧应用，智能硬件（比如无人机、扫地机器人、门禁）、车载的各类语音算法、工业质检等等

处于延时、价格、隐私的角度考虑，很多传统模型的场景并没有消失，甚至随着大模型让AI的普及，这些场景变得越来越多了

### 1.2 AI落地的流程

那么，AI算法的研发是如何走向用户的呢？这背后涉及三个团队（算法、推理部署、应用），整个 AI 落地流程可以分解为以下几个阶段：

**算法团队**：使用 PyTorch、TensorFlow 等深度学习框架进行算法开发。主要交付成果包括：

- **模型文件**：训练完成的模型文件
- **基于 Python 的部署代码**：从训练代码中提炼出的完整推理部署流程，涵盖数据预处理、模型推理、后处理等环节，**理想情况下应该是可直接演示效果的用户界面**
- **技术文档**：算法说明，包含**算法流程图**、各个阶段的输入输出、配置参数等信息
- **测试资源**：验证数据集和性能评估脚本

**推理部署团队**：拿到算法团队的交付无，负责将算法转化为在生产环境中稳定运行的 AI 服务。主要工作内容包括：

- **模型格式转换与量化**：将训练框架的模型转换为推理优化格式（如 ONNX→TensorRT、PyTorch→MNN、ONNX→OpenVINO 等），并根据部署需求进行模型低精度量化
- **多端部署**：针对不同的部署环境（Linux 服务器、Windows/Mac 桌面端、iOS/Android 移动端、边缘端等）进行专门的适配
- **前后处理工程化实现**：将 Python 原型代码中的数据预处理和结果后处理逻辑，使用 C++ 重新实现
- **性能优化**：针对不同计算平台（NVIDIA GPU、Intel CPU、ARM 处理器、移动端 AI 加速器等）的高性能算子、内存优化、并发优化、图优化等等
- **标准化 SDK**：将底层实现封装成统一、简洁的 API 接口，为应用团队提供开箱即用的算法 SDK

**应用团队**：调用推理部署团队提供的算法 SDK，将 AI 能力集成到具体的产品中，处理用户交互、业务逻辑和产品体验。
- 比如To C的应用，剪辑、文档处理的应用，给算法sdk传入输入数据、拿到算法处理的结果，去做优化，比如视频人特效剪辑，编解码拿到视频帧传给算法，算法处理完成，返回人体关键点，应用层拿到关键去做特效
- 比如AI box，视频推流给算法sdk，处理结果返回给后台，后台在可视化给看板，比如检测上班是否玩手机，摄像头推流数据给算法，算法处理完成后，返回是否玩手机，并截图出该帧作为证据，

### 1.3 AI落地的痛点

一个算法要落地，算法工程师通常的技能以Python+算法，他没法做具体端侧的cpp部署，推理部署需要来解决上述问题，应用工程师不了解算法细节，往往像现在大模型一样，调用api为主，再更多的细节它无法去做。到了一个算法需要端侧部署，上述的每一个人都有痛点。


作为算法工程师，你是否经历过这样的困境：

- **交付折磨**：开发完了算法之后，交付给推理部署团队还有很多工作要干，比从训练代码中梳理出的完整推理部署代码、写交付文档、开会讲清楚这个算法的具体逻辑，对于有些传统的人脸聚类算法，后处理的逻辑异常复杂等，以现在大模型为例哈，每个算法的前后处理都是你要去用cpp手写一个tokenizer和sample

- **展示效果不够酷炫**：辛苦调优出的 SOTA 算法，想要展示这个惊艳的效果，让大家都能体验到算法的强大能力。结果却只能打开命令行窗口，敲着`python main.py`命令启动，大家看着 log 一脸懵逼，这也是为什么Gradio很火的原因，大家都需要更好的展示，现在基本上每个好用的算法都会带一个基于Gradio的webui了，

假如你的算法是需要端侧部署，nndeploy可以通过可视化工作流展示你的算法，你的交付物本来就需要些部署代码，这个的时候，假如你直接把你的预处理、推理、后处理写成python节点，既满足的交付的需求，也满足了可视化的需求。

这个是我当时在做算法部署老照片修复的时候，这个模型有6个模型（每个模型都有前处理、推理、后处理）+ 一个传统算法，当时部署代码算法工程师就从原始的框架梳理出来搞了好几天，这些算法要写清楚逻辑也花了好几天，当时我们就没有一个统一的工具，假如它写部署的时候直接基于nndeploy自定义python节点写，他也不用写文档了，给领导展示也更加好看了（老板不需要再部署工程师完成之后，才能看到效果，否则就只能看算法工程师提供的图片了），我部署起来也更加方便了。

作为推理部署工程师，你面对的从来不是"单一模型推理"，而是：

- **推理框架碎片化**：想让模型在不同设备上跑起来，每个硬件平台都有最优推理框架，你得学好多套接口。NVidia 显卡要用 TensorRT，Intel CPU 要用 OpenVINO，手机上要用 ncnn 或者 MNN，苹果设备要用 CoreML，华为昇腾要用 AscendCL，瑞芯微要用 RKNN...每个框架的用法都不一样，光学这些接口就够你喝一壶的，更别说开发以及后期维护了

- **模型本身的复杂性**：有的模型只要一张图片就行，有的需要好几个输入；有的输出就一个结果，有的输出一大堆；还有的模型输入输出形状是固定的，有的是可变的。当你还想做内存零拷贝优化的时候，这些差异点组合起来简直让人头大，通常只有那些踩过很多坑的老手才知道怎么处理

- **模型部署不仅只有推理**：模型部署不仅仅只有模型推理，还有前处理、后处理，推理框架往往只提供模型推理的功能。通常需要部署工程师基于对原始算法的理解，通过 C++/SIMD/CUDA 开发该算法前后处理

- **性能优化**：高性能算子、内存优化、并行调度、图优化等等优化工作难度很高，即使经验丰富的工程师也需要数周时间才能将模型从"能跑"优化到"跑得快"

- **多模型组合场景复杂**：现代 AI 应用往往需要多个模型协同工作，比如文生图：clip→unet/dit→vae。缺乏统一框架支持的情况下，开发者需要编写大量胶水代码来串联各个模型，代码耦合度高、维护困难，更无法进行有效的并行优化

作为非 AI 领域的程序员，你是否也有这样的困扰：

- **有创意但缺技术**：脑子里有很多 AI 产品创意 —— 想在图片编辑应用里加个"老照片修复"功能，想在短视频工具里做"AI 换脸"，想在电商平台搭个"商品图 AIGC 美化"工具

- **技术门槛太高**：核心的业务逻辑、前后端开发、产品设计你都能搞定，唯独卡在了 AI 功能上。作为非 AI 专业的开发者，你只想快速给产品加上 AI 能力，但是不懂 AI，不会推理部署，这些技术门槛让你的百万级创意想法只能停留在想象阶段

## 2 nndeploy具体是什么（针对每个痛点的解决方案）

### 2.1 一段话描述nndeploy是什么

### 2.2 有哪些特点，这些特点是如何解决那些AI落地的痛点

### 2.3 案例展示

## 3 nndeploy的实现

### 3.1 整体架构

三层

### 3.2 部署

- dag

- 并行

- 多端推理

- 数据容器

- 设备管理

- 算子

### 3.3 推理子模块

- 架构

- IR

- 模型解释

- 计算图

- 运行时

- CANN算子

- 图优化

- 内存优化

- 并行优化

- 开发算子

### 3.4 前端

### 3.5 后端

## 4 快速开始

- 安装

- 启动

- 部署

- 自定义节点

## 5 下一步规划讨论

- AIGC工作流

- 传统模型端侧部署

- AI Box

- 具身智能

- 端侧LLM

- LLM应用工作流

- 转型为纯工具

- 邀请大家来开发新仓库