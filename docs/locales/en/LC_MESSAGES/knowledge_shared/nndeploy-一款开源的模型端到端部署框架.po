# SOME DESCRIPTIVE TITLE.
# Copyright (C) nndeploy
# This file is distributed under the same license as the nndeploy package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2025.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: nndeploy\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2025-05-10 16:10+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: en <LL@li.org>\n"
"Language: en\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Plural-Forms: nplurals=2; plural=(n != 1);\n"
"Generated-By: Babel 2.17.0\n"

#: ../../knowledge_shared/nndeploy-一款开源的模型端到端部署框架.md:2
#: f0cdf0728766484296226cd812482d7c
msgid "nndeploy - 一款模型端到端部署框架"
msgstr "nndeploy - An End-to-End Model Deployment Framework"

#: ../../knowledge_shared/nndeploy-一款开源的模型端到端部署框架.md:4
#: 197a63cf3ff247348f48bfadb4acc282
msgid "1 需求分析"
msgstr "1 Requirements Analysis"

#: ../../knowledge_shared/nndeploy-一款开源的模型端到端部署框架.md:6
#: f1801b1657884ac7aece99d3f7ddef37
msgid "首先是需求分析，也就是为什么要做nndeploy，模型多端部署有什么实际场景，目前模型多端部署以及模型部署有哪些痛点。"
msgstr ""
"First is the requirements analysis, which is why we need to do nndeploy, "
"what are the actual scenarios for multi-end model deployment, and what are "
"the current pain points in model multi-end deployment and model deployment."

#: ../../knowledge_shared/nndeploy-一款开源的模型端到端部署框架.md:8
#: f8e7015a7d86466c84193db7e65292ab
msgid "1.1 多端部署实际案例"
msgstr "1.1 Multi-End Deployment Actual Cases"

#: ../../knowledge_shared/nndeploy-一款开源的模型端到端部署框架.md:10
#: 4d6f7f538cf64927903f58199ceda710
msgid ""
"这是一个AI智能抠图多端部署的实际案例，通过人像分割模型，把蒙娜丽批从图片中抠出来，使用的是国内的某p图软件，该软件有ios、android、网页、电脑（win"
" mac 麒麟）等众多版本，这个例子说明了模型有多端部署的实际的需求。"
msgstr ""
"This is an actual case of AI intelligent matting multi-end deployment, "
"through a portrait segmentation model, extracting the Mona Lisa batch from "
"the image, using a certain domestic image software, which has versions for "
"iOS, Android, web, and computer (Win, Mac, Linux), etc. This case "
"illustrates the actual need for multi-end deployment of models."

#: ../../knowledge_shared/nndeploy-一款开源的模型端到端部署框架.md
#: ../../knowledge_shared/nndeploy-一款开源的模型端到端部署框架.md:12
#: 0a2a3497c8c14748b731dee3635d1dd9 956bebad532d4517b19394e5079e0f7f
msgid "multi_end_deploy_case"
msgstr "multi_end_deploy_case"

#: ../../knowledge_shared/nndeploy-一款开源的模型端到端部署框架.md:15
#: dd3f8683bbf84a1bbc0c2f94e3a2d66d
msgid "1.2 模型多端部署以及模型部署痛点"
msgstr "1.2 Model Multi-End Deployment and Model Deployment Pain Points"

#: ../../knowledge_shared/nndeploy-一款开源的模型端到端部署框架.md:17
#: d6e1d2d32d194c5688f7358480082e56
msgid "1.2.1 推理框架的碎片化"
msgstr "1.2.1 Fragmentation of Inference Frameworks"

#: ../../knowledge_shared/nndeploy-一款开源的模型端到端部署框架.md:19
#: 73c320365d934da2898774c8ef917a9e
msgid ""
"模型多端部署第一个痛点 - "
"推理框架的碎片化。现在业界尚不存在各方面都远超其同类产品的推理框架，不同推理框架在不同平台、硬件下分别具有各自的优势。例如，在NVidia "
"显卡机器推理，TensorRT 是性能最好的推理框架；在x86 CPU 机器推理，OpenVINO "
"是性能最好的推理框架；在苹果生态下，coreml是性能最好的推理框架；在ARM Android 下，有 "
"ncnn、MNN、TFLite、TNN等一系列选择；在瑞芯微下，RKNN是性能最好的推理框架。总结而言：在具体硬件下，通常就采用硬件公司推出的推理框架。"
msgstr ""
"The first pain point of model multi-end deployment - the fragmentation of "
"inference frameworks. Currently, there is no inference framework in the "
"industry that surpasses its counterparts in all aspects. Different inference"
" frameworks have their own advantages on different platforms and hardware. "
"For example, on Nvidia GPU machines, TensorRT is the best performing "
"inference framework; on x86 CPU machines, OpenVINO is the best performing "
"inference framework; in the Apple ecosystem, CoreML is the best performing "
"inference framework; on ARM Android, there are choices like ncnn, MNN, "
"TFLite, TNN, etc.; under Rockchip, RKNN is the best performing inference "
"framework. In summary: on specific hardware, the inference framework usually"
" recommended by the hardware company is adopted."

#: ../../knowledge_shared/nndeploy-一款开源的模型端到端部署框架.md
#: ../../knowledge_shared/nndeploy-一款开源的模型端到端部署框架.md:21
#: 610222a8e1624b6fb917722d8da1843f f23e9b45974c444eb9722c08957dfbf2
msgid "fragmentation_in_inference_frameworks"
msgstr "fragmentation_in_inference_frameworks"

#: ../../knowledge_shared/nndeploy-一款开源的模型端到端部署框架.md:23
#: e6eabf30b5dd48e7ad680d9f7a7d2c13
msgid "1.2.2 多个推理框架的学习成本、开发成本、维护成本"
msgstr ""
"1.2.2 Learning Costs, Development Costs, and Maintenance Costs of Multiple "
"Inference Frameworks"

#: ../../knowledge_shared/nndeploy-一款开源的模型端到端部署框架.md:25
#: 32b66481c72d4c628de6a7c48f01cb8c
msgid ""
"模型多端部署第二个痛点 - 多个推理框架 的 "
"学习成本、开发成本、维护成本。不同的推理框架有不一样的推理接口、超参数配置、Tensor等等，假如一个模型需要多端部署，针对不同推理框架都需要写一套代码，这对模型部署工程师而言，将带来较大学习成本、开发成本、维护成本。"
msgstr ""
"The second pain point of model multi-end deployment - the learning costs, "
"development costs, and maintenance costs of multiple inference frameworks. "
"Different inference frameworks have different inference interfaces, "
"hyperparameter configurations, Tensors, etc. If a model needs multi-end "
"deployment, it requires writing a set of code for each different inference "
"framework, which will bring significant learning costs, development costs, "
"and maintenance costs to the model deployment engineers."

#: ../../knowledge_shared/nndeploy-一款开源的模型端到端部署框架.md
#: ../../knowledge_shared/nndeploy-一款开源的模型端到端部署框架.md:27
#: 0e42066e6346493a8e951364c2eb0bb8 366866d1547e4a5b9f1516f6c8452fdb
msgid "inference_difference"
msgstr "inference_difference"

#: ../../knowledge_shared/nndeploy-一款开源的模型端到端部署框架.md:30
#: 1a3d31f7f1d54d69a16aecdf7ba846e1
msgid "1.2.3 模型的多样性"
msgstr "1.2.3 Diversity of Models"

#: ../../knowledge_shared/nndeploy-一款开源的模型端到端部署框架.md:32
#: d4b56a6e2b114a2abf4b04f686d5c741
msgid ""
"上述两个痛点都是针对模型多端部署的痛点，第三个痛点是模型部署本身的痛点 - "
"模型的多样性。从模型部署的角度出发，可以分为单输入、多输入、单输出、多输出、静态形状输入、动态形状输入、静态形状输出、动态形状输出一系列不同，当上述的差异点与内存零拷贝优化结合的时候（直接操作推理框架内部分配输入输出），通常只有具备丰富模型部署经验的工程师才能快速找到最优解"
msgstr ""
"The first two pain points are about the pain points of model multi-end "
"deployment, the third pain point is about the pain point of model deployment"
" itself - the diversity of models. From the perspective of model deployment,"
" it can be divided into single input, multiple inputs, single output, "
"multiple outputs, static shape input, dynamic shape input, static shape "
"output, dynamic shape output, etc. When combining the above differences with"
" memory zero-copy optimization (directly operating the internal allocation "
"of inputs and outputs of the inference framework), usually only engineers "
"with rich model deployment experience can quickly find the optimal solution."

#: ../../knowledge_shared/nndeploy-一款开源的模型端到端部署框架.md:34
#: dd6788241a9643469018e889887ca3b9
msgid "以下是结合了模型特性、描述、TensorRT手动构图以及实际算法例子的表格："
msgstr ""
"The following is a table combining model characteristics, descriptions, "
"TensorRT manual graph construction, and actual algorithm examples:"

#: ../../knowledge_shared/nndeploy-一款开源的模型端到端部署框架.md:98
#: 8030a90eed3d4b9e86e23757080016fa
msgid "1.2.4 模型高性能的前后处理"
msgstr "1.2.4 Pre- and Post-Processing for Model High Performance"

#: ../../knowledge_shared/nndeploy-一款开源的模型端到端部署框架.md:100
#: b8ae7a3ff7b744d1854f8b7eff58759b
msgid ""
"第四个痛点也是模型部署本身的痛点 - "
"模型的前后处理。模型部署不仅仅只有模型推理，还有前处理、后处理，推理框架往往只提供模型推理的功能。通常需要部署工程师基于对原始算法的理解，通过c++开发该算法前后处理，就cv类算法而言，前处理通常由如下算子（cvtcolor、resize、padding、"
" "
"warp_affine、crop、normalize、transpose）组合而成，对于大部分cv类模型而言，前处理有较多共性，对于某一个类别的算法而言，后处理算法又特别相似，故前后处理可以被复用，当某个前后处理被大量复用时，可以考虑重点优化，从而获得进一步加速"
msgstr ""
"The fourth pain point is also about the pain point of model deployment "
"itself - the pre- and post-processing of models. Model deployment is not "
"just about model inference, but also includes pre-processing and post-"
"processing. Inference frameworks often only provide the functionality of "
"model inference. Usually, deployment engineers need to develop the pre- and "
"post-processing algorithms based on their understanding of the original "
"algorithms through C++. For CV class algorithms, pre-processing is usually "
"composed of operators such as cvtcolor, resize, padding, warp_affine, crop, "
"normalize, transpose, etc. For most CV class models, pre-processing has a "
"lot in common. For a certain category of algorithms, post-processing "
"algorithms are particularly similar, so pre- and post-processing can be "
"reused. When a certain pre- or post-processing is heavily reused, it can be "
"considered for key optimization, thereby achieving further acceleration."

#: ../../knowledge_shared/nndeploy-一款开源的模型端到端部署框架.md
#: ../../knowledge_shared/nndeploy-一款开源的模型端到端部署框架.md:102
#: 2ce3e8c7d9544fb19b592fa5abcd755c 2d48a2e532614acda0f31a0dd72a5252
msgid "model_pre_post"
msgstr "model_pre_post"

#: ../../knowledge_shared/nndeploy-一款开源的模型端到端部署框架.md:105
#: b67729c8d91e45d79fced472d049a204
msgid "1.2.5 多模型的复杂场景"
msgstr "1.2.5 Complex Scenarios of Multiple Models"

#: ../../knowledge_shared/nndeploy-一款开源的模型端到端部署框架.md:107
#: d682bf49e6f1468fa97beaad79a75066
msgid ""
"第五个也是模型部署的痛点 - 多模型组合复杂的场景。目前很多场景是需要由多个模型组合解决该业务问题，例如老照片修复，该算法有6个模型 + "
"1个传统算法（老照片->划痕检测->划痕修复->超分辨率->condition(loop(人脸检测->人脸矫正->人脸修复->人脸贴回))->修复后的照片）组合，没有部署框架的支持，会有大量业务代码、模型耦合度高、灵活性差、代码不适合并行等等问题（出bug、可维护性）。"
msgstr ""
"The fifth pain point is also about model deployment - the complex scenarios "
"of multiple model combinations. Currently, many scenarios require solving "
"business problems through the combination of multiple models. For example, "
"old photo restoration, this algorithm has 6 models + 1 traditional algorithm"
" (old photo->scratch detection->scratch restoration->super-"
"resolution->condition(loop(face detection->face alignment->face "
"restoration->face pasting))->restored photo) combination. Without the "
"support of a deployment framework, there will be a lot of business code, "
"high model coupling, poor flexibility, code not suitable for parallel "
"processing, and other issues (prone to bugs, maintainability)."

#: ../../knowledge_shared/nndeploy-一款开源的模型端到端部署框架.md
#: ../../knowledge_shared/nndeploy-一款开源的模型端到端部署框架.md:109
#: 37256494e39c41feb15b54d33550e4b1 471fc2b7f4e645258129b2d342bdba41
msgid "old_photo"
msgstr "old_photo"

#: ../../knowledge_shared/nndeploy-一款开源的模型端到端部署框架.md:112
#: 8fb5053767cf4f348dec8b189a133555
msgid "2 概述"
msgstr "2 Overview"

#: ../../knowledge_shared/nndeploy-一款开源的模型端到端部署框架.md:114
#: 6041e75740e1460b9b86afd84e30aa64
msgid ""
"nndeploy是一款模型端到端部署框架。下图为nndeploy的整体架构，以多端推理以及基于有向无环图模型部署为基础，致力为用户提供跨平台、简单易用、高性能的模型部署体验。"
msgstr ""
"nndeploy is an end-to-end model deployment framework. The following figure "
"shows the overall architecture of nndeploy, based on multi-end inference and"
" directed acyclic graph model deployment, striving to provide users with a "
"cross-platform, simple to use, high-performance model deployment experience."

#: ../../knowledge_shared/nndeploy-一款开源的模型端到端部署框架.md
#: ../../knowledge_shared/nndeploy-一款开源的模型端到端部署框架.md:116
#: 2500ab2ae9144a3b9d9eb51d9815cf59 c41190ed2cc0459daa882f0e57dec83c
msgid "Architecture"
msgstr "Architecture"

#: ../../knowledge_shared/nndeploy-一款开源的模型端到端部署框架.md:118
#: c1188fa40f944a948d2b1fa3656e0e42
msgid "2.1 特点"
msgstr "2.1 Features"

#: ../../knowledge_shared/nndeploy-一款开源的模型端到端部署框架.md:120
#: becefe76c628470aa9218b446d884a25
msgid "2.1.1 开箱即用的算法"
msgstr "2.1.1 Ready-to-Use Algorithms"

#: ../../knowledge_shared/nndeploy-一款开源的模型端到端部署框架.md:122
#: 230b7a92bbd3479fa17c3e758a733134
msgid "目前已完成 YOLOV5、YOLOV6、YOLOV8 、SAM模型的部署，可供您直接使用，后续我们持续不断去部署其它开源模型，让您开箱即用"
msgstr ""
"Currently, the deployment of YOLOV5, YOLOV6, YOLOV8, and SAM models has been"
" completed, available for your direct use. We will continue to deploy other "
"open-source models, allowing you to use them out of the box."

#: ../../knowledge_shared/nndeploy-一款开源的模型端到端部署框架.md:161
#: 3e6fa21a0f8c4fbfa6b8df5fadc34559
msgid "2.1.2 支持跨平台和多推理框架"
msgstr "2.1.2 Support for Cross-Platform and Multiple Inference Frameworks"

#: ../../knowledge_shared/nndeploy-一款开源的模型端到端部署框架.md:163
#: fe136399f28f498c89e5fa2d2ae21ed2
msgid ""
"一套代码多端部署：通过切换推理配置，一套代码即可完成模型跨多个平台以及多个推理框架部署。主要是针对痛点一（推理框架的碎片化）和痛点二（多个推理框架的学习成本、开发成本、维护成本）"
msgstr ""
"One set of code for multi-end deployment: by switching inference "
"configurations, one set of code can complete the deployment of models across"
" multiple platforms and multiple inference frameworks. Mainly targeting pain"
" point one (fragmentation of inference frameworks) and pain point two "
"(learning costs, development costs, and maintenance costs of multiple "
"inference frameworks)."

#: ../../knowledge_shared/nndeploy-一款开源的模型端到端部署框架.md:165
#: 6421dca64fc84fcdb5327bbcc7792f1b
msgid "当前支持的推理框架如下："
msgstr "Currently supported inference frameworks are as follows:"

#: ../../knowledge_shared/nndeploy-一款开源的模型端到端部署框架.md:285
#: 7e829c49bbca43e294315170575df105
msgid "2.1.3 简单易用"
msgstr "2.1.3 Simple and Easy to Use"

#: ../../knowledge_shared/nndeploy-一款开源的模型端到端部署框架.md:287
#: 6843816037174452b7c5545248d42d86
msgid ""
"基于有向无环图部署模型： 将 AI 算法端到端（前处理->推理->后处理）的部署抽象为有向无环图 Graph，前处理为一个 Node，推理也为一个 "
"Node，后处理也为一个 Node。主要是针对痛点四（复用模型的前后处理）"
msgstr ""
"Based on directed acyclic graph model deployment: abstracting the deployment"
" of AI algorithms end-to-end (pre-processing->inference->post-processing) "
"into a directed acyclic graph Graph, pre-processing as a Node, inference "
"also as a Node, post-processing also as a Node. Mainly targeting pain point "
"four (reusing model pre- and post-processing)."

#: ../../knowledge_shared/nndeploy-一款开源的模型端到端部署框架.md:289
#: 1178d7b01f944d21bb3eaf5a2e4c1976
msgid ""
"推理模板Infer： 基于多端推理模块Inference + "
"有向无环图节点Node再设计功能强大的推理模板Infer，Infer推理模板可以帮您在内部处理不同的模型带来差异，例如单输入、多输入、单输出、多输出、静态形状输入、动态形状输入、静态形状输出、动态形状输出一系列不同。主要是针对痛点三（模型的多样性）"
msgstr ""
"Inference template Infer: Based on the multi-end inference module Inference "
"+ directed acyclic graph node Node redesign, the powerful inference template"
" Infer can help you internally handle different model variations, such as "
"single input, multiple inputs, single output, multiple outputs, static shape"
" input, dynamic shape input, static shape output, dynamic shape output, etc."
" Mainly targeting pain point three (model diversity)."

#: ../../knowledge_shared/nndeploy-一款开源的模型端到端部署框架.md:291
#: 9dc2aa8fd9f540f28e3ffae0e7933ea2
msgid ""
"高效解决多模型的复杂场景：在多模型组合共同完成一个任务的复杂场景下（例如老照片修复），每个模型都可以是独立的Graph，nndeploy的有向无环图支持图中嵌入图灵活且强大的功能，将大问题拆分为小问题，通过组合的方式快速解决多模型的复杂场景问题"
msgstr ""
"Efficiently solving complex scenarios of multiple models: In complex "
"scenarios where multiple models are combined to complete a task (e.g., old "
"photo restoration), each model can be an independent Graph. nndeploy's "
"directed acyclic graph supports embedding graphs flexibly and powerfully, "
"breaking down big problems into small ones, quickly solving complex "
"scenarios of multiple models through combination."

#: ../../knowledge_shared/nndeploy-一款开源的模型端到端部署框架.md:293
#: 83854546546d4b0c8975f9211ebb0812
msgid ""
"快速构建demo：对于已部署好的模型，需要编写demo展示效果，而demo需要处理多种格式的输入，例如图片输入输出、文件夹中多张图片的输入输出、视频的输入输出等，通过将上述编解码节点化，可以更通用以及更高效的完成demo的编写，达到快速展示效果的目的（目前主要实现了基于OpneCV的编解码节点化）"
msgstr ""
"Quickly constructing demo: For already deployed models, writing a demo to "
"showcase effects is necessary, and the demo needs to handle multiple format "
"inputs, such as image input output, folder containing multiple images input "
"output, video input output, etc. By node-izing the above codec, it can be "
"more versatile and efficient to complete the demo writing, achieving the "
"goal of quickly showcasing effects (currently mainly implemented OpenCV-"
"based codec node-ization)."

#: ../../knowledge_shared/nndeploy-一款开源的模型端到端部署框架.md:295
#: 8b97b7c73aa944c388b128c366604781
msgid "2.1.4 高性能"
msgstr "2.1.4 High Performance"

#: ../../knowledge_shared/nndeploy-一款开源的模型端到端部署框架.md:297
#: dbf353e332584641b1dfeb73c2233e62
msgid ""
"推理框架的高性能抽象：每个推理框架也都有其各自的特性，需要足够尊重以及理解这些推理框架，才能在抽象中不丢失推理框架的特性，并做到统一的使用的体验。nndeploy"
" 可配置第三方推理框架绝大部分参数，保证了推理性能。可直接操作推理框架内部分配的输入输出，实现前后处理的零拷贝，提升模型部署端到端的性能。"
msgstr ""
"Abstraction of the inference framework's high performance: Each inference "
"framework has its own characteristics, which need to be respected and "
"understood to ensure the abstraction does not lose the framework's features,"
" providing a unified usage experience. nndeploy can configure most "
"parameters of third-party inference frameworks, ensuring inference "
"performance. It can directly operate the input and output allocated inside "
"the inference framework, achieving zero-copy of pre and post-processing, "
"improving the end-to-end performance of model deployment."

#: ../../knowledge_shared/nndeploy-一款开源的模型端到端部署框架.md:299
#: 58f3a53061624206a6033f9a2ed58ad4
msgid ""
"线程池：提高模型部署的并发性能和资源利用率（thread "
"pool）。此外，还支持CPU端算子自动并行，可提升CPU算子执行性能（parallel_for）。"
msgstr ""
"Thread pool: Improves the concurrency performance and resource utilization "
"rate of model deployment (thread pool). Additionally, it supports automatic "
"parallelization of CPU operators, improving CPU operator execution "
"performance (parallel_for)."

#: ../../knowledge_shared/nndeploy-一款开源的模型端到端部署框架.md:301
#: 99992114cf664a84a697e45f9c088675
msgid "内存池：完成后可实现高效的内存分配与释放(TODO)"
msgstr ""
"Memory pool: After completion, efficient memory allocation and release can "
"be achieved (TODO)."

#: ../../knowledge_shared/nndeploy-一款开源的模型端到端部署框架.md:303
#: 9371dba8413847e3b150fc599aeadcab
msgid "一组高性能的算子：完成后将加速您模型前后处理速度(TODO)"
msgstr ""
"A set of high-performance operators: After completion, will accelerate your "
"model's pre and post-processing speed (TODO)."

#: ../../knowledge_shared/nndeploy-一款开源的模型端到端部署框架.md:305
#: 2e004a44f99b4565a9029b0146d0390e
msgid "2.1.5 并行"
msgstr "2.1.5 Parallel"

#: ../../knowledge_shared/nndeploy-一款开源的模型端到端部署框架.md:307
#: ff588ac484bf43728bf7583990d72baa
msgid "串行：按照模型部署的有向无环图的拓扑排序，依次执行每个节点。"
msgstr ""
"Serial: Executes each node in order according to the topological sorting of "
"the model deployment's directed acyclic graph."

#: ../../knowledge_shared/nndeploy-一款开源的模型端到端部署框架.md:309
#: e20c81cd567a4ed49874d8b325cfdb60
msgid ""
"流水线并行：在处理多帧的场景下，基于有向无环图的模型部署方式，可将前处理 Node、推理 Node、后处理 "
"Node绑定三个不同的线程，每个线程又可绑定不同的硬件设备下，从而三个Node可流水线并行处理。在多模型以及多硬件设备的的复杂场景下，更加可以发挥流水线并行的优势，从而可显著提高整体吞吐量。"
msgstr ""
"Pipeline parallel: In scenarios processing multiple frames, based on the "
"model deployment method of directed acyclic graph, the pre-processing Node, "
"inference Node, and post-processing Node can be bound to three different "
"threads, each thread can also be bound to different hardware devices, "
"enabling the three Nodes to process in pipeline parallel. In complex "
"scenarios of multiple models and multiple hardware devices, the advantage of"
" pipeline parallel can be more utilized, significantly improving overall "
"throughput."

#: ../../knowledge_shared/nndeploy-一款开源的模型端到端部署框架.md:311
#: 0512ae44c6584b16ad0cfcb0c7e11d64
msgid "任务并行：在多模型以及多硬件设备的的复杂场景下，基于有向无环图的模型部署方式，可充分挖掘模型部署中的并行性，缩短单次算法全流程运行耗时"
msgstr ""
"Task parallel: In complex scenarios of multiple models and multiple hardware"
" devices, based on the model deployment method of directed acyclic graph, "
"the parallelism in model deployment can be fully explored, reducing the "
"runtime overhead of a single algorithm's full process."

#: ../../knowledge_shared/nndeploy-一款开源的模型端到端部署框架.md:313
#: db7456f73bc245db943f397582adadda
msgid ""
"上述模式的组合并行：在多模型、多硬件设备以及处理多帧的复杂场景下，nndeploy的有向无环图支持图中嵌入图的功能，每个图都可以有独立的并行模式，故用户可以任意组合模型部署任务的并行模式，具备强大的表达能力且可充分发挥硬件性能。"
msgstr ""
"Combination parallel of the above modes: In complex scenarios of multiple "
"models, multiple hardware devices, and processing multiple frames, "
"nndeploy's directed acyclic graph supports embedding graphs, each graph can "
"have an independent parallel mode, allowing users to arbitrarily combine the"
" parallel modes of model deployment tasks, with powerful expressive ability "
"and can fully utilize hardware performance."

#: ../../knowledge_shared/nndeploy-一款开源的模型端到端部署框架.md:315
#: b93c725290884714aa95a54820145d0e
msgid "3 架构简介"
msgstr "3 Architecture Introduction"

#: ../../knowledge_shared/nndeploy-一款开源的模型端到端部署框架.md:317
#: 911df33b98e946edaed00c38ac49f3b7
msgid ""
"nndeploy是以多端推理以及基于有向无环图模型部署为基础的模型端到端部署框架。故架构简介从多端推理以及基于有向无环图模型部署两个为引子去介绍整体架构。"
msgstr ""
"nndeploy is a model end-to-end deployment framework based on multi-end "
"inference and directed acyclic graph model deployment. The architecture "
"introduction starts with multi-end inference and directed acyclic graph "
"model deployment as the leads to introduce the overall architecture."

#: ../../knowledge_shared/nndeploy-一款开源的模型端到端部署框架.md:319
#: 9d4ec9bdd1a24ef2ba5798581815731c
msgid "与多端推理有关的三个模块"
msgstr "Three modules related to multi-end inference"

#: ../../knowledge_shared/nndeploy-一款开源的模型端到端部署框架.md:321
#: a86427a0b6734a8182ad266722db56a4
msgid "3.1 多端推理"
msgstr "3.1 Multi-end Inference"

#: ../../knowledge_shared/nndeploy-一款开源的模型端到端部署框架.md:323
#: 4bab8b6423884ac5847076ce26be0f8e
msgid ""
"多端推理子模块（Inference）。提供统一的模型推理的方法去操作不同的推理后端。下图梳理nndeploy接入一个新推理框架的整体流程，这里以MNN为例。1."
" 首先是理解MNN；2. 理解Inference子模块（推理超参数配置类InferenceParam，推理基类Inference）；3. "
"在理解MNN与Inference基类之上，编写推理适配器（继承基类Inference，编写MnnInference；继承基类InferenceParam，编写MnnInferenceParam；编写推理相关数据结构的转换工具类MnnConvert）；4."
" 基于MNN后端跑通YOLOV5s"
msgstr ""
"Multi-end inference submodule (Inference). Provides a unified method of "
"model inference to operate different inference backends. The following "
"diagram outlines the overall flow of nndeploy integrating a new inference "
"framework, here taking MNN as an example. 1. First, understand MNN; 2. "
"Understand the Inference submodule (inference super parameter configuration "
"class InferenceParam, inference base class Inference); 3. Based on "
"understanding MNN and the Inference base class, write the inference adapter "
"(inherit the base class Inference, write MnnInference; inherit the base "
"class InferenceParam, write MnnInferenceParam; write the inference-related "
"data structure conversion tool class MnnConvert); 4. Run YOLOV5s based on "
"the MNN backend."

#: ../../knowledge_shared/nndeploy-一款开源的模型端到端部署框架.md
#: ../../knowledge_shared/nndeploy-一款开源的模型端到端部署框架.md:325
#: 0e45e0d0444c410dbfc32c4cad670c43 bdeba812987d421395d7da2ee2d942fc
msgid "how_to_support_new_inference"
msgstr "how_to_support_new_inference"

#: ../../knowledge_shared/nndeploy-一款开源的模型端到端部署框架.md:327
#: 9306ef7dd2cd412c96c970e4a9491d88
msgid "3.1.1 模型推理类Inference"
msgstr "3.1.1 Model Inference Class Inference"

#: ../../knowledge_shared/nndeploy-一款开源的模型端到端部署框架.md:329
#: 67eb204a08dd49a8bf42ab3245ffe91c
msgid ""
"对应文件为<path>\\include\\nndeploy\\inference.h和<path>\\source\\nndeploy\\inference.cc，文件中有较为详细的注释说明，主要功能如图所示"
msgstr ""
"The corresponding files are <path>\\include\\nndeploy\\inference.h and "
"<path>\\source\\nndeploy\\inference.cc, with detailed comments in the files,"
" main functions as shown in the figure."

#: ../../knowledge_shared/nndeploy-一款开源的模型端到端部署框架.md
#: ../../knowledge_shared/nndeploy-一款开源的模型端到端部署框架.md:331
#: 1dd2cc2dea6149fcbd830be56d79307f 512a293f14634d0cbe8154db4e0adb7a
msgid "inference"
msgstr "inference"

#: ../../knowledge_shared/nndeploy-一款开源的模型端到端部署框架.md:333
#: 223462718d4d49f2a67f1550d1b39a82
msgid "3.1.2 InferenceParam 推理超参数配置类"
msgstr "3.1.2 InferenceParam Inference Super Parameter Configuration Class"

#: ../../knowledge_shared/nndeploy-一款开源的模型端到端部署框架.md:335
#: 48b57ffe714d48349014aaf39f3c0e04
msgid ""
"对应文件为<path>\\include\\nndeploy\\inference_param.h和<path>\\source\\nndeploy\\inference_param.cc，每个推理实例都需要超参数配置，例如模型推理时精度、是否为动态形状等等，详细功能如图所示"
msgstr ""
"The corresponding files are <path>\\include\\nndeploy\\inference_param.h and"
" <path>\\source\\nndeploy\\inference_param.cc. Each inference instance "
"requires super parameter configuration, such as model inference precision, "
"whether it is dynamic shape, etc. Detailed functions as shown in the figure."

#: ../../knowledge_shared/nndeploy-一款开源的模型端到端部署框架.md
#: ../../knowledge_shared/nndeploy-一款开源的模型端到端部署框架.md:337
#: 9ce7ddf1f4c740bc841dce67e3f23236 dfa1c6b2c52946d6a1bcfe78d639c6b1
msgid "inference_param"
msgstr "inference_param"

#: ../../knowledge_shared/nndeploy-一款开源的模型端到端部署框架.md:339
#: 5ec3d4614c014e3cb94e157898c362bd
msgid "3.1.3 推理相关数据结构的转换工具类"
msgstr "3.1.3 Inference-related Data Structure Conversion Tool Class"

#: ../../knowledge_shared/nndeploy-一款开源的模型端到端部署框架.md:341
#: 12a75d56fab9420f9fdff781dfa81ba0
msgid ""
"nndeploy提供了统一的Tensor以及推理所需的超参数数据结构，每个推理框架都有自定义Tensor以及超参数数据结构，为了保证统一的接口调用的体验，需编写转化器模块。由于每个推理框架定义都不相同，故该工具类无法定义基类，该工具类的主要也是服务推理框架适配器内部使用，也不需要基类。可参考<path>\\include/nndeploy/inference/mnn/mnn_converter.h和<path>\\source/nndeploy/inference/mnn/mnn_converter.c。具体实现如下图所示"
msgstr ""
"nndeploy provides a unified Tensor and the hyperparameter data structure "
"required for inference. Each inference framework has its own custom Tensor "
"and hyperparameter data structure. To ensure a unified interface calling "
"experience, a converter module needs to be written. Since each inference "
"framework's definitions differ, this tool class cannot define a base class; "
"its main purpose is to serve the internal use of the inference framework "
"adapter, and it does not require a base class. For reference, see "
"<path>\\include/nndeploy/inference/mnn/mnn_converter.h and "
"<path>\\source/nndeploy/inference/mnn/mnn_converter.c. The specific "
"implementation is as shown in the following diagram."

#: ../../knowledge_shared/nndeploy-一款开源的模型端到端部署框架.md
#: ../../knowledge_shared/nndeploy-一款开源的模型端到端部署框架.md:343
#: 4ed3a8e861bd4f1887415337d9d512ea e4b41a37bfc64b02958b5519986b3bd4
msgid "mnn_converter"
msgstr "mnn_converter"

#: ../../knowledge_shared/nndeploy-一款开源的模型端到端部署框架.md:345
#: e624244f67db43e0b6bdb5a043680237
msgid "3.2 数据容器 Tensor && Buffer"
msgstr "3.2 Data Containers Tensor && Buffer"

#: ../../knowledge_shared/nndeploy-一款开源的模型端到端部署框架.md:347
#: 20982a73bbc84fa998900188a37250fe
msgid ""
"每个推理框架都有不一样的数据交互方式，例如TensorRT为io_binding的方式、OpenVINO为ov::Tensor、TNN的TNN::Blob。不仅需要提供统一推理类以及推理超参数配置类，还需要设计一个通用的Tensor，Tensor的成员变量以及TensorDesc的成员变量如图所示。"
msgstr ""
"Each inference framework has different data interaction methods, such as "
"TensorRT's io_binding method, OpenVINO's ov::Tensor, TNN's TNN::Blob. Not "
"only is it necessary to provide a unified inference class and inference "
"hyperparameter configuration class, but also to design a universal Tensor, "
"with the member variables of Tensor and TensorDesc as shown in the diagram."

#: ../../knowledge_shared/nndeploy-一款开源的模型端到端部署框架.md
#: ../../knowledge_shared/nndeploy-一款开源的模型端到端部署框架.md:349
#: 0583addf0d8c4fb8be158fd5c06fbaa8 813c8487ad8c47418442100f277e032f
msgid "tensor"
msgstr "tensor"

#: ../../knowledge_shared/nndeploy-一款开源的模型端到端部署框架.md:351
#: d5d81e9cadcd45b2b0b1499363352724
msgid ""
"模型推理的输入输出可以是异构设备上的数据，例如TensorRT的输入为CUDA内存。引入Buffer，将Tensor与异构设备解绑。Buffer的成员变量以及BufferDesc的成员变量如图所示。"
msgstr ""
"The input and output of model inference can be data on heterogeneous "
"devices, such as TensorRT's input being CUDA memory. Introducing Buffer, "
"Tensor is decoupled from heterogeneous devices. The member variables of "
"Buffer and BufferDesc are as shown in the diagram."

#: ../../knowledge_shared/nndeploy-一款开源的模型端到端部署框架.md
#: ../../knowledge_shared/nndeploy-一款开源的模型端到端部署框架.md:353
#: 6b708633738b4613bf290a8db2c1daf6 b2c43b35be1840fba62d011e32775a76
msgid "buffer"
msgstr "buffer"

#: ../../knowledge_shared/nndeploy-一款开源的模型端到端部署框架.md:355
#: 9b4714400edc46f893fdc93202f1079e
msgid "3.3 设备管理"
msgstr "3.3 Device Management"

#: ../../knowledge_shared/nndeploy-一款开源的模型端到端部署框架.md:357
#: 6edfb0d100d548dabdeeedaffb948e37
msgid ""
"设备是nndeploy对硬件设备的抽象，通过对硬件设备的抽象，从而屏蔽不同硬件设备编程模型带来的差异性，nndeploy当前已经支持CPU、X86、ARM、CUDA、AscendCL等设备。主要功能如下"
msgstr ""
"The device is nndeploy's abstraction of hardware devices. Through the "
"abstraction of hardware devices, it shields the differences brought by "
"different hardware programming models. nndeploy currently supports devices "
"such as CPU, X86, ARM, CUDA, AscendCL, etc. The main functionalities are as "
"follows:"

#: ../../knowledge_shared/nndeploy-一款开源的模型端到端部署框架.md:359
#: 392f771407624eef99264ed0a123a9a1
msgid "统一的内存分配：为不同设备提供统一的内存分配接口，从而可简化数据容器Buffer、Mat、Tensor的内存分配"
msgstr ""
"Unified memory allocation: provides a unified memory allocation interface "
"for different devices, simplifying the memory allocation for data containers"
" Buffer, Mat, Tensor."

#: ../../knowledge_shared/nndeploy-一款开源的模型端到端部署框架.md:361
#: 6220368484fd4ab4a9af0d37cd41e2be
msgid ""
"统一的内存拷贝：为不同设备提供统一的内存拷贝接口（设备间拷贝、主从设备间上传/下载），从而可简化数据容器Buffer、Mat、Tensor的内存拷贝"
msgstr ""
"Unified memory copy: provides a unified memory copy interface for different "
"devices (device-to-device copy, host-to-device upload/download), simplifying"
" the memory copy for data containers Buffer, Mat, Tensor."

#: ../../knowledge_shared/nndeploy-一款开源的模型端到端部署框架.md:363
#: cf8d89d3622f42d0aabce6c278c3f1fb
msgid "统一的同步操作：为不同设备提供统一的同步操作接口，可简化设备端模型推理、算子等同步操作"
msgstr ""
"Unified synchronization operation: provides a unified synchronization "
"operation interface for different devices, simplifying device-side model "
"inference, operator synchronization operations."

#: ../../knowledge_shared/nndeploy-一款开源的模型端到端部署框架.md:365
#: 6cdfaa0666df46f48eedc138b24edf8c
msgid "统一的硬件设备信息查询：为不同设备提供统一的硬件设备信息查询接口，帮助用户更好的选择模型全流程部署的运行设备"
msgstr ""
"Unified hardware device information query: provides a unified hardware "
"device information query interface for different devices, helping users "
"better choose the runtime devices for the entire model deployment process."

#: ../../knowledge_shared/nndeploy-一款开源的模型端到端部署框架.md:367
#: ca5a2d7e1dc04ea6af105c9e76455561
msgid "与基于有向无环图的模型部署有关的三个模块"
msgstr ""
"Three modules related to model deployment based on directed acyclic graphs."

#: ../../knowledge_shared/nndeploy-一款开源的模型端到端部署框架.md:369
#: d8ce7f61107949a3b2d5bbb3447f9e82
msgid "3.4 基于有向无环图的模型部署"
msgstr "3.4 Model Deployment Based on Directed Acyclic Graphs"

#: ../../knowledge_shared/nndeploy-一款开源的模型端到端部署框架.md:371
#: b16a64862ab848e99d268d729703e568
msgid "下图是YOLOv8n的实际例子。"
msgstr "The following diagram is an actual example of YOLOv8n."

#: ../../knowledge_shared/nndeploy-一款开源的模型端到端部署框架.md
#: ../../knowledge_shared/nndeploy-一款开源的模型端到端部署框架.md:373
#: bb5f3c3e39e74b6eaa906a221407fc54 fd20955850fd4d058abb4c53884605cd
msgid "yolov8n"
msgstr "yolov8n"

#: ../../knowledge_shared/nndeploy-一款开源的模型端到端部署框架.md:375
#: 06141db9997743569818729bc81d3118
msgid ""
"这是一个非常典型的有向无环图，模型前处理->模型推理->模型推理构成NNDEPLOY_YOLOV8 "
"DAG(可供外部调用的库)，该DAG与编解码节点以及画框节点又可以共同构成一个新的DAG(可执行程序的demo)"
msgstr ""
"This is a very typical directed acyclic graph, model preprocessing->model "
"inference->model inference constitutes NNDEPLOY_YOLOV8 DAG (a library that "
"can be called externally), this DAG and the decoding node as well as the "
"drawing box node can together form a new DAG (a demo executable program)."

#: ../../knowledge_shared/nndeploy-一款开源的模型端到端部署框架.md
#: ../../knowledge_shared/nndeploy-一款开源的模型端到端部署框架.md:377
#: 09b0c1276d114db79d339ec797b4d727 bc90e30a1d98403687395149faa6d0d2
msgid "yolov8_dag"
msgstr "yolov8_dag"

#: ../../knowledge_shared/nndeploy-一款开源的模型端到端部署框架.md:379
#: a6f9377efde84edab9e095edd8e9a1cf
msgid ""
"注：对于已部署好的模型，需要编写demo展示效果，而demo需要处理多种格式的输入，例如图片输入输出、文件夹中多张图片的输入输出、视频的输入输出等，通过将上述编解码节点化，可以更通用以及更高效的完成demo的编写，达到快速展示效果的目的。"
msgstr ""
"Note: For already deployed models, it's necessary to write a demo to show "
"the effect, and the demo needs to handle inputs of multiple formats, such as"
" image input/output, multiple images in a folder input/output, video "
"input/output, etc. By modularizing the above decoding nodes, it can be more "
"universal and efficient to complete the writing of the demo, achieving the "
"goal of quickly showing the effect."

#: ../../knowledge_shared/nndeploy-一款开源的模型端到端部署框架.md:381
#: c2bd7a6efa50423e8fa10a9753cc66a2
msgid "3.5 流水线并行"
msgstr "3.5 Pipeline Parallelism"

#: ../../knowledge_shared/nndeploy-一款开源的模型端到端部署框架.md:383
#: 0bc6cc6659074fd4a26a7e3a605db848
msgid ""
"在处理多帧的场景下，基于有向无环图的模型部署方式，可将前处理 Node、推理 Node、后处理 "
"Node绑定三个不同的线程，每个线程又可绑定不同的硬件设备下，从而三个Node可流水线并行处理。在多模型以及多硬件设备的的复杂场景下，更加可以发挥流水线并行的优势，从而可显著提高整体吞吐量。下图为有向无环图"
" + 流水线并行 优化 YOLOv8n实际例子"
msgstr ""
"In scenarios processing multiple frames, based on the model deployment "
"method of directed acyclic graphs, the preprocessing Node, inference Node, "
"and postprocessing Node can be bound to three different threads, each thread"
" can also be bound to different hardware devices, thus the three Nodes can "
"process in pipeline parallel. In complex scenarios with multiple models and "
"multiple hardware devices, the advantages of pipeline parallelism can be "
"more fully utilized, significantly improving overall throughput. The "
"following diagram is an actual example of directed acyclic graph + pipeline "
"parallelism optimized YOLOv8n."

#: ../../knowledge_shared/nndeploy-一款开源的模型端到端部署框架.md
#: ../../knowledge_shared/nndeploy-一款开源的模型端到端部署框架.md:385
#: 285ff0587ddf4c5e84b941a1fdda8f7a d27427c96a59475c866f084e596cea76
msgid "pipeline_parallel"
msgstr "pipeline_parallel"

#: ../../knowledge_shared/nndeploy-一款开源的模型端到端部署框架.md:387
#: 81433ab67e5640159820ad99ff592737
msgid "3.6 多模型的复杂场景"
msgstr "3.6 Complex Scenarios with Multiple Models"

#: ../../knowledge_shared/nndeploy-一款开源的模型端到端部署框架.md:389
#: 9111cca3f24148ca86dc3d4cd6e829b4
msgid ""
"下图是老照片修复算法的实际例子，该算法有6个模型 + "
"1个传统算法（老照片->划痕检测->划痕修复->超分辨率->condition(loop(人脸检测->人脸矫正->人脸修复->人脸贴回))->修复后的照片）组合，基于nndeploy通过dag来部署非常直接且开发的心智负担很小。假如不用dag来部署，实际的代码中每个模型都需要手动串联，会有大量业务代码、模型耦合度高、灵活性差、代码不适合并行等等一些问题"
msgstr ""
"The following diagram is an actual example of an old photo restoration "
"algorithm, which includes 6 models + 1 traditional algorithm (old "
"photo->scratch detection->scratch restoration->super-"
"resolution->condition(loop(face detection->face correction->face "
"restoration->face pasting))->restored photo) combination. Based on nndeploy,"
" deploying through dag is very straightforward and the mental burden of "
"development is very small. If dag is not used for deployment, in actual "
"code, each model needs to be manually concatenated, which would involve a "
"large amount of business code, high model coupling, poor flexibility, code "
"not suitable for parallelization, and other issues."

#: ../../knowledge_shared/nndeploy-一款开源的模型端到端部署框架.md
#: ../../knowledge_shared/nndeploy-一款开源的模型端到端部署框架.md:391
#: a8fdd1e54946429d8ed7125ca95702ed c9cb8bd4f0844678a08805287e2dc365
msgid "complex"
msgstr "complex"

#: ../../knowledge_shared/nndeploy-一款开源的模型端到端部署框架.md:393
#: c75eba3ccae6464284121d09420da143
msgid "4 下一步规划"
msgstr "4 Next Steps Planning"

#: ../../knowledge_shared/nndeploy-一款开源的模型端到端部署框架.md:395
#: 2db44ecd284b4a4f88d9c86aa497e036
msgid "推理后端"
msgstr "Inference backend"

#: ../../knowledge_shared/nndeploy-一款开源的模型端到端部署框架.md:396
#: 330efd2f18be43d0bd40a6ed755de134
msgid "完善已接入的推理框架coreml"
msgstr "Improve the already integrated inference framework coreml"

#: ../../knowledge_shared/nndeploy-一款开源的模型端到端部署框架.md:397
#: f29f0006febf4ed5a8b60439f968ca4a
msgid "完善已接入的推理框架paddle-lite"
msgstr "Improve the already integrated inference framework paddle-lite"

#: ../../knowledge_shared/nndeploy-一款开源的模型端到端部署框架.md:398
#: d73b009d0e2443209573ea62931db9a3
msgid "接入新的推理框架TFLite"
msgstr "Integrate the new inference framework TFLite"

#: ../../knowledge_shared/nndeploy-一款开源的模型端到端部署框架.md:399
#: 272515cee5044943a2b9d330dc100c6d
msgid "设备管理模块"
msgstr "Device management module"

#: ../../knowledge_shared/nndeploy-一款开源的模型端到端部署框架.md:400
#: 1b60196895e34471baa5573afef7d504
msgid "新增OpenCL的设备管理模块"
msgstr "Add OpenCL's device management module"

#: ../../knowledge_shared/nndeploy-一款开源的模型端到端部署框架.md:401
#: cd588bcb8e0340b79fe37f28a3ca5c2e
msgid "新增ROCM的设备管理模块"
msgstr "Add ROCM's device management module"

#: ../../knowledge_shared/nndeploy-一款开源的模型端到端部署框架.md:402
#: ac8a3538cfe244d3a46820c5f45bd01e
msgid "新增OpenGL的设备管理模块"
msgstr "Add OpenGL's device management module"

#: ../../knowledge_shared/nndeploy-一款开源的模型端到端部署框架.md:403
#: aa85ad18fe034a9bbb937f3dccdf88ae
msgid "内存优化"
msgstr "Memory optimization"

#: ../../knowledge_shared/nndeploy-一款开源的模型端到端部署框架.md:404
#: 2e32f65bdf1543f892008aa6a86ad033
msgid "主从内存拷贝优化：针对统一内存的架构，通过主从内存映射、主从内存地址共享等方式替代主从内存拷贝"
msgstr ""
"Master-slave memory copy optimization: for the unified memory architecture, "
"replace master-slave memory copy with master-slave memory mapping, master-"
"slave memory address sharing, etc."

#: ../../knowledge_shared/nndeploy-一款开源的模型端到端部署框架.md:405
#: 0d3b7d9b598a421b9eaeaaceb29a816a
msgid "内存池：针对nndeploy的内部的数据容器Buffer、Mat、Tensor，建立异构设备的内存池，实现高性能的内存分配与释放"
msgstr ""
"Memory pool: for nndeploy's internal data containers Buffer, Mat, Tensor, "
"establish a memory pool for heterogeneous devices, achieving high-"
"performance memory allocation and release."

#: ../../knowledge_shared/nndeploy-一款开源的模型端到端部署框架.md:406
#: 6b1d63331f584f8b941f2adaafed4964
msgid "多节点共享内存机制：针对多模型串联场景下，基于模型部署的有向无环图，在串行执行的模式下，支持多推理节点共享内存机制"
msgstr ""
"Multi-node shared memory mechanism: for multi-model serial scenarios, based "
"on the directed acyclic graph of model deployment, support multi-inference "
"node shared memory mechanism in serial execution mode."

#: ../../knowledge_shared/nndeploy-一款开源的模型端到端部署框架.md:407
#: 2b6febbe61884bbaa7ab9aee70b61f76
msgid "边的环形队列内存复用机制：基于模型部署的有向无环图，在流水线并行执行的模式下，支持边的环形队列共享内存机制"
msgstr ""
"Edge circular queue memory reuse mechanism: based on the directed acyclic "
"graph of model deployment, support edge circular queue shared memory "
"mechanism in pipeline parallel execution mode."

#: ../../knowledge_shared/nndeploy-一款开源的模型端到端部署框架.md:408
#: d4dc362e9e6a4155ad11b7cc3d292ede
msgid "stable diffusion model"
msgstr "stable diffusion model"

#: ../../knowledge_shared/nndeploy-一款开源的模型端到端部署框架.md:409
#: 6737a4319d214c5090543aab8ef5ad50
msgid "部署stable diffusion model"
msgstr "Deploy stable diffusion model"

#: ../../knowledge_shared/nndeploy-一款开源的模型端到端部署框架.md:410
#: 0112cfd14ecd4c54a6b62fbddc92b614
msgid "针对stable diffusion model搭建stable_diffusion.cpp（推理子模块，手动构建计算图的方式）"
msgstr ""
"For the stable diffusion model with stable_diffusion.cpp (inference module, "
"manually constructing the computation graph)"

#: ../../knowledge_shared/nndeploy-一款开源的模型端到端部署框架.md:411
#: b37f395275e84622aaf8a572214f26d3
msgid "高性能op"
msgstr "High-performance op"

#: ../../knowledge_shared/nndeploy-一款开源的模型端到端部署框架.md:412
#: 13ba01ed5b9346ae8816c7f28c7a8565
msgid "分布式"
msgstr "Distributed"

#: ../../knowledge_shared/nndeploy-一款开源的模型端到端部署框架.md:413
#: 1ca8483bbf9449c79b1f9cb79fc72db7
msgid "在多模型共同完成一个任务的场景里，将多个模型调度到多个机器上分布式执行"
msgstr ""
"In scenarios where multiple models jointly complete a task, distribute "
"multiple models to multiple machines for distributed execution"

#: ../../knowledge_shared/nndeploy-一款开源的模型端到端部署框架.md:414
#: 4b3e87757e724e189f340ff67d1bd5fb
msgid "在大模型的场景下，通过切割大模型为多个子模型的方式，将多个子模型调度到多个机器上分布式执行"
msgstr ""
"In the context of large models, by splitting the large model into multiple "
"sub-models, distribute multiple sub-models to multiple machines for "
"distributed execution"

#: ../../knowledge_shared/nndeploy-一款开源的模型端到端部署框架.md:417
#: b45c259f63294392a87faed865e73bee
msgid "5 参考"
msgstr "5 References"

#: ../../knowledge_shared/nndeploy-一款开源的模型端到端部署框架.md:419
#: 4d4b688898b842d5b2c9c67945dd7c02
msgid "TNN"
msgstr "TNN"

#: ../../knowledge_shared/nndeploy-一款开源的模型端到端部署框架.md:420
#: 87565b550186478d829d2a308cc1c747
msgid "FastDeploy"
msgstr "FastDeploy"

#: ../../knowledge_shared/nndeploy-一款开源的模型端到端部署框架.md:421
#: aff148862728477f9cac5ac78514e3f7
msgid "opencv"
msgstr "opencv"

#: ../../knowledge_shared/nndeploy-一款开源的模型端到端部署框架.md:422
#: 7c2f38a10c3141a8b4443c9d21cb3c56
msgid "CGraph"
msgstr "CGraph"

#: ../../knowledge_shared/nndeploy-一款开源的模型端到端部署框架.md:423
#: 71ad7f06797a4f6f8fab2a6dfb64e72e
msgid "CThreadPool"
msgstr "CThreadPool"

#: ../../knowledge_shared/nndeploy-一款开源的模型端到端部署框架.md:424
#: a40085f2cfa449b786484e64af9b7aee
msgid "tvm"
msgstr "tvm"

#: ../../knowledge_shared/nndeploy-一款开源的模型端到端部署框架.md:425
#: c902ad0a4f404ec58978c92d559b0250
msgid "mmdeploy"
msgstr "mmdeploy"

#: ../../knowledge_shared/nndeploy-一款开源的模型端到端部署框架.md:426
#: 3b441035b5fa4a6ba02ab9f5a070568f
msgid "FlyCV"
msgstr "FlyCV"

#: ../../knowledge_shared/nndeploy-一款开源的模型端到端部署框架.md:427
#: 56bdbcd1e2cb43089c6281e402bb428f
msgid "torchpipe"
msgstr "torchpipe"

#: ../../knowledge_shared/nndeploy-一款开源的模型端到端部署框架.md:430
#: 7d1f15f1374c4ff29236d4abbf0b14a8
msgid "6 加入我们"
msgstr "6 Join Us"

#: ../../knowledge_shared/nndeploy-一款开源的模型端到端部署框架.md:432
#: b4391812b1a94f68ae7b61c6c2bf92f1
msgid ""
"nndeploy是由一群志同道合的网友共同开发以及维护，我们不定时讨论技术，分享行业见解。当前nndeploy正处于发展阶段，如果您热爱开源、喜欢折腾，不论是出于学习目的，抑或是有更好的想法，欢迎加入我们。"
msgstr ""
"nndeploy is developed and maintained by a group of like-minded netizens "
"together. We regularly discuss technology and share industry insights. "
"Currently, nndeploy is in the development stage. If you love open source, "
"enjoy challenges, whether for learning purposes or have better ideas, you "
"are welcome to join us."

#: ../../knowledge_shared/nndeploy-一款开源的模型端到端部署框架.md:433
#: c77236aa81f14d688b24490f8dd8f5bb
msgid "微信：Always031856 (可加我微信进nndeploy交流群，备注：nndeploy+姓名)"
msgstr ""
"WeChat: Always031856 (You can add my WeChat to join the nndeploy discussion "
"group, note: nndeploy+name)"
