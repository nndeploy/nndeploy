
# 基于diffusers二次开发（diffusers、xdit、cache-dit、dax、onediff、stable_fast、diffsyth）

## 性能优化

### SAMPLING

### 缓存 (Cache-DiT)

#### DBCache: 双块缓存 (Dual Block Cache)
- **双重缓存机制**：同时缓存多个Transformer块的中间结果
- **智能缓存策略**：根据计算成本和内存使用动态选择缓存块
- **缓存命中优化**：提高缓存命中率，减少重复计算

#### DBPrune: 动态块剪枝 (Dynamic Block Prune)
- **动态剪枝策略**：运行时动态决定跳过哪些Transformer块
- **质量保持**：在保证生成质量的前提下减少计算量
- **自适应阈值**：根据输入内容自动调整剪枝阈值

#### 混合缓存CFG (Hybrid Cache CFG)
- **分类器自由引导缓存**：针对CFG过程的专门缓存优化
- **条件缓存**：分别缓存有条件和无条件的推理结果
- **引导强度自适应**：根据引导强度动态调整缓存策略

#### TaylorSeer校准器 (Hybrid TaylorSeer Calibrator)
- **泰勒展开预测**：使用泰勒级数预测跳过块的输出
- **校准机制**：动态校准预测精度，保证生成质量
- **混合策略**：结合缓存和预测，实现最优性能

### 并行（针对Dit模型的优化）

#### 序列并行 (Sequence Parallelism)

#### 流水线并行

#### CFG并行

#### 张量并行

#### 数据并行

### 量化 (Quantization)

### 算子 (Operators)

#### 注意力算子
- **Flash Attention (FA2/FA3)**：内存高效的注意力计算
  - FA3优先使用（如果已安装），否则回退到FA2
  - 显著减少注意力计算的内存占用
- **Sage Attention**：针对扩散模型优化的注意力算子
  - 推荐用于WAN2.1等大型扩散模型
  - 量化友好的注意力实现
- **xFormers**：Facebook开源的高效Transformer算子库
- **SDPA (Scaled Dot-Product Attention)**：PyTorch原生注意力实现（默认后备选项）

#### 高性能算子库 cudnn/cutlass

#### 自定义算子
- 用户自定义CUDA kernel

#### 通信算子

### torch.compile优化

### 静态图优化
- Trace
- CUDA GRAPH
- 内存布局优化 NHWC/NCHW/NCDHW/NDCHW(服务算子优化)

### ComfyUI节点化工作流优化

#### 同一份图结构

#### 中间结果缓存 (Intermediate Result Caching)
- **智能缓存策略**：当输入参数未变化时，自动跳过节点执行并复用缓存结果
- **CLIP编码缓存**：提示词未变化时，CLIP文本编码结果可直接复用
- **VAE编码缓存**：相同输入图像的VAE编码结果缓存
- **模型权重缓存**：避免重复加载相同的模型检查点
- **条件向量缓存**：缓存文本条件、图像条件等编码结果

## 内存优化

### 内存分类
- **权重 (Parameters)**：模型的可训练参数，如线性层权重、卷积核等
- **缓冲区 (Buffers)**：模型的非可训练状态，如BatchNorm的running_mean、running_var等，add算子（一个来自激活值，另一个来自Buffers）
- **激活值 (Activations)**：前向传播过程中的中间计算结果
- **梯度 (Gradients)**：反向传播计算的参数梯度（推理时不需要）
- **优化器状态 (Optimizer States)**：Adam等优化器的动量、方差估计等状态（推理时不需要）
- **临时张量 (Temporary Tensors)**：计算过程中的临时变量和中间结果（比如卷积需要paddding，中间需要临时Tensor）


### 激活值

根据文档中的内容以及PyTorch的内存管理机制，我来详细解答激活值的释放时机：

## PyTorch激活值释放时机

### **训练时 (Training)**

激活值的释放时机：**在反向传播完成后释放**

1. **前向传播阶段**：
   - 所有中间激活值都会被保存在内存中
   - PyTorch会自动构建计算图，保留所有需要梯度的张量
   - 这些激活值用于反向传播时计算梯度

2. **反向传播阶段**：
   - 从输出层开始，逐层计算梯度
   - 每一层的梯度计算完成后，对应的激活值就可以被释放
   - 释放顺序：**从后向前**，与反向传播的计算顺序一致

3. **具体释放时机**：
   ```python
   # 伪代码示例
   y = model(x)           # 前向传播：保存所有激活值
   loss = criterion(y)    # 计算损失：保存损失相关激活值
   loss.backward()        # 反向传播：逐层释放激活值
   # backward()完成后，所有激活值的引用都被解除，可以被垃圾回收
   ```

### **推理时 (Inference)**

激活值的释放时机：**每层计算完成后立即释放**（如果没有其他引用）

1. **使用 `torch.no_grad()` 或 `model.eval()` 模式**：
   - 不构建计算图，不保存梯度信息
   - 每层的激活值在传递给下一层后，如果没有其他引用就可以立即释放
   - 释放时机：**逐层释放，实时回收**

2. **具体释放时机**：
   ```python
   with torch.no_grad():
       x1 = layer1(x)      # x可以被释放（如果没有其他引用）
       x2 = layer2(x1)     # x1可以被释放（如果是线性结构）
       x3 = layer3(x2)     # x2可以被释放
       output = layer4(x3) # x3可以被释放
   ```

3. **特殊情况**：
   - **非线性结构**（如残差连接、U-Net）：需要保留跳跃连接的中间结果，直到它们被使用完
   - **显式引用**：如果代码中保存了中间激活值的引用，需要等引用解除才能释放

### **关键区别总结**

| 特性 | 训练时 | 推理时 |
|------|--------|--------|
| **释放时机** | 反向传播完成后 | 每层计算后立即释放 |
| **释放顺序** | 从后向前（反向传播顺序） | 从前向后（前向传播顺序） |
| **内存占用** | 需要保存全部激活值 | 只保存当前层和必要的跳跃连接 |
| **生命周期** | 前向+反向全程 | 单层传播期间 |

### **优化建议**（文档中已提到的）

1. **非线性模型的内存优化**：
   - 将中间结果移到CPU（host端）
   - 在需要时再移回GPU

2. **手动释放**：
   - 在nn.Module内部，显式删除不再需要的中间值
   - 使用 `del` 语句或让变量超出作用域

3. **梯度检查点 (Gradient Checkpointing)**：
   - 训练时不保存所有激活值
   - 反向传播时重新计算需要的激活值
   - 牺牲计算时间换取内存空间

希望这个解释清楚地说明了PyTorch在不同场景下激活值的释放机制！

### 模型使用才加载
- clip
- dit
- vae

### dit
- 权重：权重按需加载，用完即卸载（块级别、算子级别）
- 激活值
  - 将有残差连接卸载到CPU
  - 手动释放一个nn.Module中的已经不再使用的内存
- 重计算

- 多流模式

### 量化

### 


