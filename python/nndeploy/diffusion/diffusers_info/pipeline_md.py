# Automatically generated by extract_model_paths.py
DIFFUSERS_PIPELINE_MD = {
    "src/diffusers/pipelines/allegro/pipeline_allegro.py": (
        '''
        Examples:
                ```py
                >>> import torch
                >>> from diffusers import AutoencoderKLAllegro, AllegroPipeline
                >>> from diffusers.utils import export_to_video
        
                >>> vae = AutoencoderKLAllegro.from_pretrained("rhymes-ai/Allegro", subfolder="vae", torch_dtype=torch.float32)
                >>> pipe = AllegroPipeline.from_pretrained("rhymes-ai/Allegro", vae=vae, torch_dtype=torch.bfloat16).to("cuda")
                >>> pipe.enable_vae_tiling()
        
                >>> prompt = (
                ...     "A seaside harbor with bright sunlight and sparkling seawater, with many boats in the water. From an aerial view, "
                ...     "the boats vary in size and color, some moving and some stationary. Fishing boats in the water suggest that this "
                ...     "location might be a popular spot for docking fishing boats."
                ... )
                >>> video = pipe(prompt, guidance_scale=7.5, max_sequence_length=512).frames[0]
                >>> export_to_video(video, "output.mp4", fps=15)
                ```
        '''
    ),
    "src/diffusers/pipelines/amused/pipeline_amused.py": (
        '''
        Examples:
                ```py
                >>> import torch
                >>> from diffusers import AmusedPipeline
        
                >>> pipe = AmusedPipeline.from_pretrained("amused/amused-512", variant="fp16", torch_dtype=torch.float16)
                >>> pipe = pipe.to("cuda")
        
                >>> prompt = "a photo of an astronaut riding a horse on mars"
                >>> image = pipe(prompt).images[0]
                ```
        '''
    ),
    "src/diffusers/pipelines/amused/pipeline_amused_img2img.py": (
        '''
        Examples:
                ```py
                >>> import torch
                >>> from diffusers import AmusedImg2ImgPipeline
                >>> from diffusers.utils import load_image
        
                >>> pipe = AmusedImg2ImgPipeline.from_pretrained(
                ...     "amused/amused-512", variant="fp16", torch_dtype=torch.float16
                ... )
                >>> pipe = pipe.to("cuda")
        
                >>> prompt = "winter mountains"
                >>> input_image = (
                ...     load_image(
                ...         "https://huggingface.co/datasets/diffusers/docs-images/resolve/main/open_muse/mountains.jpg"
                ...     )
                ...     .resize((512, 512))
                ...     .convert("RGB")
                ... )
                >>> image = pipe(prompt, input_image).images[0]
                ```
        '''
    ),
    "src/diffusers/pipelines/amused/pipeline_amused_inpaint.py": (
        '''
        Examples:
                ```py
                >>> import torch
                >>> from diffusers import AmusedInpaintPipeline
                >>> from diffusers.utils import load_image
        
                >>> pipe = AmusedInpaintPipeline.from_pretrained(
                ...     "amused/amused-512", variant="fp16", torch_dtype=torch.float16
                ... )
                >>> pipe = pipe.to("cuda")
        
                >>> prompt = "fall mountains"
                >>> input_image = (
                ...     load_image(
                ...         "https://huggingface.co/datasets/diffusers/docs-images/resolve/main/open_muse/mountains_1.jpg"
                ...     )
                ...     .resize((512, 512))
                ...     .convert("RGB")
                ... )
                >>> mask = (
                ...     load_image(
                ...         "https://huggingface.co/datasets/diffusers/docs-images/resolve/main/open_muse/mountains_1_mask.png"
                ...     )
                ...     .resize((512, 512))
                ...     .convert("L")
                ... )
                >>> pipe(prompt, input_image, mask).images[0].save("out.png")
                ```
        '''
    ),
    "src/diffusers/pipelines/animatediff/pipeline_animatediff.py": (
        '''
        Examples:
                ```py
                >>> import torch
                >>> from diffusers import MotionAdapter, AnimateDiffPipeline, DDIMScheduler
                >>> from diffusers.utils import export_to_gif
        
                >>> adapter = MotionAdapter.from_pretrained("guoyww/animatediff-motion-adapter-v1-5-2")
                >>> pipe = AnimateDiffPipeline.from_pretrained("frankjoshua/toonyou_beta6", motion_adapter=adapter)
                >>> pipe.scheduler = DDIMScheduler(beta_schedule="linear", steps_offset=1, clip_sample=False)
                >>> output = pipe(prompt="A corgi walking in the park")
                >>> frames = output.frames[0]
                >>> export_to_gif(frames, "animation.gif")
                ```
        '''
    ),
    "src/diffusers/pipelines/animatediff/pipeline_animatediff_controlnet.py": (
        '''
        Examples:
                ```py
                >>> import torch
                >>> from diffusers import (
                ...     AnimateDiffControlNetPipeline,
                ...     AutoencoderKL,
                ...     ControlNetModel,
                ...     MotionAdapter,
                ...     LCMScheduler,
                ... )
                >>> from diffusers.utils import export_to_gif, load_video
        
                >>> # Additionally, you will need a preprocess videos before they can be used with the ControlNet
                >>> # HF maintains just the right package for it: `pip install controlnet_aux`
                >>> from controlnet_aux.processor import ZoeDetector
        
                >>> # Download controlnets from https://huggingface.co/lllyasviel/ControlNet-v1-1 to use .from_single_file
                >>> # Download Diffusers-format controlnets, such as https://huggingface.co/lllyasviel/sd-controlnet-depth, to use .from_pretrained()
                >>> controlnet = ControlNetModel.from_single_file("control_v11f1p_sd15_depth.pth", torch_dtype=torch.float16)
        
                >>> # We use AnimateLCM for this example but one can use the original motion adapters as well (for example, https://huggingface.co/guoyww/animatediff-motion-adapter-v1-5-3)
                >>> motion_adapter = MotionAdapter.from_pretrained("wangfuyun/AnimateLCM")
        
                >>> vae = AutoencoderKL.from_pretrained("stabilityai/sd-vae-ft-mse", torch_dtype=torch.float16)
                >>> pipe: AnimateDiffControlNetPipeline = AnimateDiffControlNetPipeline.from_pretrained(
                ...     "SG161222/Realistic_Vision_V5.1_noVAE",
                ...     motion_adapter=motion_adapter,
                ...     controlnet=controlnet,
                ...     vae=vae,
                ... ).to(device="cuda", dtype=torch.float16)
                >>> pipe.scheduler = LCMScheduler.from_config(pipe.scheduler.config, beta_schedule="linear")
                >>> pipe.load_lora_weights(
                ...     "wangfuyun/AnimateLCM", weight_name="AnimateLCM_sd15_t2v_lora.safetensors", adapter_name="lcm-lora"
                ... )
                >>> pipe.set_adapters(["lcm-lora"], [0.8])
        
                >>> depth_detector = ZoeDetector.from_pretrained("lllyasviel/Annotators").to("cuda")
                >>> video = load_video(
                ...     "https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/animatediff-vid2vid-input-1.gif"
                ... )
                >>> conditioning_frames = []
        
                >>> with pipe.progress_bar(total=len(video)) as progress_bar:
                ...     for frame in video:
                ...         conditioning_frames.append(depth_detector(frame))
                ...         progress_bar.update()
        
                >>> prompt = "a panda, playing a guitar, sitting in a pink boat, in the ocean, mountains in background, realistic, high quality"
                >>> negative_prompt = "bad quality, worst quality"
        
                >>> video = pipe(
                ...     prompt=prompt,
                ...     negative_prompt=negative_prompt,
                ...     num_frames=len(video),
                ...     num_inference_steps=10,
                ...     guidance_scale=2.0,
                ...     conditioning_frames=conditioning_frames,
                ...     generator=torch.Generator().manual_seed(42),
                ... ).frames[0]
        
                >>> export_to_gif(video, "animatediff_controlnet.gif", fps=8)
                ```
        '''
    ),
    "src/diffusers/pipelines/animatediff/pipeline_animatediff_sdxl.py": (
        '''
        Examples:
                ```py
                >>> import torch
                >>> from diffusers.models import MotionAdapter
                >>> from diffusers import AnimateDiffSDXLPipeline, DDIMScheduler
                >>> from diffusers.utils import export_to_gif
        
                >>> adapter = MotionAdapter.from_pretrained(
                ...     "a-r-r-o-w/animatediff-motion-adapter-sdxl-beta", torch_dtype=torch.float16
                ... )
        
                >>> model_id = "stabilityai/stable-diffusion-xl-base-1.0"
                >>> scheduler = DDIMScheduler.from_pretrained(
                ...     model_id,
                ...     subfolder="scheduler",
                ...     clip_sample=False,
                ...     timestep_spacing="linspace",
                ...     beta_schedule="linear",
                ...     steps_offset=1,
                ... )
                >>> pipe = AnimateDiffSDXLPipeline.from_pretrained(
                ...     model_id,
                ...     motion_adapter=adapter,
                ...     scheduler=scheduler,
                ...     torch_dtype=torch.float16,
                ...     variant="fp16",
                ... ).to("cuda")
        
                >>> # enable memory savings
                >>> pipe.enable_vae_slicing()
                >>> pipe.enable_vae_tiling()
        
                >>> output = pipe(
                ...     prompt="a panda surfing in the ocean, realistic, high quality",
                ...     negative_prompt="low quality, worst quality",
                ...     num_inference_steps=20,
                ...     guidance_scale=8,
                ...     width=1024,
                ...     height=1024,
                ...     num_frames=16,
                ... )
        
                >>> frames = output.frames[0]
                >>> export_to_gif(frames, "animation.gif")
                ```
        '''
    ),
    "src/diffusers/pipelines/animatediff/pipeline_animatediff_sparsectrl.py": (
        '''
        Examples:
                ```python
                >>> import torch
                >>> from diffusers import AnimateDiffSparseControlNetPipeline
                >>> from diffusers.models import AutoencoderKL, MotionAdapter, SparseControlNetModel
                >>> from diffusers.schedulers import DPMSolverMultistepScheduler
                >>> from diffusers.utils import export_to_gif, load_image
        
                >>> model_id = "SG161222/Realistic_Vision_V5.1_noVAE"
                >>> motion_adapter_id = "guoyww/animatediff-motion-adapter-v1-5-3"
                >>> controlnet_id = "guoyww/animatediff-sparsectrl-scribble"
                >>> lora_adapter_id = "guoyww/animatediff-motion-lora-v1-5-3"
                >>> vae_id = "stabilityai/sd-vae-ft-mse"
                >>> device = "cuda"
        
                >>> motion_adapter = MotionAdapter.from_pretrained(motion_adapter_id, torch_dtype=torch.float16).to(device)
                >>> controlnet = SparseControlNetModel.from_pretrained(controlnet_id, torch_dtype=torch.float16).to(device)
                >>> vae = AutoencoderKL.from_pretrained(vae_id, torch_dtype=torch.float16).to(device)
                >>> scheduler = DPMSolverMultistepScheduler.from_pretrained(
                ...     model_id,
                ...     subfolder="scheduler",
                ...     beta_schedule="linear",
                ...     algorithm_type="dpmsolver++",
                ...     use_karras_sigmas=True,
                ... )
                >>> pipe = AnimateDiffSparseControlNetPipeline.from_pretrained(
                ...     model_id,
                ...     motion_adapter=motion_adapter,
                ...     controlnet=controlnet,
                ...     vae=vae,
                ...     scheduler=scheduler,
                ...     torch_dtype=torch.float16,
                ... ).to(device)
                >>> pipe.load_lora_weights(lora_adapter_id, adapter_name="motion_lora")
                >>> pipe.fuse_lora(lora_scale=1.0)
        
                >>> prompt = "an aerial view of a cyberpunk city, night time, neon lights, masterpiece, high quality"
                >>> negative_prompt = "low quality, worst quality, letterboxed"
        
                >>> image_files = [
                ...     "https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/animatediff-scribble-1.png",
                ...     "https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/animatediff-scribble-2.png",
                ...     "https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/animatediff-scribble-3.png",
                ... ]
                >>> condition_frame_indices = [0, 8, 15]
                >>> conditioning_frames = [load_image(img_file) for img_file in image_files]
        
                >>> video = pipe(
                ...     prompt=prompt,
                ...     negative_prompt=negative_prompt,
                ...     num_inference_steps=25,
                ...     conditioning_frames=conditioning_frames,
                ...     controlnet_conditioning_scale=1.0,
                ...     controlnet_frame_indices=condition_frame_indices,
                ...     generator=torch.Generator().manual_seed(1337),
                ... ).frames[0]
                >>> export_to_gif(video, "output.gif")
                ```
        '''
    ),
    "src/diffusers/pipelines/animatediff/pipeline_animatediff_video2video.py": (
        '''
        Examples:
                ```py
                >>> import imageio
                >>> import requests
                >>> import torch
                >>> from diffusers import AnimateDiffVideoToVideoPipeline, DDIMScheduler, MotionAdapter
                >>> from diffusers.utils import export_to_gif
                >>> from io import BytesIO
                >>> from PIL import Image
        
                >>> adapter = MotionAdapter.from_pretrained(
                ...     "guoyww/animatediff-motion-adapter-v1-5-2", torch_dtype=torch.float16
                ... )
                >>> pipe = AnimateDiffVideoToVideoPipeline.from_pretrained(
                ...     "SG161222/Realistic_Vision_V5.1_noVAE", motion_adapter=adapter
                ... ).to("cuda")
                >>> pipe.scheduler = DDIMScheduler(
                ...     beta_schedule="linear", steps_offset=1, clip_sample=False, timespace_spacing="linspace"
                ... )
        
        
                >>> def load_video(file_path: str):
                ...     images = []
        
                ...     if file_path.startswith(("http://", "https://")):
                ...         # If the file_path is a URL
                ...         response = requests.get(file_path)
                ...         response.raise_for_status()
                ...         content = BytesIO(response.content)
                ...         vid = imageio.get_reader(content)
                ...     else:
                ...         # Assuming it's a local file path
                ...         vid = imageio.get_reader(file_path)
        
                ...     for frame in vid:
                ...         pil_image = Image.fromarray(frame)
                ...         images.append(pil_image)
        
                ...     return images
        
        
                >>> video = load_video(
                ...     "https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/animatediff-vid2vid-input-1.gif"
                ... )
                >>> output = pipe(
                ...     video=video, prompt="panda playing a guitar, on a boat, in the ocean, high quality", strength=0.5
                ... )
                >>> frames = output.frames[0]
                >>> export_to_gif(frames, "animation.gif")
                ```
        '''
    ),
    "src/diffusers/pipelines/animatediff/pipeline_animatediff_video2video_controlnet.py": (
        '''
        Examples:
                ```py
                >>> import torch
                >>> from PIL import Image
                >>> from tqdm.auto import tqdm
        
                >>> from diffusers import AnimateDiffVideoToVideoControlNetPipeline
                >>> from diffusers.utils import export_to_gif, load_video
                >>> from diffusers import AutoencoderKL, ControlNetModel, MotionAdapter, LCMScheduler
        
                >>> controlnet = ControlNetModel.from_pretrained(
                ...     "lllyasviel/sd-controlnet-openpose", torch_dtype=torch.float16
                ... )
                >>> motion_adapter = MotionAdapter.from_pretrained("wangfuyun/AnimateLCM")
                >>> vae = AutoencoderKL.from_pretrained("stabilityai/sd-vae-ft-mse", torch_dtype=torch.float16)
        
                >>> pipe = AnimateDiffVideoToVideoControlNetPipeline.from_pretrained(
                ...     "SG161222/Realistic_Vision_V5.1_noVAE",
                ...     motion_adapter=motion_adapter,
                ...     controlnet=controlnet,
                ...     vae=vae,
                ... ).to(device="cuda", dtype=torch.float16)
        
                >>> pipe.scheduler = LCMScheduler.from_config(pipe.scheduler.config, beta_schedule="linear")
                >>> pipe.load_lora_weights(
                ...     "wangfuyun/AnimateLCM", weight_name="AnimateLCM_sd15_t2v_lora.safetensors", adapter_name="lcm-lora"
                ... )
                >>> pipe.set_adapters(["lcm-lora"], [0.8])
        
                >>> video = load_video(
                ...     "https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/dance.gif"
                ... )
                >>> video = [frame.convert("RGB") for frame in video]
        
                >>> from controlnet_aux.processor import OpenposeDetector
        
                >>> open_pose = OpenposeDetector.from_pretrained("lllyasviel/Annotators").to("cuda")
                >>> for frame in tqdm(video):
                ...     conditioning_frames.append(open_pose(frame))
        
                >>> prompt = "astronaut in space, dancing"
                >>> negative_prompt = "bad quality, worst quality, jpeg artifacts, ugly"
        
                >>> strength = 0.8
                >>> with torch.inference_mode():
                ...     video = pipe(
                ...         video=video,
                ...         prompt=prompt,
                ...         negative_prompt=negative_prompt,
                ...         num_inference_steps=10,
                ...         guidance_scale=2.0,
                ...         controlnet_conditioning_scale=0.75,
                ...         conditioning_frames=conditioning_frames,
                ...         strength=strength,
                ...         generator=torch.Generator().manual_seed(42),
                ...     ).frames[0]
        
                >>> video = [frame.resize(conditioning_frames[0].size) for frame in video]
                >>> export_to_gif(video, f"animatediff_vid2vid_controlnet.gif", fps=8)
                ```
        '''
    ),
    "src/diffusers/pipelines/audioldm/pipeline_audioldm.py": (
        '''
        Examples:
                ```py
                >>> from diffusers import AudioLDMPipeline
                >>> import torch
                >>> import scipy
        
                >>> repo_id = "cvssp/audioldm-s-full-v2"
                >>> pipe = AudioLDMPipeline.from_pretrained(repo_id, torch_dtype=torch.float16)
                >>> pipe = pipe.to("cuda")
        
                >>> prompt = "Techno music with a strong, upbeat tempo and high melodic riffs"
                >>> audio = pipe(prompt, num_inference_steps=10, audio_length_in_s=5.0).audios[0]
        
                >>> # save the audio sample as a .wav file
                >>> scipy.io.wavfile.write("techno.wav", rate=16000, data=audio)
                ```
        '''
    ),
    "src/diffusers/pipelines/audioldm2/pipeline_audioldm2.py": (
        '''
        Examples:
                ```py
                >>> import scipy
                >>> import torch
                >>> from diffusers import AudioLDM2Pipeline
        
                >>> repo_id = "cvssp/audioldm2"
                >>> pipe = AudioLDM2Pipeline.from_pretrained(repo_id, torch_dtype=torch.float16)
                >>> pipe = pipe.to("cuda")
        
                >>> # define the prompts
                >>> prompt = "The sound of a hammer hitting a wooden surface."
                >>> negative_prompt = "Low quality."
        
                >>> # set the seed for generator
                >>> generator = torch.Generator("cuda").manual_seed(0)
        
                >>> # run the generation
                >>> audio = pipe(
                ...     prompt,
                ...     negative_prompt=negative_prompt,
                ...     num_inference_steps=200,
                ...     audio_length_in_s=10.0,
                ...     num_waveforms_per_prompt=3,
                ...     generator=generator,
                ... ).audios
        
                >>> # save the best audio sample (index 0) as a .wav file
                >>> scipy.io.wavfile.write("techno.wav", rate=16000, data=audio[0])
                ```
                ```
                #Using AudioLDM2 for Text To Speech
                >>> import scipy
                >>> import torch
                >>> from diffusers import AudioLDM2Pipeline
        
                >>> repo_id = "anhnct/audioldm2_gigaspeech"
                >>> pipe = AudioLDM2Pipeline.from_pretrained(repo_id, torch_dtype=torch.float16)
                >>> pipe = pipe.to("cuda")
        
                >>> # define the prompts
                >>> prompt = "A female reporter is speaking"
                >>> transcript = "wish you have a good day"
        
                >>> # set the seed for generator
                >>> generator = torch.Generator("cuda").manual_seed(0)
        
                >>> # run the generation
                >>> audio = pipe(
                ...     prompt,
                ...     transcription=transcript,
                ...     num_inference_steps=200,
                ...     audio_length_in_s=10.0,
                ...     num_waveforms_per_prompt=2,
                ...     generator=generator,
                ...     max_new_tokens=512,          #Must set max_new_tokens equa to 512 for TTS
                ... ).audios
        
                >>> # save the best audio sample (index 0) as a .wav file
                >>> scipy.io.wavfile.write("tts.wav", rate=16000, data=audio[0])
                ```
        '''
    ),
    "src/diffusers/pipelines/aura_flow/pipeline_aura_flow.py": (
        '''
        Examples:
                ```py
                >>> import torch
                >>> from diffusers import AuraFlowPipeline
        
                >>> pipe = AuraFlowPipeline.from_pretrained("fal/AuraFlow", torch_dtype=torch.float16)
                >>> pipe = pipe.to("cuda")
                >>> prompt = "A cat holding a sign that says hello world"
                >>> image = pipe(prompt).images[0]
                >>> image.save("aura_flow.png")
                ```
        '''
    ),
    "src/diffusers/pipelines/blip_diffusion/pipeline_blip_diffusion.py": (
        '''
        Examples:
                ```py
                >>> from diffusers.pipelines import BlipDiffusionPipeline
                >>> from diffusers.utils import load_image
                >>> import torch
        
                >>> blip_diffusion_pipe = BlipDiffusionPipeline.from_pretrained(
                ...     "Salesforce/blipdiffusion", torch_dtype=torch.float16
                ... ).to("cuda")
        
        
                >>> cond_subject = "dog"
                >>> tgt_subject = "dog"
                >>> text_prompt_input = "swimming underwater"
        
                >>> cond_image = load_image(
                ...     "https://huggingface.co/datasets/ayushtues/blipdiffusion_images/resolve/main/dog.jpg"
                ... )
                >>> guidance_scale = 7.5
                >>> num_inference_steps = 25
                >>> negative_prompt = "over-exposure, under-exposure, saturated, duplicate, out of frame, lowres, cropped, worst quality, low quality, jpeg artifacts, morbid, mutilated, out of frame, ugly, bad anatomy, bad proportions, deformed, blurry, duplicate"
        
        
                >>> output = blip_diffusion_pipe(
                ...     text_prompt_input,
                ...     cond_image,
                ...     cond_subject,
                ...     tgt_subject,
                ...     guidance_scale=guidance_scale,
                ...     num_inference_steps=num_inference_steps,
                ...     neg_prompt=negative_prompt,
                ...     height=512,
                ...     width=512,
                ... ).images
                >>> output[0].save("image.png")
                ```
        '''
    ),
    "src/diffusers/pipelines/bria/pipeline_bria.py": (
        '''
        Examples:
                ```py
                >>> import torch
                >>> from diffusers import BriaPipeline
        
                >>> pipe = BriaPipeline.from_pretrained("briaai/BRIA-3.2", torch_dtype=torch.bfloat16)
                >>> pipe.to("cuda")
                # BRIA's T5 text encoder is sensitive to precision. We need to cast it to bfloat16 and keep the final layer in float32.
        
                >>> pipe.text_encoder = pipe.text_encoder.to(dtype=torch.bfloat16)
                >>> for block in pipe.text_encoder.encoder.block:
                ...     block.layer[-1].DenseReluDense.wo.to(dtype=torch.float32)
                # BRIA's VAE is not supported in mixed precision, so we use float32.
        
                >>> if pipe.vae.config.shift_factor == 0:
                ...     pipe.vae.to(dtype=torch.float32)
        
                >>> prompt = "Photorealistic food photography of a stack of fluffy pancakes on a white plate, with maple syrup being poured over them. On top of the pancakes are the words 'BRIA 3.2' in bold, yellow, 3D letters. The background is dark and out of focus."
                >>> image = pipe(prompt).images[0]
                >>> image.save("bria.png")
                ```
        '''
    ),
    "src/diffusers/pipelines/chroma/pipeline_chroma.py": (
        '''
        Examples:
                ```py
                >>> import torch
                >>> from diffusers import ChromaPipeline
        
                >>> model_id = "lodestones/Chroma"
                >>> ckpt_path = "https://huggingface.co/lodestones/Chroma/blob/main/chroma-unlocked-v37.safetensors"
                >>> transformer = ChromaTransformer2DModel.from_single_file(ckpt_path, torch_dtype=torch.bfloat16)
                >>> pipe = ChromaPipeline.from_pretrained(
                ...     model_id,
                ...     transformer=transformer,
                ...     torch_dtype=torch.bfloat16,
                ... )
                >>> pipe.enable_model_cpu_offload()
                >>> prompt = [
                ...     "A high-fashion close-up portrait of a blonde woman in clear sunglasses. The image uses a bold teal and red color split for dramatic lighting. The background is a simple teal-green. The photo is sharp and well-composed, and is designed for viewing with anaglyph 3D glasses for optimal effect. It looks professionally done."
                ... ]
                >>> negative_prompt = [
                ...     "low quality, ugly, unfinished, out of focus, deformed, disfigure, blurry, smudged, restricted palette, flat colors"
                ... ]
                >>> image = pipe(prompt, negative_prompt=negative_prompt).images[0]
                >>> image.save("chroma.png")
                ```
        '''
    ),
    "src/diffusers/pipelines/chroma/pipeline_chroma_img2img.py": (
        '''
        Examples:
                ```py
                >>> import torch
                >>> from diffusers import ChromaTransformer2DModel, ChromaImg2ImgPipeline
        
                >>> model_id = "lodestones/Chroma"
                >>> ckpt_path = "https://huggingface.co/lodestones/Chroma/blob/main/chroma-unlocked-v37.safetensors"
                >>> pipe = ChromaImg2ImgPipeline.from_pretrained(
                ...     model_id,
                ...     transformer=transformer,
                ...     torch_dtype=torch.bfloat16,
                ... )
                >>> pipe.enable_model_cpu_offload()
                >>> init_image = load_image(
                ...     "https://raw.githubusercontent.com/CompVis/stable-diffusion/main/assets/stable-samples/img2img/sketch-mountains-input.jpg"
                ... )
                >>> prompt = "a scenic fastasy landscape with a river and mountains in the background, vibrant colors, detailed, high resolution"
                >>> negative_prompt = "low quality, ugly, unfinished, out of focus, deformed, disfigure, blurry, smudged, restricted palette, flat colors"
                >>> image = pipe(prompt, image=init_image, negative_prompt=negative_prompt).images[0]
                >>> image.save("chroma-img2img.png")
                ```
        '''
    ),
    "src/diffusers/pipelines/cogvideo/pipeline_cogvideox.py": (
        '''
        Examples:
                ```python
                >>> import torch
                >>> from diffusers import CogVideoXPipeline
                >>> from diffusers.utils import export_to_video
        
                >>> # Models: "THUDM/CogVideoX-2b" or "THUDM/CogVideoX-5b"
                >>> pipe = CogVideoXPipeline.from_pretrained("THUDM/CogVideoX-2b", torch_dtype=torch.float16).to("cuda")
                >>> prompt = (
                ...     "A panda, dressed in a small, red jacket and a tiny hat, sits on a wooden stool in a serene bamboo forest. "
                ...     "The panda's fluffy paws strum a miniature acoustic guitar, producing soft, melodic tunes. Nearby, a few other "
                ...     "pandas gather, watching curiously and some clapping in rhythm. Sunlight filters through the tall bamboo, "
                ...     "casting a gentle glow on the scene. The panda's face is expressive, showing concentration and joy as it plays. "
                ...     "The background includes a small, flowing stream and vibrant green foliage, enhancing the peaceful and magical "
                ...     "atmosphere of this unique musical performance."
                ... )
                >>> video = pipe(prompt=prompt, guidance_scale=6, num_inference_steps=50).frames[0]
                >>> export_to_video(video, "output.mp4", fps=8)
                ```
        '''
    ),
    "src/diffusers/pipelines/cogvideo/pipeline_cogvideox_fun_control.py": (
        '''
        Examples:
                ```python
                >>> import torch
                >>> from diffusers import CogVideoXFunControlPipeline, DDIMScheduler
                >>> from diffusers.utils import export_to_video, load_video
        
                >>> pipe = CogVideoXFunControlPipeline.from_pretrained(
                ...     "alibaba-pai/CogVideoX-Fun-V1.1-5b-Pose", torch_dtype=torch.bfloat16
                ... )
                >>> pipe.scheduler = DDIMScheduler.from_config(pipe.scheduler.config)
                >>> pipe.to("cuda")
        
                >>> control_video = load_video(
                ...     "https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/hiker.mp4"
                ... )
                >>> prompt = (
                ...     "An astronaut stands triumphantly at the peak of a towering mountain. Panorama of rugged peaks and "
                ...     "valleys. Very futuristic vibe and animated aesthetic. Highlights of purple and golden colors in "
                ...     "the scene. The sky is looks like an animated/cartoonish dream of galaxies, nebulae, stars, planets, "
                ...     "moons, but the remainder of the scene is mostly realistic."
                ... )
        
                >>> video = pipe(prompt=prompt, control_video=control_video).frames[0]
                >>> export_to_video(video, "output.mp4", fps=8)
                ```
        '''
    ),
    "src/diffusers/pipelines/cogvideo/pipeline_cogvideox_image2video.py": (
        '''
        Examples:
                ```py
                >>> import torch
                >>> from diffusers import CogVideoXImageToVideoPipeline
                >>> from diffusers.utils import export_to_video, load_image
        
                >>> pipe = CogVideoXImageToVideoPipeline.from_pretrained("THUDM/CogVideoX-5b-I2V", torch_dtype=torch.bfloat16)
                >>> pipe.to("cuda")
        
                >>> prompt = "An astronaut hatching from an egg, on the surface of the moon, the darkness and depth of space realised in the background. High quality, ultrarealistic detail and breath-taking movie-like camera shot."
                >>> image = load_image(
                ...     "https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/astronaut.jpg"
                ... )
                >>> video = pipe(image, prompt, use_dynamic_cfg=True)
                >>> export_to_video(video.frames[0], "output.mp4", fps=8)
                ```
        '''
    ),
    "src/diffusers/pipelines/cogvideo/pipeline_cogvideox_video2video.py": (
        '''
        Examples:
                ```python
                >>> import torch
                >>> from diffusers import CogVideoXDPMScheduler, CogVideoXVideoToVideoPipeline
                >>> from diffusers.utils import export_to_video, load_video
        
                >>> # Models: "THUDM/CogVideoX-2b" or "THUDM/CogVideoX-5b"
                >>> pipe = CogVideoXVideoToVideoPipeline.from_pretrained("THUDM/CogVideoX-5b", torch_dtype=torch.bfloat16)
                >>> pipe.to("cuda")
                >>> pipe.scheduler = CogVideoXDPMScheduler.from_config(pipe.scheduler.config)
        
                >>> input_video = load_video(
                ...     "https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/hiker.mp4"
                ... )
                >>> prompt = (
                ...     "An astronaut stands triumphantly at the peak of a towering mountain. Panorama of rugged peaks and "
                ...     "valleys. Very futuristic vibe and animated aesthetic. Highlights of purple and golden colors in "
                ...     "the scene. The sky is looks like an animated/cartoonish dream of galaxies, nebulae, stars, planets, "
                ...     "moons, but the remainder of the scene is mostly realistic."
                ... )
        
                >>> video = pipe(
                ...     video=input_video, prompt=prompt, strength=0.8, guidance_scale=6, num_inference_steps=50
                ... ).frames[0]
                >>> export_to_video(video, "output.mp4", fps=8)
                ```
        '''
    ),
    "src/diffusers/pipelines/cogview3/pipeline_cogview3plus.py": (
        '''
        Examples:
                ```python
                >>> import torch
                >>> from diffusers import CogView3PlusPipeline
        
                >>> pipe = CogView3PlusPipeline.from_pretrained("THUDM/CogView3-Plus-3B", torch_dtype=torch.bfloat16)
                >>> pipe.to("cuda")
        
                >>> prompt = "A photo of an astronaut riding a horse on mars"
                >>> image = pipe(prompt).images[0]
                >>> image.save("output.png")
                ```
        '''
    ),
    "src/diffusers/pipelines/cogview4/pipeline_cogview4.py": (
        '''
        Examples:
                ```python
                >>> import torch
                >>> from diffusers import CogView4Pipeline
        
                >>> pipe = CogView4Pipeline.from_pretrained("THUDM/CogView4-6B", torch_dtype=torch.bfloat16)
                >>> pipe.to("cuda")
        
                >>> prompt = "A photo of an astronaut riding a horse on mars"
                >>> image = pipe(prompt).images[0]
                >>> image.save("output.png")
                ```
        '''
    ),
    "src/diffusers/pipelines/cogview4/pipeline_cogview4_control.py": (
        '''
        Examples:
                ```python
                >>> import torch
                >>> from diffusers import CogView4ControlPipeline
        
                >>> pipe = CogView4ControlPipeline.from_pretrained("THUDM/CogView4-6B-Control", torch_dtype=torch.bfloat16)
                >>> control_image = load_image(
                ...     "https://huggingface.co/datasets/hf-internal-testing/diffusers-images/resolve/main/sd_controlnet/bird_canny.png"
                ... )
                >>> prompt = "A bird in space"
                >>> image = pipe(prompt, control_image=control_image, height=1024, width=1024, guidance_scale=3.5).images[0]
                >>> image.save("cogview4-control.png")
                ```
        '''
    ),
    "src/diffusers/pipelines/consisid/pipeline_consisid.py": (
        '''
        Examples:
                ```python
                >>> import torch
                >>> from diffusers import ConsisIDPipeline
                >>> from diffusers.pipelines.consisid.consisid_utils import prepare_face_models, process_face_embeddings_infer
                >>> from diffusers.utils import export_to_video
                >>> from huggingface_hub import snapshot_download
        
                >>> snapshot_download(repo_id="BestWishYsh/ConsisID-preview", local_dir="BestWishYsh/ConsisID-preview")
                >>> (
                ...     face_helper_1,
                ...     face_helper_2,
                ...     face_clip_model,
                ...     face_main_model,
                ...     eva_transform_mean,
                ...     eva_transform_std,
                ... ) = prepare_face_models("BestWishYsh/ConsisID-preview", device="cuda", dtype=torch.bfloat16)
                >>> pipe = ConsisIDPipeline.from_pretrained("BestWishYsh/ConsisID-preview", torch_dtype=torch.bfloat16)
                >>> pipe.to("cuda")
        
                >>> # ConsisID works well with long and well-described prompts. Make sure the face in the image is clearly visible (e.g., preferably half-body or full-body).
                >>> prompt = "The video captures a boy walking along a city street, filmed in black and white on a classic 35mm camera. His expression is thoughtful, his brow slightly furrowed as if he's lost in contemplation. The film grain adds a textured, timeless quality to the image, evoking a sense of nostalgia. Around him, the cityscape is filled with vintage buildings, cobblestone sidewalks, and softly blurred figures passing by, their outlines faint and indistinct. Streetlights cast a gentle glow, while shadows play across the boy's path, adding depth to the scene. The lighting highlights the boy's subtle smile, hinting at a fleeting moment of curiosity. The overall cinematic atmosphere, complete with classic film still aesthetics and dramatic contrasts, gives the scene an evocative and introspective feel."
                >>> image = "https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/consisid/consisid_input.png?download=true"
        
                >>> id_cond, id_vit_hidden, image, face_kps = process_face_embeddings_infer(
                ...     face_helper_1,
                ...     face_clip_model,
                ...     face_helper_2,
                ...     eva_transform_mean,
                ...     eva_transform_std,
                ...     face_main_model,
                ...     "cuda",
                ...     torch.bfloat16,
                ...     image,
                ...     is_align_face=True,
                ... )
        
                >>> video = pipe(
                ...     image=image,
                ...     prompt=prompt,
                ...     num_inference_steps=50,
                ...     guidance_scale=6.0,
                ...     use_dynamic_cfg=False,
                ...     id_vit_hidden=id_vit_hidden,
                ...     id_cond=id_cond,
                ...     kps_cond=face_kps,
                ...     generator=torch.Generator("cuda").manual_seed(42),
                ... )
                >>> export_to_video(video.frames[0], "output.mp4", fps=8)
                ```
        '''
    ),
    "src/diffusers/pipelines/consistency_models/pipeline_consistency_models.py": (
        '''
        Examples:
                ```py
                >>> import torch
        
                >>> from diffusers import ConsistencyModelPipeline
        
                >>> device = "cuda"
                >>> # Load the cd_imagenet64_l2 checkpoint.
                >>> model_id_or_path = "openai/diffusers-cd_imagenet64_l2"
                >>> pipe = ConsistencyModelPipeline.from_pretrained(model_id_or_path, torch_dtype=torch.float16)
                >>> pipe.to(device)
        
                >>> # Onestep Sampling
                >>> image = pipe(num_inference_steps=1).images[0]
                >>> image.save("cd_imagenet64_l2_onestep_sample.png")
        
                >>> # Onestep sampling, class-conditional image generation
                >>> # ImageNet-64 class label 145 corresponds to king penguins
                >>> image = pipe(num_inference_steps=1, class_labels=145).images[0]
                >>> image.save("cd_imagenet64_l2_onestep_sample_penguin.png")
        
                >>> # Multistep sampling, class-conditional image generation
                >>> # Timesteps can be explicitly specified; the particular timesteps below are from the original GitHub repo:
                >>> # https://github.com/openai/consistency_models/blob/main/scripts/launch.sh#L77
                >>> image = pipe(num_inference_steps=None, timesteps=[22, 0], class_labels=145).images[0]
                >>> image.save("cd_imagenet64_l2_multistep_sample_penguin.png")
                ```
        '''
    ),
    "src/diffusers/pipelines/controlnet/pipeline_controlnet.py": (
        '''
        Examples:
                ```py
                >>> # !pip install opencv-python transformers accelerate
                >>> from diffusers import StableDiffusionControlNetPipeline, ControlNetModel, UniPCMultistepScheduler
                >>> from diffusers.utils import load_image
                >>> import numpy as np
                >>> import torch
        
                >>> import cv2
                >>> from PIL import Image
        
                >>> # download an image
                >>> image = load_image(
                ...     "https://hf.co/datasets/huggingface/documentation-images/resolve/main/diffusers/input_image_vermeer.png"
                ... )
                >>> image = np.array(image)
        
                >>> # get canny image
                >>> image = cv2.Canny(image, 100, 200)
                >>> image = image[:, :, None]
                >>> image = np.concatenate([image, image, image], axis=2)
                >>> canny_image = Image.fromarray(image)
        
                >>> # load control net and stable diffusion v1-5
                >>> controlnet = ControlNetModel.from_pretrained("lllyasviel/sd-controlnet-canny", torch_dtype=torch.float16)
                >>> pipe = StableDiffusionControlNetPipeline.from_pretrained(
                ...     "stable-diffusion-v1-5/stable-diffusion-v1-5", controlnet=controlnet, torch_dtype=torch.float16
                ... )
        
                >>> # speed up diffusion process with faster scheduler and memory optimization
                >>> pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)
                >>> # remove following line if xformers is not installed
                >>> pipe.enable_xformers_memory_efficient_attention()
        
                >>> pipe.enable_model_cpu_offload()
        
                >>> # generate image
                >>> generator = torch.manual_seed(0)
                >>> image = pipe(
                ...     "futuristic-looking woman", num_inference_steps=20, generator=generator, image=canny_image
                ... ).images[0]
                ```
        '''
    ),
    "src/diffusers/pipelines/controlnet/pipeline_controlnet_blip_diffusion.py": (
        '''
        Examples:
                ```py
                >>> from diffusers.pipelines import BlipDiffusionControlNetPipeline
                >>> from diffusers.utils import load_image
                >>> from controlnet_aux import CannyDetector
                >>> import torch
        
                >>> blip_diffusion_pipe = BlipDiffusionControlNetPipeline.from_pretrained(
                ...     "Salesforce/blipdiffusion-controlnet", torch_dtype=torch.float16
                ... ).to("cuda")
        
                >>> style_subject = "flower"
                >>> tgt_subject = "teapot"
                >>> text_prompt = "on a marble table"
        
                >>> cldm_cond_image = load_image(
                ...     "https://huggingface.co/datasets/ayushtues/blipdiffusion_images/resolve/main/kettle.jpg"
                ... ).resize((512, 512))
                >>> canny = CannyDetector()
                >>> cldm_cond_image = canny(cldm_cond_image, 30, 70, output_type="pil")
                >>> style_image = load_image(
                ...     "https://huggingface.co/datasets/ayushtues/blipdiffusion_images/resolve/main/flower.jpg"
                ... )
                >>> guidance_scale = 7.5
                >>> num_inference_steps = 50
                >>> negative_prompt = "over-exposure, under-exposure, saturated, duplicate, out of frame, lowres, cropped, worst quality, low quality, jpeg artifacts, morbid, mutilated, out of frame, ugly, bad anatomy, bad proportions, deformed, blurry, duplicate"
        
        
                >>> output = blip_diffusion_pipe(
                ...     text_prompt,
                ...     style_image,
                ...     cldm_cond_image,
                ...     style_subject,
                ...     tgt_subject,
                ...     guidance_scale=guidance_scale,
                ...     num_inference_steps=num_inference_steps,
                ...     neg_prompt=negative_prompt,
                ...     height=512,
                ...     width=512,
                ... ).images
                >>> output[0].save("image.png")
                ```
        '''
    ),
    "src/diffusers/pipelines/controlnet/pipeline_controlnet_img2img.py": (
        '''
        Examples:
                ```py
                >>> # !pip install opencv-python transformers accelerate
                >>> from diffusers import StableDiffusionControlNetImg2ImgPipeline, ControlNetModel, UniPCMultistepScheduler
                >>> from diffusers.utils import load_image
                >>> import numpy as np
                >>> import torch
        
                >>> import cv2
                >>> from PIL import Image
        
                >>> # download an image
                >>> image = load_image(
                ...     "https://hf.co/datasets/huggingface/documentation-images/resolve/main/diffusers/input_image_vermeer.png"
                ... )
                >>> np_image = np.array(image)
        
                >>> # get canny image
                >>> np_image = cv2.Canny(np_image, 100, 200)
                >>> np_image = np_image[:, :, None]
                >>> np_image = np.concatenate([np_image, np_image, np_image], axis=2)
                >>> canny_image = Image.fromarray(np_image)
        
                >>> # load control net and stable diffusion v1-5
                >>> controlnet = ControlNetModel.from_pretrained("lllyasviel/sd-controlnet-canny", torch_dtype=torch.float16)
                >>> pipe = StableDiffusionControlNetImg2ImgPipeline.from_pretrained(
                ...     "stable-diffusion-v1-5/stable-diffusion-v1-5", controlnet=controlnet, torch_dtype=torch.float16
                ... )
        
                >>> # speed up diffusion process with faster scheduler and memory optimization
                >>> pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)
                >>> pipe.enable_model_cpu_offload()
        
                >>> # generate image
                >>> generator = torch.manual_seed(0)
                >>> image = pipe(
                ...     "futuristic-looking woman",
                ...     num_inference_steps=20,
                ...     generator=generator,
                ...     image=image,
                ...     control_image=canny_image,
                ... ).images[0]
                ```
        '''
    ),
    "src/diffusers/pipelines/controlnet/pipeline_controlnet_inpaint.py": (
        '''
        Examples:
                ```py
                >>> # !pip install transformers accelerate
                >>> from diffusers import StableDiffusionControlNetInpaintPipeline, ControlNetModel, DDIMScheduler
                >>> from diffusers.utils import load_image
                >>> import numpy as np
                >>> import torch
        
                >>> init_image = load_image(
                ...     "https://huggingface.co/datasets/diffusers/test-arrays/resolve/main/stable_diffusion_inpaint/boy.png"
                ... )
                >>> init_image = init_image.resize((512, 512))
        
                >>> generator = torch.Generator(device="cpu").manual_seed(1)
        
                >>> mask_image = load_image(
                ...     "https://huggingface.co/datasets/diffusers/test-arrays/resolve/main/stable_diffusion_inpaint/boy_mask.png"
                ... )
                >>> mask_image = mask_image.resize((512, 512))
        
        
                >>> def make_canny_condition(image):
                ...     image = np.array(image)
                ...     image = cv2.Canny(image, 100, 200)
                ...     image = image[:, :, None]
                ...     image = np.concatenate([image, image, image], axis=2)
                ...     image = Image.fromarray(image)
                ...     return image
        
        
                >>> control_image = make_canny_condition(init_image)
        
                >>> controlnet = ControlNetModel.from_pretrained(
                ...     "lllyasviel/control_v11p_sd15_inpaint", torch_dtype=torch.float16
                ... )
                >>> pipe = StableDiffusionControlNetInpaintPipeline.from_pretrained(
                ...     "stable-diffusion-v1-5/stable-diffusion-v1-5", controlnet=controlnet, torch_dtype=torch.float16
                ... )
        
                >>> pipe.scheduler = DDIMScheduler.from_config(pipe.scheduler.config)
                >>> pipe.enable_model_cpu_offload()
        
                >>> # generate image
                >>> image = pipe(
                ...     "a handsome man with ray-ban sunglasses",
                ...     num_inference_steps=20,
                ...     generator=generator,
                ...     eta=1.0,
                ...     image=init_image,
                ...     mask_image=mask_image,
                ...     control_image=control_image,
                ... ).images[0]
                ```
        '''
    ),
    "src/diffusers/pipelines/controlnet/pipeline_controlnet_inpaint_sd_xl.py": (
        '''
        Examples:
                ```py
                >>> # !pip install transformers accelerate
                >>> from diffusers import StableDiffusionXLControlNetInpaintPipeline, ControlNetModel, DDIMScheduler
                >>> from diffusers.utils import load_image
                >>> from PIL import Image
                >>> import numpy as np
                >>> import torch
        
                >>> init_image = load_image(
                ...     "https://huggingface.co/datasets/diffusers/test-arrays/resolve/main/stable_diffusion_inpaint/boy.png"
                ... )
                >>> init_image = init_image.resize((1024, 1024))
        
                >>> generator = torch.Generator(device="cpu").manual_seed(1)
        
                >>> mask_image = load_image(
                ...     "https://huggingface.co/datasets/diffusers/test-arrays/resolve/main/stable_diffusion_inpaint/boy_mask.png"
                ... )
                >>> mask_image = mask_image.resize((1024, 1024))
        
        
                >>> def make_canny_condition(image):
                ...     image = np.array(image)
                ...     image = cv2.Canny(image, 100, 200)
                ...     image = image[:, :, None]
                ...     image = np.concatenate([image, image, image], axis=2)
                ...     image = Image.fromarray(image)
                ...     return image
        
        
                >>> control_image = make_canny_condition(init_image)
        
                >>> controlnet = ControlNetModel.from_pretrained(
                ...     "diffusers/controlnet-canny-sdxl-1.0", torch_dtype=torch.float16
                ... )
                >>> pipe = StableDiffusionXLControlNetInpaintPipeline.from_pretrained(
                ...     "stabilityai/stable-diffusion-xl-base-1.0", controlnet=controlnet, torch_dtype=torch.float16
                ... )
        
                >>> pipe.enable_model_cpu_offload()
        
                >>> # generate image
                >>> image = pipe(
                ...     "a handsome man with ray-ban sunglasses",
                ...     num_inference_steps=20,
                ...     generator=generator,
                ...     eta=1.0,
                ...     image=init_image,
                ...     mask_image=mask_image,
                ...     control_image=control_image,
                ... ).images[0]
                ```
        '''
    ),
    "src/diffusers/pipelines/controlnet/pipeline_controlnet_sd_xl.py": (
        '''
        Examples:
                ```py
                >>> # !pip install opencv-python transformers accelerate
                >>> from diffusers import StableDiffusionXLControlNetPipeline, ControlNetModel, AutoencoderKL
                >>> from diffusers.utils import load_image
                >>> import numpy as np
                >>> import torch
        
                >>> import cv2
                >>> from PIL import Image
        
                >>> prompt = "aerial view, a futuristic research complex in a bright foggy jungle, hard lighting"
                >>> negative_prompt = "low quality, bad quality, sketches"
        
                >>> # download an image
                >>> image = load_image(
                ...     "https://hf.co/datasets/hf-internal-testing/diffusers-images/resolve/main/sd_controlnet/hf-logo.png"
                ... )
        
                >>> # initialize the models and pipeline
                >>> controlnet_conditioning_scale = 0.5  # recommended for good generalization
                >>> controlnet = ControlNetModel.from_pretrained(
                ...     "diffusers/controlnet-canny-sdxl-1.0", torch_dtype=torch.float16
                ... )
                >>> vae = AutoencoderKL.from_pretrained("madebyollin/sdxl-vae-fp16-fix", torch_dtype=torch.float16)
                >>> pipe = StableDiffusionXLControlNetPipeline.from_pretrained(
                ...     "stabilityai/stable-diffusion-xl-base-1.0", controlnet=controlnet, vae=vae, torch_dtype=torch.float16
                ... )
                >>> pipe.enable_model_cpu_offload()
        
                >>> # get canny image
                >>> image = np.array(image)
                >>> image = cv2.Canny(image, 100, 200)
                >>> image = image[:, :, None]
                >>> image = np.concatenate([image, image, image], axis=2)
                >>> canny_image = Image.fromarray(image)
        
                >>> # generate image
                >>> image = pipe(
                ...     prompt, controlnet_conditioning_scale=controlnet_conditioning_scale, image=canny_image
                ... ).images[0]
                ```
        '''
    ),
    "src/diffusers/pipelines/controlnet/pipeline_controlnet_sd_xl_img2img.py": (
        '''
        Examples:
                ```py
                >>> # pip install accelerate transformers safetensors diffusers
        
                >>> import torch
                >>> import numpy as np
                >>> from PIL import Image
        
                >>> from transformers import DPTImageProcessor, DPTForDepthEstimation
                >>> from diffusers import ControlNetModel, StableDiffusionXLControlNetImg2ImgPipeline, AutoencoderKL
                >>> from diffusers.utils import load_image
        
        
                >>> depth_estimator = DPTForDepthEstimation.from_pretrained("Intel/dpt-hybrid-midas").to("cuda")
                >>> feature_extractor = DPTImageProcessor.from_pretrained("Intel/dpt-hybrid-midas")
                >>> controlnet = ControlNetModel.from_pretrained(
                ...     "diffusers/controlnet-depth-sdxl-1.0-small",
                ...     variant="fp16",
                ...     use_safetensors=True,
                ...     torch_dtype=torch.float16,
                ... )
                >>> vae = AutoencoderKL.from_pretrained("madebyollin/sdxl-vae-fp16-fix", torch_dtype=torch.float16)
                >>> pipe = StableDiffusionXLControlNetImg2ImgPipeline.from_pretrained(
                ...     "stabilityai/stable-diffusion-xl-base-1.0",
                ...     controlnet=controlnet,
                ...     vae=vae,
                ...     variant="fp16",
                ...     use_safetensors=True,
                ...     torch_dtype=torch.float16,
                ... )
                >>> pipe.enable_model_cpu_offload()
        
        
                >>> def get_depth_map(image):
                ...     image = feature_extractor(images=image, return_tensors="pt").pixel_values.to("cuda")
                ...     with torch.no_grad(), torch.autocast("cuda"):
                ...         depth_map = depth_estimator(image).predicted_depth
        
                ...     depth_map = torch.nn.functional.interpolate(
                ...         depth_map.unsqueeze(1),
                ...         size=(1024, 1024),
                ...         mode="bicubic",
                ...         align_corners=False,
                ...     )
                ...     depth_min = torch.amin(depth_map, dim=[1, 2, 3], keepdim=True)
                ...     depth_max = torch.amax(depth_map, dim=[1, 2, 3], keepdim=True)
                ...     depth_map = (depth_map - depth_min) / (depth_max - depth_min)
                ...     image = torch.cat([depth_map] * 3, dim=1)
                ...     image = image.permute(0, 2, 3, 1).cpu().numpy()[0]
                ...     image = Image.fromarray((image * 255.0).clip(0, 255).astype(np.uint8))
                ...     return image
        
        
                >>> prompt = "A robot, 4k photo"
                >>> image = load_image(
                ...     "https://huggingface.co/datasets/hf-internal-testing/diffusers-images/resolve/main"
                ...     "/kandinsky/cat.png"
                ... ).resize((1024, 1024))
                >>> controlnet_conditioning_scale = 0.5  # recommended for good generalization
                >>> depth_image = get_depth_map(image)
        
                >>> images = pipe(
                ...     prompt,
                ...     image=image,
                ...     control_image=depth_image,
                ...     strength=0.99,
                ...     num_inference_steps=50,
                ...     controlnet_conditioning_scale=controlnet_conditioning_scale,
                ... ).images
                >>> images[0].save(f"robot_cat.png")
                ```
        '''
    ),
    "src/diffusers/pipelines/controlnet/pipeline_controlnet_union_inpaint_sd_xl.py": (
        '''
        Examples:
                ```py
                from diffusers import StableDiffusionXLControlNetUnionInpaintPipeline, ControlNetUnionModel, AutoencoderKL
                from diffusers.utils import load_image
                import torch
                import numpy as np
                from PIL import Image
        
                prompt = "A cat"
                # download an image
                image = load_image(
                    "https://huggingface.co/datasets/hf-internal-testing/diffusers-images/resolve/main/in_paint/overture-creations-5sI6fQgYIuo.png"
                ).resize((1024, 1024))
                mask = load_image(
                    "https://huggingface.co/datasets/hf-internal-testing/diffusers-images/resolve/main/in_paint/overture-creations-5sI6fQgYIuo_mask.png"
                ).resize((1024, 1024))
                # initialize the models and pipeline
                controlnet = ControlNetUnionModel.from_pretrained(
                    "brad-twinkl/controlnet-union-sdxl-1.0-promax", torch_dtype=torch.float16
                )
                vae = AutoencoderKL.from_pretrained("madebyollin/sdxl-vae-fp16-fix", torch_dtype=torch.float16)
                pipe = StableDiffusionXLControlNetUnionInpaintPipeline.from_pretrained(
                    "stabilityai/stable-diffusion-xl-base-1.0",
                    controlnet=controlnet,
                    vae=vae,
                    torch_dtype=torch.float16,
                    variant="fp16",
                )
                pipe.enable_model_cpu_offload()
                controlnet_img = image.copy()
                controlnet_img_np = np.array(controlnet_img)
                mask_np = np.array(mask)
                controlnet_img_np[mask_np > 0] = 0
                controlnet_img = Image.fromarray(controlnet_img_np)
                # generate image
                image = pipe(prompt, image=image, mask_image=mask, control_image=[controlnet_img], control_mode=[7]).images[0]
                image.save("inpaint.png")
                ```
        '''
    ),
    "src/diffusers/pipelines/controlnet/pipeline_controlnet_union_sd_xl.py": (
        '''
        Examples:
                ```py
                >>> # !pip install controlnet_aux
                >>> from controlnet_aux import LineartAnimeDetector
                >>> from diffusers import StableDiffusionXLControlNetUnionPipeline, ControlNetUnionModel, AutoencoderKL
                >>> from diffusers.utils import load_image
                >>> import torch
        
                >>> prompt = "A cat"
                >>> # download an image
                >>> image = load_image(
                ...     "https://huggingface.co/datasets/hf-internal-testing/diffusers-images/resolve/main/kandinsky/cat.png"
                ... ).resize((1024, 1024))
                >>> # initialize the models and pipeline
                >>> controlnet = ControlNetUnionModel.from_pretrained(
                ...     "xinsir/controlnet-union-sdxl-1.0", torch_dtype=torch.float16
                ... )
                >>> vae = AutoencoderKL.from_pretrained("madebyollin/sdxl-vae-fp16-fix", torch_dtype=torch.float16)
                >>> pipe = StableDiffusionXLControlNetUnionPipeline.from_pretrained(
                ...     "stabilityai/stable-diffusion-xl-base-1.0",
                ...     controlnet=controlnet,
                ...     vae=vae,
                ...     torch_dtype=torch.float16,
                ...     variant="fp16",
                ... )
                >>> pipe.enable_model_cpu_offload()
                >>> # prepare image
                >>> processor = LineartAnimeDetector.from_pretrained("lllyasviel/Annotators")
                >>> controlnet_img = processor(image, output_type="pil")
                >>> # generate image
                >>> image = pipe(prompt, control_image=[controlnet_img], control_mode=[3], height=1024, width=1024).images[0]
                ```
        '''
    ),
    "src/diffusers/pipelines/controlnet/pipeline_controlnet_union_sd_xl_img2img.py": (
        '''
        Examples:
                ```py
                # !pip install controlnet_aux
                from diffusers import (
                    StableDiffusionXLControlNetUnionImg2ImgPipeline,
                    ControlNetUnionModel,
                    AutoencoderKL,
                )
                from diffusers.utils import load_image
                import torch
                from PIL import Image
                import numpy as np
        
                prompt = "A cat"
                # download an image
                image = load_image(
                    "https://huggingface.co/datasets/hf-internal-testing/diffusers-images/resolve/main/kandinsky/cat.png"
                )
                # initialize the models and pipeline
                controlnet = ControlNetUnionModel.from_pretrained(
                    "brad-twinkl/controlnet-union-sdxl-1.0-promax", torch_dtype=torch.float16
                )
                vae = AutoencoderKL.from_pretrained("madebyollin/sdxl-vae-fp16-fix", torch_dtype=torch.float16)
                pipe = StableDiffusionXLControlNetUnionImg2ImgPipeline.from_pretrained(
                    "stabilityai/stable-diffusion-xl-base-1.0",
                    controlnet=controlnet,
                    vae=vae,
                    torch_dtype=torch.float16,
                    variant="fp16",
                ).to("cuda")
                # `enable_model_cpu_offload` is not recommended due to multiple generations
                height = image.height
                width = image.width
                ratio = np.sqrt(1024.0 * 1024.0 / (width * height))
                # 3 * 3 upscale correspond to 16 * 3 multiply, 2 * 2 correspond to 16 * 2 multiply and so on.
                scale_image_factor = 3
                base_factor = 16
                factor = scale_image_factor * base_factor
                W, H = int(width * ratio) // factor * factor, int(height * ratio) // factor * factor
                image = image.resize((W, H))
                target_width = W // scale_image_factor
                target_height = H // scale_image_factor
                images = []
                crops_coords_list = [
                    (0, 0),
                    (0, width // 2),
                    (height // 2, 0),
                    (width // 2, height // 2),
                    0,
                    0,
                    0,
                    0,
                    0,
                ]
                for i in range(scale_image_factor):
                    for j in range(scale_image_factor):
                        left = j * target_width
                        top = i * target_height
                        right = left + target_width
                        bottom = top + target_height
                        cropped_image = image.crop((left, top, right, bottom))
                        cropped_image = cropped_image.resize((W, H))
                        images.append(cropped_image)
                # set ControlNetUnion input
                result_images = []
                for sub_img, crops_coords in zip(images, crops_coords_list):
                    new_width, new_height = W, H
                    out = pipe(
                        prompt=[prompt] * 1,
                        image=sub_img,
                        control_image=[sub_img],
                        control_mode=[6],
                        width=new_width,
                        height=new_height,
                        num_inference_steps=30,
                        crops_coords_top_left=(W, H),
                        target_size=(W, H),
                        original_size=(W * 2, H * 2),
                    )
                    result_images.append(out.images[0])
                new_im = Image.new("RGB", (new_width * scale_image_factor, new_height * scale_image_factor))
                new_im.paste(result_images[0], (0, 0))
                new_im.paste(result_images[1], (new_width, 0))
                new_im.paste(result_images[2], (new_width * 2, 0))
                new_im.paste(result_images[3], (0, new_height))
                new_im.paste(result_images[4], (new_width, new_height))
                new_im.paste(result_images[5], (new_width * 2, new_height))
                new_im.paste(result_images[6], (0, new_height * 2))
                new_im.paste(result_images[7], (new_width, new_height * 2))
                new_im.paste(result_images[8], (new_width * 2, new_height * 2))
                ```
        '''
    ),
    "src/diffusers/pipelines/controlnet/pipeline_flax_controlnet.py": (
        '''
        Examples:
                ```py
                >>> import jax
                >>> import numpy as np
                >>> import jax.numpy as jnp
                >>> from flax.jax_utils import replicate
                >>> from flax.training.common_utils import shard
                >>> from diffusers.utils import load_image, make_image_grid
                >>> from PIL import Image
                >>> from diffusers import FlaxStableDiffusionControlNetPipeline, FlaxControlNetModel
        
        
                >>> def create_key(seed=0):
                ...     return jax.random.PRNGKey(seed)
        
        
                >>> rng = create_key(0)
        
                >>> # get canny image
                >>> canny_image = load_image(
                ...     "https://huggingface.co/datasets/YiYiXu/test-doc-assets/resolve/main/blog_post_cell_10_output_0.jpeg"
                ... )
        
                >>> prompts = "best quality, extremely detailed"
                >>> negative_prompts = "monochrome, lowres, bad anatomy, worst quality, low quality"
        
                >>> # load control net and stable diffusion v1-5
                >>> controlnet, controlnet_params = FlaxControlNetModel.from_pretrained(
                ...     "lllyasviel/sd-controlnet-canny", from_pt=True, dtype=jnp.float32
                ... )
                >>> pipe, params = FlaxStableDiffusionControlNetPipeline.from_pretrained(
                ...     "stable-diffusion-v1-5/stable-diffusion-v1-5",
                ...     controlnet=controlnet,
                ...     revision="flax",
                ...     dtype=jnp.float32,
                ... )
                >>> params["controlnet"] = controlnet_params
        
                >>> num_samples = jax.device_count()
                >>> rng = jax.random.split(rng, jax.device_count())
        
                >>> prompt_ids = pipe.prepare_text_inputs([prompts] * num_samples)
                >>> negative_prompt_ids = pipe.prepare_text_inputs([negative_prompts] * num_samples)
                >>> processed_image = pipe.prepare_image_inputs([canny_image] * num_samples)
        
                >>> p_params = replicate(params)
                >>> prompt_ids = shard(prompt_ids)
                >>> negative_prompt_ids = shard(negative_prompt_ids)
                >>> processed_image = shard(processed_image)
        
                >>> output = pipe(
                ...     prompt_ids=prompt_ids,
                ...     image=processed_image,
                ...     params=p_params,
                ...     prng_seed=rng,
                ...     num_inference_steps=50,
                ...     neg_prompt_ids=negative_prompt_ids,
                ...     jit=True,
                ... ).images
        
                >>> output_images = pipe.numpy_to_pil(np.asarray(output.reshape((num_samples,) + output.shape[-3:])))
                >>> output_images = make_image_grid(output_images, num_samples // 4, 4)
                >>> output_images.save("generated_image.png")
                ```
        '''
    ),
    "src/diffusers/pipelines/controlnet_hunyuandit/pipeline_hunyuandit_controlnet.py": (
        '''
        Examples:
                ```py
                from diffusers import HunyuanDiT2DControlNetModel, HunyuanDiTControlNetPipeline
                import torch
        
                controlnet = HunyuanDiT2DControlNetModel.from_pretrained(
                    "Tencent-Hunyuan/HunyuanDiT-v1.1-ControlNet-Diffusers-Canny", torch_dtype=torch.float16
                )
        
                pipe = HunyuanDiTControlNetPipeline.from_pretrained(
                    "Tencent-Hunyuan/HunyuanDiT-v1.1-Diffusers", controlnet=controlnet, torch_dtype=torch.float16
                )
                pipe.to("cuda")
        
                from diffusers.utils import load_image
        
                cond_image = load_image(
                    "https://huggingface.co/Tencent-Hunyuan/HunyuanDiT-v1.1-ControlNet-Diffusers-Canny/resolve/main/canny.jpg?download=true"
                )
        
                ## You may also use English prompt as HunyuanDiT supports both English and Chinese
                prompt = "在夜晚的酒店门前，一座古老的中国风格的狮子雕像矗立着，它的眼睛闪烁着光芒，仿佛在守护着这座建筑。背景是夜晚的酒店前，构图方式是特写，平视，居中构图。这张照片呈现了真实摄影风格，蕴含了中国雕塑文化，同时展现了神秘氛围"
                # prompt="At night, an ancient Chinese-style lion statue stands in front of the hotel, its eyes gleaming as if guarding the building. The background is the hotel entrance at night, with a close-up, eye-level, and centered composition. This photo presents a realistic photographic style, embodies Chinese sculpture culture, and reveals a mysterious atmosphere."
                image = pipe(
                    prompt,
                    height=1024,
                    width=1024,
                    control_image=cond_image,
                    num_inference_steps=50,
                ).images[0]
                ```
        '''
    ),
    "src/diffusers/pipelines/controlnet_sd3/pipeline_stable_diffusion_3_controlnet.py": (
        '''
        Examples:
                ```py
                >>> import torch
                >>> from diffusers import StableDiffusion3ControlNetPipeline
                >>> from diffusers.models import SD3ControlNetModel, SD3MultiControlNetModel
                >>> from diffusers.utils import load_image
        
                >>> controlnet = SD3ControlNetModel.from_pretrained("InstantX/SD3-Controlnet-Canny", torch_dtype=torch.float16)
        
                >>> pipe = StableDiffusion3ControlNetPipeline.from_pretrained(
                ...     "stabilityai/stable-diffusion-3-medium-diffusers", controlnet=controlnet, torch_dtype=torch.float16
                ... )
                >>> pipe.to("cuda")
                >>> control_image = load_image(
                ...     "https://huggingface.co/datasets/hf-internal-testing/diffusers-images/resolve/main/sd_controlnet/bird_canny.png"
                ... )
                >>> prompt = "A bird in space"
                >>> image = pipe(
                ...     prompt, control_image=control_image, height=1024, width=768, controlnet_conditioning_scale=0.7
                ... ).images[0]
                >>> image.save("sd3.png")
                ```
        '''
    ),
    "src/diffusers/pipelines/controlnet_sd3/pipeline_stable_diffusion_3_controlnet_inpainting.py": (
        '''
        Examples:
                ```py
                >>> import torch
                >>> from diffusers.utils import load_image, check_min_version
                >>> from diffusers.pipelines import StableDiffusion3ControlNetInpaintingPipeline
                >>> from diffusers.models.controlnet_sd3 import SD3ControlNetModel
        
                >>> controlnet = SD3ControlNetModel.from_pretrained(
                ...     "alimama-creative/SD3-Controlnet-Inpainting", use_safetensors=True, extra_conditioning_channels=1
                ... )
                >>> pipe = StableDiffusion3ControlNetInpaintingPipeline.from_pretrained(
                ...     "stabilityai/stable-diffusion-3-medium-diffusers",
                ...     controlnet=controlnet,
                ...     torch_dtype=torch.float16,
                ... )
                >>> pipe.text_encoder.to(torch.float16)
                >>> pipe.controlnet.to(torch.float16)
                >>> pipe.to("cuda")
        
                >>> image = load_image(
                ...     "https://huggingface.co/alimama-creative/SD3-Controlnet-Inpainting/resolve/main/images/dog.png"
                ... )
                >>> mask = load_image(
                ...     "https://huggingface.co/alimama-creative/SD3-Controlnet-Inpainting/resolve/main/images/dog_mask.png"
                ... )
                >>> width = 1024
                >>> height = 1024
                >>> prompt = "A cat is sitting next to a puppy."
                >>> generator = torch.Generator(device="cuda").manual_seed(24)
                >>> res_image = pipe(
                ...     negative_prompt="deformed, distorted, disfigured, poorly drawn, bad anatomy, wrong anatomy, extra limb, missing limb, floating limbs, mutated hands and fingers, disconnected limbs, mutation, mutated, ugly, disgusting, blurry, amputation, NSFW",
                ...     prompt=prompt,
                ...     height=height,
                ...     width=width,
                ...     control_image=image,
                ...     control_mask=mask,
                ...     num_inference_steps=28,
                ...     generator=generator,
                ...     controlnet_conditioning_scale=0.95,
                ...     guidance_scale=7,
                ... ).images[0]
                >>> res_image.save(f"sd3.png")
                ```
        '''
    ),
    "src/diffusers/pipelines/controlnet_xs/pipeline_controlnet_xs.py": (
        '''
        Examples:
                ```py
                >>> # !pip install opencv-python transformers accelerate
                >>> from diffusers import StableDiffusionControlNetXSPipeline, ControlNetXSAdapter
                >>> from diffusers.utils import load_image
                >>> import numpy as np
                >>> import torch
        
                >>> import cv2
                >>> from PIL import Image
        
                >>> prompt = "aerial view, a futuristic research complex in a bright foggy jungle, hard lighting"
                >>> negative_prompt = "low quality, bad quality, sketches"
        
                >>> # download an image
                >>> image = load_image(
                ...     "https://hf.co/datasets/hf-internal-testing/diffusers-images/resolve/main/sd_controlnet/hf-logo.png"
                ... )
        
                >>> # initialize the models and pipeline
                >>> controlnet_conditioning_scale = 0.5
        
                >>> controlnet = ControlNetXSAdapter.from_pretrained(
                ...     "UmerHA/Testing-ConrolNetXS-SD2.1-canny", torch_dtype=torch.float16
                ... )
                >>> pipe = StableDiffusionControlNetXSPipeline.from_pretrained(
                ...     "stabilityai/stable-diffusion-2-1-base", controlnet=controlnet, torch_dtype=torch.float16
                ... )
                >>> pipe.enable_model_cpu_offload()
        
                >>> # get canny image
                >>> image = np.array(image)
                >>> image = cv2.Canny(image, 100, 200)
                >>> image = image[:, :, None]
                >>> image = np.concatenate([image, image, image], axis=2)
                >>> canny_image = Image.fromarray(image)
                >>> # generate image
                >>> image = pipe(
                ...     prompt, controlnet_conditioning_scale=controlnet_conditioning_scale, image=canny_image
                ... ).images[0]
                ```
        '''
    ),
    "src/diffusers/pipelines/controlnet_xs/pipeline_controlnet_xs_sd_xl.py": (
        '''
        Examples:
                ```py
                >>> # !pip install opencv-python transformers accelerate
                >>> from diffusers import StableDiffusionXLControlNetXSPipeline, ControlNetXSAdapter, AutoencoderKL
                >>> from diffusers.utils import load_image
                >>> import numpy as np
                >>> import torch
        
                >>> import cv2
                >>> from PIL import Image
        
                >>> prompt = "aerial view, a futuristic research complex in a bright foggy jungle, hard lighting"
                >>> negative_prompt = "low quality, bad quality, sketches"
        
                >>> # download an image
                >>> image = load_image(
                ...     "https://hf.co/datasets/hf-internal-testing/diffusers-images/resolve/main/sd_controlnet/hf-logo.png"
                ... )
        
                >>> # initialize the models and pipeline
                >>> controlnet_conditioning_scale = 0.5
                >>> vae = AutoencoderKL.from_pretrained("madebyollin/sdxl-vae-fp16-fix", torch_dtype=torch.float16)
                >>> controlnet = ControlNetXSAdapter.from_pretrained(
                ...     "UmerHA/Testing-ConrolNetXS-SDXL-canny", torch_dtype=torch.float16
                ... )
                >>> pipe = StableDiffusionXLControlNetXSPipeline.from_pretrained(
                ...     "stabilityai/stable-diffusion-xl-base-1.0", controlnet=controlnet, torch_dtype=torch.float16
                ... )
                >>> pipe.enable_model_cpu_offload()
        
                >>> # get canny image
                >>> image = np.array(image)
                >>> image = cv2.Canny(image, 100, 200)
                >>> image = image[:, :, None]
                >>> image = np.concatenate([image, image, image], axis=2)
                >>> canny_image = Image.fromarray(image)
        
                >>> # generate image
                >>> image = pipe(
                ...     prompt, controlnet_conditioning_scale=controlnet_conditioning_scale, image=canny_image
                ... ).images[0]
                ```
        '''
    ),
    "src/diffusers/pipelines/cosmos/pipeline_cosmos2_text2image.py": (
        '''
        Examples:
                ```python
                >>> import torch
                >>> from diffusers import Cosmos2TextToImagePipeline
        
                >>> # Available checkpoints: nvidia/Cosmos-Predict2-2B-Text2Image, nvidia/Cosmos-Predict2-14B-Text2Image
                >>> model_id = "nvidia/Cosmos-Predict2-2B-Text2Image"
                >>> pipe = Cosmos2TextToImagePipeline.from_pretrained(model_id, torch_dtype=torch.bfloat16)
                >>> pipe.to("cuda")
        
                >>> prompt = "A close-up shot captures a vibrant yellow scrubber vigorously working on a grimy plate, its bristles moving in circular motions to lift stubborn grease and food residue. The dish, once covered in remnants of a hearty meal, gradually reveals its original glossy surface. Suds form and bubble around the scrubber, creating a satisfying visual of cleanliness in progress. The sound of scrubbing fills the air, accompanied by the gentle clinking of the dish against the sink. As the scrubber continues its task, the dish transforms, gleaming under the bright kitchen lights, symbolizing the triumph of cleanliness over mess."
                >>> negative_prompt = "The video captures a series of frames showing ugly scenes, static with no motion, motion blur, over-saturation, shaky footage, low resolution, grainy texture, pixelated images, poorly lit areas, underexposed and overexposed scenes, poor color balance, washed out colors, choppy sequences, jerky movements, low frame rate, artifacting, color banding, unnatural transitions, outdated special effects, fake elements, unconvincing visuals, poorly edited content, jump cuts, visual noise, and flickering. Overall, the video is of poor quality."
        
                >>> output = pipe(
                ...     prompt=prompt, negative_prompt=negative_prompt, generator=torch.Generator().manual_seed(1)
                ... ).images[0]
                >>> output.save("output.png")
                ```
        '''
    ),
    "src/diffusers/pipelines/cosmos/pipeline_cosmos2_video2world.py": (
        '''
        Examples:
                ```python
                >>> import torch
                >>> from diffusers import Cosmos2VideoToWorldPipeline
                >>> from diffusers.utils import export_to_video, load_image
        
                >>> # Available checkpoints: nvidia/Cosmos-Predict2-2B-Video2World, nvidia/Cosmos-Predict2-14B-Video2World
                >>> model_id = "nvidia/Cosmos-Predict2-2B-Video2World"
                >>> pipe = Cosmos2VideoToWorldPipeline.from_pretrained(model_id, torch_dtype=torch.bfloat16)
                >>> pipe.to("cuda")
        
                >>> prompt = "A close-up shot captures a vibrant yellow scrubber vigorously working on a grimy plate, its bristles moving in circular motions to lift stubborn grease and food residue. The dish, once covered in remnants of a hearty meal, gradually reveals its original glossy surface. Suds form and bubble around the scrubber, creating a satisfying visual of cleanliness in progress. The sound of scrubbing fills the air, accompanied by the gentle clinking of the dish against the sink. As the scrubber continues its task, the dish transforms, gleaming under the bright kitchen lights, symbolizing the triumph of cleanliness over mess."
                >>> negative_prompt = "The video captures a series of frames showing ugly scenes, static with no motion, motion blur, over-saturation, shaky footage, low resolution, grainy texture, pixelated images, poorly lit areas, underexposed and overexposed scenes, poor color balance, washed out colors, choppy sequences, jerky movements, low frame rate, artifacting, color banding, unnatural transitions, outdated special effects, fake elements, unconvincing visuals, poorly edited content, jump cuts, visual noise, and flickering. Overall, the video is of poor quality."
                >>> image = load_image(
                ...     "https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/yellow-scrubber.png"
                ... )
        
                >>> video = pipe(
                ...     image=image, prompt=prompt, negative_prompt=negative_prompt, generator=torch.Generator().manual_seed(1)
                ... ).frames[0]
                >>> export_to_video(video, "output.mp4", fps=16)
                ```
        '''
    ),
    "src/diffusers/pipelines/cosmos/pipeline_cosmos_text2world.py": (
        '''
        Examples:
                ```python
                >>> import torch
                >>> from diffusers import CosmosTextToWorldPipeline
                >>> from diffusers.utils import export_to_video
        
                >>> model_id = "nvidia/Cosmos-1.0-Diffusion-7B-Text2World"
                >>> pipe = CosmosTextToWorldPipeline.from_pretrained(model_id, torch_dtype=torch.bfloat16)
                >>> pipe.to("cuda")
        
                >>> prompt = "A sleek, humanoid robot stands in a vast warehouse filled with neatly stacked cardboard boxes on industrial shelves. The robot's metallic body gleams under the bright, even lighting, highlighting its futuristic design and intricate joints. A glowing blue light emanates from its chest, adding a touch of advanced technology. The background is dominated by rows of boxes, suggesting a highly organized storage system. The floor is lined with wooden pallets, enhancing the industrial setting. The camera remains static, capturing the robot's poised stance amidst the orderly environment, with a shallow depth of field that keeps the focus on the robot while subtly blurring the background for a cinematic effect."
        
                >>> output = pipe(prompt=prompt).frames[0]
                >>> export_to_video(output, "output.mp4", fps=30)
                ```
        '''
    ),
    "src/diffusers/pipelines/cosmos/pipeline_cosmos_video2world.py": (
        '''
        Examples:
                Image conditioning:
        
                ```python
                >>> import torch
                >>> from diffusers import CosmosVideoToWorldPipeline
                >>> from diffusers.utils import export_to_video, load_image
        
                >>> model_id = "nvidia/Cosmos-1.0-Diffusion-7B-Video2World"
                >>> pipe = CosmosVideoToWorldPipeline.from_pretrained(model_id, torch_dtype=torch.bfloat16)
                >>> pipe.to("cuda")
        
                >>> prompt = "The video depicts a long, straight highway stretching into the distance, flanked by metal guardrails. The road is divided into multiple lanes, with a few vehicles visible in the far distance. The surrounding landscape features dry, grassy fields on one side and rolling hills on the other. The sky is mostly clear with a few scattered clouds, suggesting a bright, sunny day."
                >>> image = load_image(
                ...     "https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/cosmos/cosmos-video2world-input.jpg"
                ... )
        
                >>> video = pipe(image=image, prompt=prompt).frames[0]
                >>> export_to_video(video, "output.mp4", fps=30)
                ```
        
                Video conditioning:
        
                ```python
                >>> import torch
                >>> from diffusers import CosmosVideoToWorldPipeline
                >>> from diffusers.utils import export_to_video, load_video
        
                >>> model_id = "nvidia/Cosmos-1.0-Diffusion-7B-Video2World"
                >>> pipe = CosmosVideoToWorldPipeline.from_pretrained(model_id, torch_dtype=torch.bfloat16)
                >>> pipe.transformer = torch.compile(pipe.transformer)
                >>> pipe.to("cuda")
        
                >>> prompt = "The video depicts a winding mountain road covered in snow, with a single vehicle traveling along it. The road is flanked by steep, rocky cliffs and sparse vegetation. The landscape is characterized by rugged terrain and a river visible in the distance. The scene captures the solitude and beauty of a winter drive through a mountainous region."
                >>> video = load_video(
                ...     "https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/cosmos/cosmos-video2world-input-vid.mp4"
                ... )[
                ...     :21
                ... ]  # This example uses only the first 21 frames
        
                >>> video = pipe(video=video, prompt=prompt).frames[0]
                >>> export_to_video(video, "output.mp4", fps=30)
                ```
        '''
    ),
    "src/diffusers/pipelines/deepfloyd_if/pipeline_if.py": (
        '''
        Examples:
                ```py
                >>> from diffusers import IFPipeline, IFSuperResolutionPipeline, DiffusionPipeline
                >>> from diffusers.utils import pt_to_pil
                >>> import torch
        
                >>> pipe = IFPipeline.from_pretrained("DeepFloyd/IF-I-XL-v1.0", variant="fp16", torch_dtype=torch.float16)
                >>> pipe.enable_model_cpu_offload()
        
                >>> prompt = 'a photo of a kangaroo wearing an orange hoodie and blue sunglasses standing in front of the eiffel tower holding a sign that says "very deep learning"'
                >>> prompt_embeds, negative_embeds = pipe.encode_prompt(prompt)
        
                >>> image = pipe(prompt_embeds=prompt_embeds, negative_prompt_embeds=negative_embeds, output_type="pt").images
        
                >>> # save intermediate image
                >>> pil_image = pt_to_pil(image)
                >>> pil_image[0].save("./if_stage_I.png")
        
                >>> super_res_1_pipe = IFSuperResolutionPipeline.from_pretrained(
                ...     "DeepFloyd/IF-II-L-v1.0", text_encoder=None, variant="fp16", torch_dtype=torch.float16
                ... )
                >>> super_res_1_pipe.enable_model_cpu_offload()
        
                >>> image = super_res_1_pipe(
                ...     image=image, prompt_embeds=prompt_embeds, negative_prompt_embeds=negative_embeds, output_type="pt"
                ... ).images
        
                >>> # save intermediate image
                >>> pil_image = pt_to_pil(image)
                >>> pil_image[0].save("./if_stage_I.png")
        
                >>> safety_modules = {
                ...     "feature_extractor": pipe.feature_extractor,
                ...     "safety_checker": pipe.safety_checker,
                ...     "watermarker": pipe.watermarker,
                ... }
                >>> super_res_2_pipe = DiffusionPipeline.from_pretrained(
                ...     "stabilityai/stable-diffusion-x4-upscaler", **safety_modules, torch_dtype=torch.float16
                ... )
                >>> super_res_2_pipe.enable_model_cpu_offload()
        
                >>> image = super_res_2_pipe(
                ...     prompt=prompt,
                ...     image=image,
                ... ).images
                >>> image[0].save("./if_stage_II.png")
                ```
        '''
    ),
    "src/diffusers/pipelines/deepfloyd_if/pipeline_if_img2img.py": (
        '''
        Examples:
                ```py
                >>> from diffusers import IFImg2ImgPipeline, IFImg2ImgSuperResolutionPipeline, DiffusionPipeline
                >>> from diffusers.utils import pt_to_pil
                >>> import torch
                >>> from PIL import Image
                >>> import requests
                >>> from io import BytesIO
        
                >>> url = "https://raw.githubusercontent.com/CompVis/stable-diffusion/main/assets/stable-samples/img2img/sketch-mountains-input.jpg"
                >>> response = requests.get(url)
                >>> original_image = Image.open(BytesIO(response.content)).convert("RGB")
                >>> original_image = original_image.resize((768, 512))
        
                >>> pipe = IFImg2ImgPipeline.from_pretrained(
                ...     "DeepFloyd/IF-I-XL-v1.0",
                ...     variant="fp16",
                ...     torch_dtype=torch.float16,
                ... )
                >>> pipe.enable_model_cpu_offload()
        
                >>> prompt = "A fantasy landscape in style minecraft"
                >>> prompt_embeds, negative_embeds = pipe.encode_prompt(prompt)
        
                >>> image = pipe(
                ...     image=original_image,
                ...     prompt_embeds=prompt_embeds,
                ...     negative_prompt_embeds=negative_embeds,
                ...     output_type="pt",
                ... ).images
        
                >>> # save intermediate image
                >>> pil_image = pt_to_pil(image)
                >>> pil_image[0].save("./if_stage_I.png")
        
                >>> super_res_1_pipe = IFImg2ImgSuperResolutionPipeline.from_pretrained(
                ...     "DeepFloyd/IF-II-L-v1.0",
                ...     text_encoder=None,
                ...     variant="fp16",
                ...     torch_dtype=torch.float16,
                ... )
                >>> super_res_1_pipe.enable_model_cpu_offload()
        
                >>> image = super_res_1_pipe(
                ...     image=image,
                ...     original_image=original_image,
                ...     prompt_embeds=prompt_embeds,
                ...     negative_prompt_embeds=negative_embeds,
                ... ).images
                >>> image[0].save("./if_stage_II.png")
                ```
        '''
    ),
    "src/diffusers/pipelines/deepfloyd_if/pipeline_if_img2img_superresolution.py": (
        '''
        Examples:
                ```py
                >>> from diffusers import IFImg2ImgPipeline, IFImg2ImgSuperResolutionPipeline, DiffusionPipeline
                >>> from diffusers.utils import pt_to_pil
                >>> import torch
                >>> from PIL import Image
                >>> import requests
                >>> from io import BytesIO
        
                >>> url = "https://raw.githubusercontent.com/CompVis/stable-diffusion/main/assets/stable-samples/img2img/sketch-mountains-input.jpg"
                >>> response = requests.get(url)
                >>> original_image = Image.open(BytesIO(response.content)).convert("RGB")
                >>> original_image = original_image.resize((768, 512))
        
                >>> pipe = IFImg2ImgPipeline.from_pretrained(
                ...     "DeepFloyd/IF-I-XL-v1.0",
                ...     variant="fp16",
                ...     torch_dtype=torch.float16,
                ... )
                >>> pipe.enable_model_cpu_offload()
        
                >>> prompt = "A fantasy landscape in style minecraft"
                >>> prompt_embeds, negative_embeds = pipe.encode_prompt(prompt)
        
                >>> image = pipe(
                ...     image=original_image,
                ...     prompt_embeds=prompt_embeds,
                ...     negative_prompt_embeds=negative_embeds,
                ...     output_type="pt",
                ... ).images
        
                >>> # save intermediate image
                >>> pil_image = pt_to_pil(image)
                >>> pil_image[0].save("./if_stage_I.png")
        
                >>> super_res_1_pipe = IFImg2ImgSuperResolutionPipeline.from_pretrained(
                ...     "DeepFloyd/IF-II-L-v1.0",
                ...     text_encoder=None,
                ...     variant="fp16",
                ...     torch_dtype=torch.float16,
                ... )
                >>> super_res_1_pipe.enable_model_cpu_offload()
        
                >>> image = super_res_1_pipe(
                ...     image=image,
                ...     original_image=original_image,
                ...     prompt_embeds=prompt_embeds,
                ...     negative_prompt_embeds=negative_embeds,
                ... ).images
                >>> image[0].save("./if_stage_II.png")
                ```
        '''
    ),
    "src/diffusers/pipelines/deepfloyd_if/pipeline_if_inpainting.py": (
        '''
        Examples:
                ```py
                >>> from diffusers import IFInpaintingPipeline, IFInpaintingSuperResolutionPipeline, DiffusionPipeline
                >>> from diffusers.utils import pt_to_pil
                >>> import torch
                >>> from PIL import Image
                >>> import requests
                >>> from io import BytesIO
        
                >>> url = "https://huggingface.co/datasets/diffusers/docs-images/resolve/main/if/person.png"
                >>> response = requests.get(url)
                >>> original_image = Image.open(BytesIO(response.content)).convert("RGB")
                >>> original_image = original_image
        
                >>> url = "https://huggingface.co/datasets/diffusers/docs-images/resolve/main/if/glasses_mask.png"
                >>> response = requests.get(url)
                >>> mask_image = Image.open(BytesIO(response.content))
                >>> mask_image = mask_image
        
                >>> pipe = IFInpaintingPipeline.from_pretrained(
                ...     "DeepFloyd/IF-I-XL-v1.0", variant="fp16", torch_dtype=torch.float16
                ... )
                >>> pipe.enable_model_cpu_offload()
        
                >>> prompt = "blue sunglasses"
                >>> prompt_embeds, negative_embeds = pipe.encode_prompt(prompt)
        
                >>> image = pipe(
                ...     image=original_image,
                ...     mask_image=mask_image,
                ...     prompt_embeds=prompt_embeds,
                ...     negative_prompt_embeds=negative_embeds,
                ...     output_type="pt",
                ... ).images
        
                >>> # save intermediate image
                >>> pil_image = pt_to_pil(image)
                >>> pil_image[0].save("./if_stage_I.png")
        
                >>> super_res_1_pipe = IFInpaintingSuperResolutionPipeline.from_pretrained(
                ...     "DeepFloyd/IF-II-L-v1.0", text_encoder=None, variant="fp16", torch_dtype=torch.float16
                ... )
                >>> super_res_1_pipe.enable_model_cpu_offload()
        
                >>> image = super_res_1_pipe(
                ...     image=image,
                ...     mask_image=mask_image,
                ...     original_image=original_image,
                ...     prompt_embeds=prompt_embeds,
                ...     negative_prompt_embeds=negative_embeds,
                ... ).images
                >>> image[0].save("./if_stage_II.png")
                ```
        '''
    ),
    "src/diffusers/pipelines/deepfloyd_if/pipeline_if_inpainting_superresolution.py": (
        '''
        Examples:
                ```py
                >>> from diffusers import IFInpaintingPipeline, IFInpaintingSuperResolutionPipeline, DiffusionPipeline
                >>> from diffusers.utils import pt_to_pil
                >>> import torch
                >>> from PIL import Image
                >>> import requests
                >>> from io import BytesIO
        
                >>> url = "https://huggingface.co/datasets/diffusers/docs-images/resolve/main/if/person.png"
                >>> response = requests.get(url)
                >>> original_image = Image.open(BytesIO(response.content)).convert("RGB")
                >>> original_image = original_image
        
                >>> url = "https://huggingface.co/datasets/diffusers/docs-images/resolve/main/if/glasses_mask.png"
                >>> response = requests.get(url)
                >>> mask_image = Image.open(BytesIO(response.content))
                >>> mask_image = mask_image
        
                >>> pipe = IFInpaintingPipeline.from_pretrained(
                ...     "DeepFloyd/IF-I-XL-v1.0", variant="fp16", torch_dtype=torch.float16
                ... )
                >>> pipe.enable_model_cpu_offload()
        
                >>> prompt = "blue sunglasses"
        
                >>> prompt_embeds, negative_embeds = pipe.encode_prompt(prompt)
                >>> image = pipe(
                ...     image=original_image,
                ...     mask_image=mask_image,
                ...     prompt_embeds=prompt_embeds,
                ...     negative_prompt_embeds=negative_embeds,
                ...     output_type="pt",
                ... ).images
        
                >>> # save intermediate image
                >>> pil_image = pt_to_pil(image)
                >>> pil_image[0].save("./if_stage_I.png")
        
                >>> super_res_1_pipe = IFInpaintingSuperResolutionPipeline.from_pretrained(
                ...     "DeepFloyd/IF-II-L-v1.0", text_encoder=None, variant="fp16", torch_dtype=torch.float16
                ... )
                >>> super_res_1_pipe.enable_model_cpu_offload()
        
                >>> image = super_res_1_pipe(
                ...     image=image,
                ...     mask_image=mask_image,
                ...     original_image=original_image,
                ...     prompt_embeds=prompt_embeds,
                ...     negative_prompt_embeds=negative_embeds,
                ... ).images
                >>> image[0].save("./if_stage_II.png")
                ```
        '''
    ),
    "src/diffusers/pipelines/deepfloyd_if/pipeline_if_superresolution.py": (
        '''
        Examples:
                ```py
                >>> from diffusers import IFPipeline, IFSuperResolutionPipeline, DiffusionPipeline
                >>> from diffusers.utils import pt_to_pil
                >>> import torch
        
                >>> pipe = IFPipeline.from_pretrained("DeepFloyd/IF-I-XL-v1.0", variant="fp16", torch_dtype=torch.float16)
                >>> pipe.enable_model_cpu_offload()
        
                >>> prompt = 'a photo of a kangaroo wearing an orange hoodie and blue sunglasses standing in front of the eiffel tower holding a sign that says "very deep learning"'
                >>> prompt_embeds, negative_embeds = pipe.encode_prompt(prompt)
        
                >>> image = pipe(prompt_embeds=prompt_embeds, negative_prompt_embeds=negative_embeds, output_type="pt").images
        
                >>> # save intermediate image
                >>> pil_image = pt_to_pil(image)
                >>> pil_image[0].save("./if_stage_I.png")
        
                >>> super_res_1_pipe = IFSuperResolutionPipeline.from_pretrained(
                ...     "DeepFloyd/IF-II-L-v1.0", text_encoder=None, variant="fp16", torch_dtype=torch.float16
                ... )
                >>> super_res_1_pipe.enable_model_cpu_offload()
        
                >>> image = super_res_1_pipe(
                ...     image=image, prompt_embeds=prompt_embeds, negative_prompt_embeds=negative_embeds
                ... ).images
                >>> image[0].save("./if_stage_II.png")
                ```
        '''
    ),
    "src/diffusers/pipelines/deprecated/alt_diffusion/pipeline_alt_diffusion.py": (
        '''
        Examples:
                ```py
                >>> import torch
                >>> from diffusers import AltDiffusionPipeline
        
                >>> pipe = AltDiffusionPipeline.from_pretrained("BAAI/AltDiffusion-m9", torch_dtype=torch.float16)
                >>> pipe = pipe.to("cuda")
        
                >>> # "dark elf princess, highly detailed, d & d, fantasy, highly detailed, digital painting, trending on artstation, concept art, sharp focus, illustration, art by artgerm and greg rutkowski and fuji choko and viktoria gavrilenko and hoang lap"
                >>> prompt = "黑暗精灵公主，非常详细，幻想，非常详细，数字绘画，概念艺术，敏锐的焦点，插图"
                >>> image = pipe(prompt).images[0]
                ```
        '''
    ),
    "src/diffusers/pipelines/deprecated/alt_diffusion/pipeline_alt_diffusion_img2img.py": (
        '''
        Examples:
                ```py
                >>> import requests
                >>> import torch
                >>> from PIL import Image
                >>> from io import BytesIO
        
                >>> from diffusers import AltDiffusionImg2ImgPipeline
        
                >>> device = "cuda"
                >>> model_id_or_path = "BAAI/AltDiffusion-m9"
                >>> pipe = AltDiffusionImg2ImgPipeline.from_pretrained(model_id_or_path, torch_dtype=torch.float16)
                >>> pipe = pipe.to(device)
        
                >>> url = "https://raw.githubusercontent.com/CompVis/stable-diffusion/main/assets/stable-samples/img2img/sketch-mountains-input.jpg"
        
                >>> response = requests.get(url)
                >>> init_image = Image.open(BytesIO(response.content)).convert("RGB")
                >>> init_image = init_image.resize((768, 512))
        
                >>> # "A fantasy landscape, trending on artstation"
                >>> prompt = "幻想风景, artstation"
        
                >>> images = pipe(prompt=prompt, image=init_image, strength=0.75, guidance_scale=7.5).images
                >>> images[0].save("幻想风景.png")
                ```
        '''
    ),
    "src/diffusers/pipelines/deprecated/stable_diffusion_variants/pipeline_stable_diffusion_paradigms.py": (
        '''
        Examples:
                ```py
                >>> import torch
                >>> from diffusers import DDPMParallelScheduler
                >>> from diffusers import StableDiffusionParadigmsPipeline
        
                >>> scheduler = DDPMParallelScheduler.from_pretrained("runwayml/stable-diffusion-v1-5", subfolder="scheduler")
        
                >>> pipe = StableDiffusionParadigmsPipeline.from_pretrained(
                ...     "runwayml/stable-diffusion-v1-5", scheduler=scheduler, torch_dtype=torch.float16
                ... )
                >>> pipe = pipe.to("cuda")
        
                >>> ngpu, batch_per_device = torch.cuda.device_count(), 5
                >>> pipe.wrapped_unet = torch.nn.DataParallel(pipe.unet, device_ids=[d for d in range(ngpu)])
        
                >>> prompt = "a photo of an astronaut riding a horse on mars"
                >>> image = pipe(prompt, parallel=ngpu * batch_per_device, num_inference_steps=1000).images[0]
                ```
        '''
    ),
    "src/diffusers/pipelines/deprecated/stable_diffusion_variants/pipeline_stable_diffusion_pix2pix_zero.py": (
        '''
        Examples:
                ```py
                >>> import requests
                >>> import torch
        
                >>> from diffusers import DDIMScheduler, StableDiffusionPix2PixZeroPipeline
        
        
                >>> def download(embedding_url, local_filepath):
                ...     r = requests.get(embedding_url)
                ...     with open(local_filepath, "wb") as f:
                ...         f.write(r.content)
        
        
                >>> model_ckpt = "CompVis/stable-diffusion-v1-4"
                >>> pipeline = StableDiffusionPix2PixZeroPipeline.from_pretrained(model_ckpt, torch_dtype=torch.float16)
                >>> pipeline.scheduler = DDIMScheduler.from_config(pipeline.scheduler.config)
                >>> pipeline.to("cuda")
        
                >>> prompt = "a high resolution painting of a cat in the style of van gough"
                >>> source_emb_url = "https://hf.co/datasets/sayakpaul/sample-datasets/resolve/main/cat.pt"
                >>> target_emb_url = "https://hf.co/datasets/sayakpaul/sample-datasets/resolve/main/dog.pt"
        
                >>> for url in [source_emb_url, target_emb_url]:
                ...     download(url, url.split("/")[-1])
        
                >>> src_embeds = torch.load(source_emb_url.split("/")[-1])
                >>> target_embeds = torch.load(target_emb_url.split("/")[-1])
                >>> images = pipeline(
                ...     prompt,
                ...     source_embeds=src_embeds,
                ...     target_embeds=target_embeds,
                ...     num_inference_steps=50,
                ...     cross_attention_guidance_amount=0.15,
                ... ).images
        
                >>> images[0].save("edited_image_dog.png")
                ```
        '''
    ),
    "src/diffusers/pipelines/easyanimate/pipeline_easyanimate.py": (
        '''
        Examples:
                ```python
                >>> import torch
                >>> from diffusers import EasyAnimatePipeline
                >>> from diffusers.utils import export_to_video
        
                >>> # Models: "alibaba-pai/EasyAnimateV5.1-12b-zh"
                >>> pipe = EasyAnimatePipeline.from_pretrained(
                ...     "alibaba-pai/EasyAnimateV5.1-7b-zh-diffusers", torch_dtype=torch.float16
                ... ).to("cuda")
                >>> prompt = (
                ...     "A panda, dressed in a small, red jacket and a tiny hat, sits on a wooden stool in a serene bamboo forest. "
                ...     "The panda's fluffy paws strum a miniature acoustic guitar, producing soft, melodic tunes. Nearby, a few other "
                ...     "pandas gather, watching curiously and some clapping in rhythm. Sunlight filters through the tall bamboo, "
                ...     "casting a gentle glow on the scene. The panda's face is expressive, showing concentration and joy as it plays. "
                ...     "The background includes a small, flowing stream and vibrant green foliage, enhancing the peaceful and magical "
                ...     "atmosphere of this unique musical performance."
                ... )
                >>> sample_size = (512, 512)
                >>> video = pipe(
                ...     prompt=prompt,
                ...     guidance_scale=6,
                ...     negative_prompt="bad detailed",
                ...     height=sample_size[0],
                ...     width=sample_size[1],
                ...     num_inference_steps=50,
                ... ).frames[0]
                >>> export_to_video(video, "output.mp4", fps=8)
                ```
        '''
    ),
    "src/diffusers/pipelines/easyanimate/pipeline_easyanimate_control.py": (
        '''
        Examples:
                ```python
                >>> import torch
                >>> from diffusers import EasyAnimateControlPipeline
                >>> from diffusers.pipelines.easyanimate.pipeline_easyanimate_control import get_video_to_video_latent
                >>> from diffusers.utils import export_to_video, load_video
        
                >>> pipe = EasyAnimateControlPipeline.from_pretrained(
                ...     "alibaba-pai/EasyAnimateV5.1-12b-zh-Control-diffusers", torch_dtype=torch.bfloat16
                ... )
                >>> pipe.to("cuda")
        
                >>> control_video = load_video(
                ...     "https://huggingface.co/alibaba-pai/EasyAnimateV5.1-12b-zh-Control/blob/main/asset/pose.mp4"
                ... )
                >>> prompt = (
                ...     "In this sunlit outdoor garden, a beautiful woman is dressed in a knee-length, sleeveless white dress. "
                ...     "The hem of her dress gently sways with her graceful dance, much like a butterfly fluttering in the breeze. "
                ...     "Sunlight filters through the leaves, casting dappled shadows that highlight her soft features and clear eyes, "
                ...     "making her appear exceptionally elegant. It seems as if every movement she makes speaks of youth and vitality. "
                ...     "As she twirls on the grass, her dress flutters, as if the entire garden is rejoicing in her dance. "
                ...     "The colorful flowers around her sway in the gentle breeze, with roses, chrysanthemums, and lilies each "
                ...     "releasing their fragrances, creating a relaxed and joyful atmosphere."
                ... )
                >>> sample_size = (672, 384)
                >>> num_frames = 49
        
                >>> input_video, _, _ = get_video_to_video_latent(control_video, num_frames, sample_size)
                >>> video = pipe(
                ...     prompt,
                ...     num_frames=num_frames,
                ...     negative_prompt="Twisted body, limb deformities, text subtitles, comics, stillness, ugliness, errors, garbled text.",
                ...     height=sample_size[0],
                ...     width=sample_size[1],
                ...     control_video=input_video,
                ... ).frames[0]
                >>> export_to_video(video, "output.mp4", fps=8)
                ```
        '''
    ),
    "src/diffusers/pipelines/easyanimate/pipeline_easyanimate_inpaint.py": (
        '''
        Examples:
                ```py
                >>> import torch
                >>> from diffusers import EasyAnimateInpaintPipeline
                >>> from diffusers.pipelines.easyanimate.pipeline_easyanimate_inpaint import get_image_to_video_latent
                >>> from diffusers.utils import export_to_video, load_image
        
                >>> pipe = EasyAnimateInpaintPipeline.from_pretrained(
                ...     "alibaba-pai/EasyAnimateV5.1-12b-zh-InP-diffusers", torch_dtype=torch.bfloat16
                ... )
                >>> pipe.to("cuda")
        
                >>> prompt = "An astronaut hatching from an egg, on the surface of the moon, the darkness and depth of space realised in the background. High quality, ultrarealistic detail and breath-taking movie-like camera shot."
                >>> validation_image_start = load_image(
                ...     "https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/astronaut.jpg"
                ... )
        
                >>> validation_image_end = None
                >>> sample_size = (448, 576)
                >>> num_frames = 49
                >>> input_video, input_video_mask = get_image_to_video_latent(
                ...     [validation_image_start], validation_image_end, num_frames, sample_size
                ... )
        
                >>> video = pipe(
                ...     prompt,
                ...     num_frames=num_frames,
                ...     negative_prompt="Twisted body, limb deformities, text subtitles, comics, stillness, ugliness, errors, garbled text.",
                ...     height=sample_size[0],
                ...     width=sample_size[1],
                ...     video=input_video,
                ...     mask_video=input_video_mask,
                ... )
                >>> export_to_video(video.frames[0], "output.mp4", fps=8)
                ```
        '''
    ),
    "src/diffusers/pipelines/flux/pipeline_flux.py": (
        '''
        Examples:
                ```py
                >>> import torch
                >>> from diffusers import FluxPipeline
        
                >>> pipe = FluxPipeline.from_pretrained("black-forest-labs/FLUX.1-schnell", torch_dtype=torch.bfloat16)
                >>> pipe.to("cuda")
                >>> prompt = "A cat holding a sign that says hello world"
                >>> # Depending on the variant being used, the pipeline call will slightly vary.
                >>> # Refer to the pipeline documentation for more details.
                >>> image = pipe(prompt, num_inference_steps=4, guidance_scale=0.0).images[0]
                >>> image.save("flux.png")
                ```
        '''
    ),
    "src/diffusers/pipelines/flux/pipeline_flux_control.py": (
        '''
        Examples:
                ```py
                >>> import torch
                >>> from controlnet_aux import CannyDetector
                >>> from diffusers import FluxControlPipeline
                >>> from diffusers.utils import load_image
        
                >>> pipe = FluxControlPipeline.from_pretrained(
                ...     "black-forest-labs/FLUX.1-Canny-dev", torch_dtype=torch.bfloat16
                ... ).to("cuda")
        
                >>> prompt = "A robot made of exotic candies and chocolates of different kinds. The background is filled with confetti and celebratory gifts."
                >>> control_image = load_image(
                ...     "https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/robot.png"
                ... )
        
                >>> processor = CannyDetector()
                >>> control_image = processor(
                ...     control_image, low_threshold=50, high_threshold=200, detect_resolution=1024, image_resolution=1024
                ... )
        
                >>> image = pipe(
                ...     prompt=prompt,
                ...     control_image=control_image,
                ...     height=1024,
                ...     width=1024,
                ...     num_inference_steps=50,
                ...     guidance_scale=30.0,
                ... ).images[0]
                >>> image.save("output.png")
                ```
        '''
    ),
    "src/diffusers/pipelines/flux/pipeline_flux_control_img2img.py": (
        '''
        Examples:
                ```py
                >>> import torch
                >>> from controlnet_aux import CannyDetector
                >>> from diffusers import FluxControlImg2ImgPipeline
                >>> from diffusers.utils import load_image
        
                >>> pipe = FluxControlImg2ImgPipeline.from_pretrained(
                ...     "black-forest-labs/FLUX.1-Canny-dev", torch_dtype=torch.bfloat16
                ... ).to("cuda")
        
                >>> prompt = "A robot made of exotic candies and chocolates of different kinds. Abstract background"
                >>> image = load_image(
                ...     "https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/watercolor-painting.jpg"
                ... )
                >>> control_image = load_image(
                ...     "https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/robot.png"
                ... )
        
                >>> processor = CannyDetector()
                >>> control_image = processor(
                ...     control_image, low_threshold=50, high_threshold=200, detect_resolution=1024, image_resolution=1024
                ... )
        
                >>> image = pipe(
                ...     prompt=prompt,
                ...     image=image,
                ...     control_image=control_image,
                ...     strength=0.8,
                ...     height=1024,
                ...     width=1024,
                ...     num_inference_steps=50,
                ...     guidance_scale=30.0,
                ... ).images[0]
                >>> image.save("output.png")
                ```
        '''
    ),
    "src/diffusers/pipelines/flux/pipeline_flux_control_inpaint.py": (
        '''
        Examples:
                ```py
                import torch
                from diffusers import FluxControlInpaintPipeline
                from diffusers.models.transformers import FluxTransformer2DModel
                from transformers import T5EncoderModel
                from diffusers.utils import load_image, make_image_grid
                from image_gen_aux import DepthPreprocessor  # https://github.com/huggingface/image_gen_aux
                from PIL import Image
                import numpy as np
        
                pipe = FluxControlInpaintPipeline.from_pretrained(
                    "black-forest-labs/FLUX.1-Depth-dev",
                    torch_dtype=torch.bfloat16,
                )
                # use following lines if you have GPU constraints
                # ---------------------------------------------------------------
                transformer = FluxTransformer2DModel.from_pretrained(
                    "sayakpaul/FLUX.1-Depth-dev-nf4", subfolder="transformer", torch_dtype=torch.bfloat16
                )
                text_encoder_2 = T5EncoderModel.from_pretrained(
                    "sayakpaul/FLUX.1-Depth-dev-nf4", subfolder="text_encoder_2", torch_dtype=torch.bfloat16
                )
                pipe.transformer = transformer
                pipe.text_encoder_2 = text_encoder_2
                pipe.enable_model_cpu_offload()
                # ---------------------------------------------------------------
                pipe.to("cuda")
        
                prompt = "a blue robot singing opera with human-like expressions"
                image = load_image("https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/robot.png")
        
                head_mask = np.zeros_like(image)
                head_mask[65:580, 300:642] = 255
                mask_image = Image.fromarray(head_mask)
        
                processor = DepthPreprocessor.from_pretrained("LiheYoung/depth-anything-large-hf")
                control_image = processor(image)[0].convert("RGB")
        
                output = pipe(
                    prompt=prompt,
                    image=image,
                    control_image=control_image,
                    mask_image=mask_image,
                    num_inference_steps=30,
                    strength=0.9,
                    guidance_scale=10.0,
                    generator=torch.Generator().manual_seed(42),
                ).images[0]
                make_image_grid([image, control_image, mask_image, output.resize(image.size)], rows=1, cols=4).save(
                    "output.png"
                )
                ```
        '''
    ),
    "src/diffusers/pipelines/flux/pipeline_flux_controlnet.py": (
        '''
        Examples:
                ```py
                >>> import torch
                >>> from diffusers.utils import load_image
                >>> from diffusers import FluxControlNetPipeline
                >>> from diffusers import FluxControlNetModel
        
                >>> base_model = "black-forest-labs/FLUX.1-dev"
                >>> controlnet_model = "InstantX/FLUX.1-dev-controlnet-canny"
                >>> controlnet = FluxControlNetModel.from_pretrained(controlnet_model, torch_dtype=torch.bfloat16)
                >>> pipe = FluxControlNetPipeline.from_pretrained(
                ...     base_model, controlnet=controlnet, torch_dtype=torch.bfloat16
                ... )
                >>> pipe.to("cuda")
                >>> control_image = load_image("https://huggingface.co/InstantX/SD3-Controlnet-Canny/resolve/main/canny.jpg")
                >>> prompt = "A girl in city, 25 years old, cool, futuristic"
                >>> image = pipe(
                ...     prompt,
                ...     control_image=control_image,
                ...     control_guidance_start=0.2,
                ...     control_guidance_end=0.8,
                ...     controlnet_conditioning_scale=1.0,
                ...     num_inference_steps=28,
                ...     guidance_scale=3.5,
                ... ).images[0]
                >>> image.save("flux.png")
                ```
        '''
    ),
    "src/diffusers/pipelines/flux/pipeline_flux_controlnet_image_to_image.py": (
        '''
        Examples:
                ```py
                >>> import torch
                >>> from diffusers import FluxControlNetImg2ImgPipeline, FluxControlNetModel
                >>> from diffusers.utils import load_image
        
                >>> device = "cuda" if torch.cuda.is_available() else "cpu"
        
                >>> controlnet = FluxControlNetModel.from_pretrained(
                ...     "InstantX/FLUX.1-dev-Controlnet-Canny-alpha", torch_dtype=torch.bfloat16
                ... )
        
                >>> pipe = FluxControlNetImg2ImgPipeline.from_pretrained(
                ...     "black-forest-labs/FLUX.1-schnell", controlnet=controlnet, torch_dtype=torch.float16
                ... )
        
                >>> pipe.text_encoder.to(torch.float16)
                >>> pipe.controlnet.to(torch.float16)
                >>> pipe.to("cuda")
        
                >>> control_image = load_image("https://huggingface.co/InstantX/SD3-Controlnet-Canny/resolve/main/canny.jpg")
                >>> init_image = load_image(
                ...     "https://raw.githubusercontent.com/CompVis/stable-diffusion/main/assets/stable-samples/img2img/sketch-mountains-input.jpg"
                ... )
        
                >>> prompt = "A girl in city, 25 years old, cool, futuristic"
                >>> image = pipe(
                ...     prompt,
                ...     image=init_image,
                ...     control_image=control_image,
                ...     control_guidance_start=0.2,
                ...     control_guidance_end=0.8,
                ...     controlnet_conditioning_scale=1.0,
                ...     strength=0.7,
                ...     num_inference_steps=2,
                ...     guidance_scale=3.5,
                ... ).images[0]
                >>> image.save("flux_controlnet_img2img.png")
                ```
        '''
    ),
    "src/diffusers/pipelines/flux/pipeline_flux_controlnet_inpainting.py": (
        '''
        Examples:
                ```py
                >>> import torch
                >>> from diffusers import FluxControlNetInpaintPipeline
                >>> from diffusers.models import FluxControlNetModel
                >>> from diffusers.utils import load_image
        
                >>> controlnet = FluxControlNetModel.from_pretrained(
                ...     "InstantX/FLUX.1-dev-controlnet-canny", torch_dtype=torch.float16
                ... )
                >>> pipe = FluxControlNetInpaintPipeline.from_pretrained(
                ...     "black-forest-labs/FLUX.1-schnell", controlnet=controlnet, torch_dtype=torch.float16
                ... )
                >>> pipe.to("cuda")
        
                >>> control_image = load_image(
                ...     "https://huggingface.co/InstantX/FLUX.1-dev-Controlnet-Canny-alpha/resolve/main/canny.jpg"
                ... )
                >>> init_image = load_image(
                ...     "https://raw.githubusercontent.com/CompVis/latent-diffusion/main/data/inpainting_examples/overture-creations-5sI6fQgYIuo.png"
                ... )
                >>> mask_image = load_image(
                ...     "https://raw.githubusercontent.com/CompVis/latent-diffusion/main/data/inpainting_examples/overture-creations-5sI6fQgYIuo_mask.png"
                ... )
        
                >>> prompt = "A girl holding a sign that says InstantX"
                >>> image = pipe(
                ...     prompt,
                ...     image=init_image,
                ...     mask_image=mask_image,
                ...     control_image=control_image,
                ...     control_guidance_start=0.2,
                ...     control_guidance_end=0.8,
                ...     controlnet_conditioning_scale=0.7,
                ...     strength=0.7,
                ...     num_inference_steps=28,
                ...     guidance_scale=3.5,
                ... ).images[0]
                >>> image.save("flux_controlnet_inpaint.png")
                ```
        '''
    ),
    "src/diffusers/pipelines/flux/pipeline_flux_fill.py": (
        '''
        Examples:
                ```py
                >>> import torch
                >>> from diffusers import FluxFillPipeline
                >>> from diffusers.utils import load_image
        
                >>> image = load_image("https://huggingface.co/datasets/YiYiXu/testing-images/resolve/main/cup.png")
                >>> mask = load_image("https://huggingface.co/datasets/YiYiXu/testing-images/resolve/main/cup_mask.png")
        
                >>> pipe = FluxFillPipeline.from_pretrained("black-forest-labs/FLUX.1-Fill-dev", torch_dtype=torch.bfloat16)
                >>> pipe.enable_model_cpu_offload()  # save some VRAM by offloading the model to CPU
        
                >>> image = pipe(
                ...     prompt="a white paper cup",
                ...     image=image,
                ...     mask_image=mask,
                ...     height=1632,
                ...     width=1232,
                ...     guidance_scale=30,
                ...     num_inference_steps=50,
                ...     max_sequence_length=512,
                ...     generator=torch.Generator("cpu").manual_seed(0),
                ... ).images[0]
                >>> image.save("flux_fill.png")
                ```
        '''
    ),
    "src/diffusers/pipelines/flux/pipeline_flux_img2img.py": (
        '''
        Examples:
                ```py
                >>> import torch
        
                >>> from diffusers import FluxImg2ImgPipeline
                >>> from diffusers.utils import load_image
        
                >>> device = "cuda"
                >>> pipe = FluxImg2ImgPipeline.from_pretrained("black-forest-labs/FLUX.1-schnell", torch_dtype=torch.bfloat16)
                >>> pipe = pipe.to(device)
        
                >>> url = "https://raw.githubusercontent.com/CompVis/stable-diffusion/main/assets/stable-samples/img2img/sketch-mountains-input.jpg"
                >>> init_image = load_image(url).resize((1024, 1024))
        
                >>> prompt = "cat wizard, gandalf, lord of the rings, detailed, fantasy, cute, adorable, Pixar, Disney, 8k"
        
                >>> images = pipe(
                ...     prompt=prompt, image=init_image, num_inference_steps=4, strength=0.95, guidance_scale=0.0
                ... ).images[0]
                ```
        '''
    ),
    "src/diffusers/pipelines/flux/pipeline_flux_inpaint.py": (
        '''
        Examples:
                ```py
                >>> import torch
                >>> from diffusers import FluxInpaintPipeline
                >>> from diffusers.utils import load_image
        
                >>> pipe = FluxInpaintPipeline.from_pretrained("black-forest-labs/FLUX.1-schnell", torch_dtype=torch.bfloat16)
                >>> pipe.to("cuda")
                >>> prompt = "Face of a yellow cat, high resolution, sitting on a park bench"
                >>> img_url = "https://raw.githubusercontent.com/CompVis/latent-diffusion/main/data/inpainting_examples/overture-creations-5sI6fQgYIuo.png"
                >>> mask_url = "https://raw.githubusercontent.com/CompVis/latent-diffusion/main/data/inpainting_examples/overture-creations-5sI6fQgYIuo_mask.png"
                >>> source = load_image(img_url)
                >>> mask = load_image(mask_url)
                >>> image = pipe(prompt=prompt, image=source, mask_image=mask).images[0]
                >>> image.save("flux_inpainting.png")
                ```
        '''
    ),
    "src/diffusers/pipelines/flux/pipeline_flux_kontext.py": (
        '''
        Examples:
                ```py
                >>> import torch
                >>> from diffusers import FluxKontextPipeline
                >>> from diffusers.utils import load_image
        
                >>> pipe = FluxKontextPipeline.from_pretrained(
                ...     "black-forest-labs/FLUX.1-Kontext-dev", torch_dtype=torch.bfloat16
                ... )
                >>> pipe.to("cuda")
        
                >>> image = load_image(
                ...     "https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/yarn-art-pikachu.png"
                ... ).convert("RGB")
                >>> prompt = "Make Pikachu hold a sign that says 'Black Forest Labs is awesome', yarn art style, detailed, vibrant colors"
                >>> image = pipe(
                ...     image=image,
                ...     prompt=prompt,
                ...     guidance_scale=2.5,
                ...     generator=torch.Generator().manual_seed(42),
                ... ).images[0]
                >>> image.save("output.png")
                ```
        '''
    ),
    "src/diffusers/pipelines/flux/pipeline_flux_kontext_inpaint.py": (
        '''
        Examples:
                # Inpainting with text only
                ```py
                >>> import torch
                >>> from diffusers import FluxKontextInpaintPipeline
                >>> from diffusers.utils import load_image
        
                >>> prompt = "Change the yellow dinosaur to green one"
                >>> img_url = (
                ...     "https://github.com/ZenAI-Vietnam/Flux-Kontext-pipelines/blob/main/assets/dinosaur_input.jpeg?raw=true"
                ... )
                >>> mask_url = (
                ...     "https://github.com/ZenAI-Vietnam/Flux-Kontext-pipelines/blob/main/assets/dinosaur_mask.png?raw=true"
                ... )
        
                >>> source = load_image(img_url)
                >>> mask = load_image(mask_url)
        
                >>> pipe = FluxKontextInpaintPipeline.from_pretrained(
                ...     "black-forest-labs/FLUX.1-Kontext-dev", torch_dtype=torch.bfloat16
                ... )
                >>> pipe.to("cuda")
        
                >>> image = pipe(prompt=prompt, image=source, mask_image=mask, strength=1.0).images[0]
                >>> image.save("kontext_inpainting_normal.png")
                ```
        
                # Inpainting with image conditioning
                ```py
                >>> import torch
                >>> from diffusers import FluxKontextInpaintPipeline
                >>> from diffusers.utils import load_image
        
                >>> pipe = FluxKontextInpaintPipeline.from_pretrained(
                ...     "black-forest-labs/FLUX.1-Kontext-dev", torch_dtype=torch.bfloat16
                ... )
                >>> pipe.to("cuda")
        
                >>> prompt = "Replace this ball"
                >>> img_url = "https://images.pexels.com/photos/39362/the-ball-stadion-football-the-pitch-39362.jpeg?auto=compress&cs=tinysrgb&dpr=1&w=500"
                >>> mask_url = (
                ...     "https://github.com/ZenAI-Vietnam/Flux-Kontext-pipelines/blob/main/assets/ball_mask.png?raw=true"
                ... )
                >>> image_reference_url = (
                ...     "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcTah3x6OL_ECMBaZ5ZlJJhNsyC-OSMLWAI-xw&s"
                ... )
        
                >>> source = load_image(img_url)
                >>> mask = load_image(mask_url)
                >>> image_reference = load_image(image_reference_url)
        
                >>> mask = pipe.mask_processor.blur(mask, blur_factor=12)
                >>> image = pipe(
                ...     prompt=prompt, image=source, mask_image=mask, image_reference=image_reference, strength=1.0
                ... ).images[0]
                >>> image.save("kontext_inpainting_ref.png")
                ```
        '''
    ),
    "src/diffusers/pipelines/flux/pipeline_flux_prior_redux.py": (
        '''
        Examples:
                ```py
                >>> import torch
                >>> from diffusers import FluxPriorReduxPipeline, FluxPipeline
                >>> from diffusers.utils import load_image
        
                >>> device = "cuda"
                >>> dtype = torch.bfloat16
        
                >>> repo_redux = "black-forest-labs/FLUX.1-Redux-dev"
                >>> repo_base = "black-forest-labs/FLUX.1-dev"
                >>> pipe_prior_redux = FluxPriorReduxPipeline.from_pretrained(repo_redux, torch_dtype=dtype).to(device)
                >>> pipe = FluxPipeline.from_pretrained(
                ...     repo_base, text_encoder=None, text_encoder_2=None, torch_dtype=torch.bfloat16
                ... ).to(device)
        
                >>> image = load_image(
                ...     "https://huggingface.co/datasets/YiYiXu/testing-images/resolve/main/style_ziggy/img5.png"
                ... )
                >>> pipe_prior_output = pipe_prior_redux(image)
                >>> images = pipe(
                ...     guidance_scale=2.5,
                ...     num_inference_steps=50,
                ...     generator=torch.Generator("cpu").manual_seed(0),
                ...     **pipe_prior_output,
                ... ).images
                >>> images[0].save("flux-redux.png")
                ```
        '''
    ),
    "src/diffusers/pipelines/hidream_image/pipeline_hidream_image.py": (
        '''
        Examples:
                ```py
                >>> import torch
                >>> from transformers import AutoTokenizer, LlamaForCausalLM
                >>> from diffusers import HiDreamImagePipeline
        
        
                >>> tokenizer_4 = AutoTokenizer.from_pretrained("meta-llama/Meta-Llama-3.1-8B-Instruct")
                >>> text_encoder_4 = LlamaForCausalLM.from_pretrained(
                ...     "meta-llama/Meta-Llama-3.1-8B-Instruct",
                ...     output_hidden_states=True,
                ...     output_attentions=True,
                ...     torch_dtype=torch.bfloat16,
                ... )
        
                >>> pipe = HiDreamImagePipeline.from_pretrained(
                ...     "HiDream-ai/HiDream-I1-Full",
                ...     tokenizer_4=tokenizer_4,
                ...     text_encoder_4=text_encoder_4,
                ...     torch_dtype=torch.bfloat16,
                ... )
                >>> pipe.enable_model_cpu_offload()
        
                >>> image = pipe(
                ...     'A cat holding a sign that says "Hi-Dreams.ai".',
                ...     height=1024,
                ...     width=1024,
                ...     guidance_scale=5.0,
                ...     num_inference_steps=50,
                ...     generator=torch.Generator("cuda").manual_seed(0),
                ... ).images[0]
                >>> image.save("output.png")
                ```
        '''
    ),
    "src/diffusers/pipelines/hunyuan_video/pipeline_hunyuan_skyreels_image2video.py": (
        '''
        Examples:
                ```python
                >>> import torch
                >>> from diffusers import HunyuanSkyreelsImageToVideoPipeline, HunyuanVideoTransformer3DModel
                >>> from diffusers.utils import load_image, export_to_video
        
                >>> model_id = "hunyuanvideo-community/HunyuanVideo"
                >>> transformer_model_id = "Skywork/SkyReels-V1-Hunyuan-I2V"
                >>> transformer = HunyuanVideoTransformer3DModel.from_pretrained(
                ...     transformer_model_id, torch_dtype=torch.bfloat16
                ... )
                >>> pipe = HunyuanSkyreelsImageToVideoPipeline.from_pretrained(
                ...     model_id, transformer=transformer, torch_dtype=torch.float16
                ... )
                >>> pipe.vae.enable_tiling()
                >>> pipe.to("cuda")
        
                >>> prompt = "An astronaut hatching from an egg, on the surface of the moon, the darkness and depth of space realised in the background. High quality, ultrarealistic detail and breath-taking movie-like camera shot."
                >>> negative_prompt = "Aerial view, aerial view, overexposed, low quality, deformation, a poor composition, bad hands, bad teeth, bad eyes, bad limbs, distortion"
                >>> image = load_image(
                ...     "https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/astronaut.jpg"
                ... )
        
                >>> output = pipe(
                ...     image=image,
                ...     prompt=prompt,
                ...     negative_prompt=negative_prompt,
                ...     num_inference_steps=30,
                ...     true_cfg_scale=6.0,
                ...     guidance_scale=1.0,
                ... ).frames[0]
                >>> export_to_video(output, "output.mp4", fps=15)
                ```
        '''
    ),
    "src/diffusers/pipelines/hunyuan_video/pipeline_hunyuan_video.py": (
        '''
        Examples:
                ```python
                >>> import torch
                >>> from diffusers import HunyuanVideoPipeline, HunyuanVideoTransformer3DModel
                >>> from diffusers.utils import export_to_video
        
                >>> model_id = "hunyuanvideo-community/HunyuanVideo"
                >>> transformer = HunyuanVideoTransformer3DModel.from_pretrained(
                ...     model_id, subfolder="transformer", torch_dtype=torch.bfloat16
                ... )
                >>> pipe = HunyuanVideoPipeline.from_pretrained(model_id, transformer=transformer, torch_dtype=torch.float16)
                >>> pipe.vae.enable_tiling()
                >>> pipe.to("cuda")
        
                >>> output = pipe(
                ...     prompt="A cat walks on the grass, realistic",
                ...     height=320,
                ...     width=512,
                ...     num_frames=61,
                ...     num_inference_steps=30,
                ... ).frames[0]
                >>> export_to_video(output, "output.mp4", fps=15)
                ```
        '''
    ),
    "src/diffusers/pipelines/hunyuan_video/pipeline_hunyuan_video_framepack.py": (
        '''
        Examples:
                ##### Image-to-Video
        
                ```python
                >>> import torch
                >>> from diffusers import HunyuanVideoFramepackPipeline, HunyuanVideoFramepackTransformer3DModel
                >>> from diffusers.utils import export_to_video, load_image
                >>> from transformers import SiglipImageProcessor, SiglipVisionModel
        
                >>> transformer = HunyuanVideoFramepackTransformer3DModel.from_pretrained(
                ...     "lllyasviel/FramePackI2V_HY", torch_dtype=torch.bfloat16
                ... )
                >>> feature_extractor = SiglipImageProcessor.from_pretrained(
                ...     "lllyasviel/flux_redux_bfl", subfolder="feature_extractor"
                ... )
                >>> image_encoder = SiglipVisionModel.from_pretrained(
                ...     "lllyasviel/flux_redux_bfl", subfolder="image_encoder", torch_dtype=torch.float16
                ... )
                >>> pipe = HunyuanVideoFramepackPipeline.from_pretrained(
                ...     "hunyuanvideo-community/HunyuanVideo",
                ...     transformer=transformer,
                ...     feature_extractor=feature_extractor,
                ...     image_encoder=image_encoder,
                ...     torch_dtype=torch.float16,
                ... )
                >>> pipe.vae.enable_tiling()
                >>> pipe.to("cuda")
        
                >>> image = load_image(
                ...     "https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/penguin.png"
                ... )
                >>> output = pipe(
                ...     image=image,
                ...     prompt="A penguin dancing in the snow",
                ...     height=832,
                ...     width=480,
                ...     num_frames=91,
                ...     num_inference_steps=30,
                ...     guidance_scale=9.0,
                ...     generator=torch.Generator().manual_seed(0),
                ...     sampling_type="inverted_anti_drifting",
                ... ).frames[0]
                >>> export_to_video(output, "output.mp4", fps=30)
                ```
        
                ##### First and Last Image-to-Video
        
                ```python
                >>> import torch
                >>> from diffusers import HunyuanVideoFramepackPipeline, HunyuanVideoFramepackTransformer3DModel
                >>> from diffusers.utils import export_to_video, load_image
                >>> from transformers import SiglipImageProcessor, SiglipVisionModel
        
                >>> transformer = HunyuanVideoFramepackTransformer3DModel.from_pretrained(
                ...     "lllyasviel/FramePackI2V_HY", torch_dtype=torch.bfloat16
                ... )
                >>> feature_extractor = SiglipImageProcessor.from_pretrained(
                ...     "lllyasviel/flux_redux_bfl", subfolder="feature_extractor"
                ... )
                >>> image_encoder = SiglipVisionModel.from_pretrained(
                ...     "lllyasviel/flux_redux_bfl", subfolder="image_encoder", torch_dtype=torch.float16
                ... )
                >>> pipe = HunyuanVideoFramepackPipeline.from_pretrained(
                ...     "hunyuanvideo-community/HunyuanVideo",
                ...     transformer=transformer,
                ...     feature_extractor=feature_extractor,
                ...     image_encoder=image_encoder,
                ...     torch_dtype=torch.float16,
                ... )
                >>> pipe.to("cuda")
        
                >>> prompt = "CG animation style, a small blue bird takes off from the ground, flapping its wings. The bird's feathers are delicate, with a unique pattern on its chest. The background shows a blue sky with white clouds under bright sunshine. The camera follows the bird upward, capturing its flight and the vastness of the sky from a close-up, low-angle perspective."
                >>> first_image = load_image(
                ...     "https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/flf2v_input_first_frame.png"
                ... )
                >>> last_image = load_image(
                ...     "https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/flf2v_input_last_frame.png"
                ... )
                >>> output = pipe(
                ...     image=first_image,
                ...     last_image=last_image,
                ...     prompt=prompt,
                ...     height=512,
                ...     width=512,
                ...     num_frames=91,
                ...     num_inference_steps=30,
                ...     guidance_scale=9.0,
                ...     generator=torch.Generator().manual_seed(0),
                ...     sampling_type="inverted_anti_drifting",
                ... ).frames[0]
                >>> export_to_video(output, "output.mp4", fps=30)
                ```
        '''
    ),
    "src/diffusers/pipelines/hunyuan_video/pipeline_hunyuan_video_image2video.py": (
        '''
        Examples:
                ```python
                >>> import torch
                >>> from diffusers import HunyuanVideoImageToVideoPipeline, HunyuanVideoTransformer3DModel
                >>> from diffusers.utils import load_image, export_to_video
        
                >>> # Available checkpoints: hunyuanvideo-community/HunyuanVideo-I2V, hunyuanvideo-community/HunyuanVideo-I2V-33ch
                >>> model_id = "hunyuanvideo-community/HunyuanVideo-I2V"
                >>> transformer = HunyuanVideoTransformer3DModel.from_pretrained(
                ...     model_id, subfolder="transformer", torch_dtype=torch.bfloat16
                ... )
                >>> pipe = HunyuanVideoImageToVideoPipeline.from_pretrained(
                ...     model_id, transformer=transformer, torch_dtype=torch.float16
                ... )
                >>> pipe.vae.enable_tiling()
                >>> pipe.to("cuda")
        
                >>> prompt = "A man with short gray hair plays a red electric guitar."
                >>> image = load_image(
                ...     "https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/guitar-man.png"
                ... )
        
                >>> # If using hunyuanvideo-community/HunyuanVideo-I2V
                >>> output = pipe(image=image, prompt=prompt, guidance_scale=6.0).frames[0]
        
                >>> # If using hunyuanvideo-community/HunyuanVideo-I2V-33ch
                >>> output = pipe(image=image, prompt=prompt, guidance_scale=1.0, true_cfg_scale=1.0).frames[0]
        
                >>> export_to_video(output, "output.mp4", fps=15)
                ```
        '''
    ),
    "src/diffusers/pipelines/hunyuandit/pipeline_hunyuandit.py": (
        '''
        Examples:
                ```py
                >>> import torch
                >>> from diffusers import HunyuanDiTPipeline
        
                >>> pipe = HunyuanDiTPipeline.from_pretrained(
                ...     "Tencent-Hunyuan/HunyuanDiT-Diffusers", torch_dtype=torch.float16
                ... )
                >>> pipe.to("cuda")
        
                >>> # You may also use English prompt as HunyuanDiT supports both English and Chinese
                >>> # prompt = "An astronaut riding a horse"
                >>> prompt = "一个宇航员在骑马"
                >>> image = pipe(prompt).images[0]
                ```
        '''
    ),
    "src/diffusers/pipelines/i2vgen_xl/pipeline_i2vgen_xl.py": (
        '''
        Examples:
                ```py
                >>> import torch
                >>> from diffusers import I2VGenXLPipeline
                >>> from diffusers.utils import export_to_gif, load_image
        
                >>> pipeline = I2VGenXLPipeline.from_pretrained(
                ...     "ali-vilab/i2vgen-xl", torch_dtype=torch.float16, variant="fp16"
                ... )
                >>> pipeline.enable_model_cpu_offload()
        
                >>> image_url = (
                ...     "https://huggingface.co/datasets/diffusers/docs-images/resolve/main/i2vgen_xl_images/img_0009.png"
                ... )
                >>> image = load_image(image_url).convert("RGB")
        
                >>> prompt = "Papers were floating in the air on a table in the library"
                >>> negative_prompt = "Distorted, discontinuous, Ugly, blurry, low resolution, motionless, static, disfigured, disconnected limbs, Ugly faces, incomplete arms"
                >>> generator = torch.manual_seed(8888)
        
                >>> frames = pipeline(
                ...     prompt=prompt,
                ...     image=image,
                ...     num_inference_steps=50,
                ...     negative_prompt=negative_prompt,
                ...     guidance_scale=9.0,
                ...     generator=generator,
                ... ).frames[0]
                >>> video_path = export_to_gif(frames, "i2v.gif")
                ```
        '''
    ),
    "src/diffusers/pipelines/kandinsky/pipeline_kandinsky.py": (
        '''
        Examples:
                ```py
                >>> from diffusers import KandinskyPipeline, KandinskyPriorPipeline
                >>> import torch
        
                >>> pipe_prior = KandinskyPriorPipeline.from_pretrained("kandinsky-community/Kandinsky-2-1-prior")
                >>> pipe_prior.to("cuda")
        
                >>> prompt = "red cat, 4k photo"
                >>> out = pipe_prior(prompt)
                >>> image_emb = out.image_embeds
                >>> negative_image_emb = out.negative_image_embeds
        
                >>> pipe = KandinskyPipeline.from_pretrained("kandinsky-community/kandinsky-2-1")
                >>> pipe.to("cuda")
        
                >>> image = pipe(
                ...     prompt,
                ...     image_embeds=image_emb,
                ...     negative_image_embeds=negative_image_emb,
                ...     height=768,
                ...     width=768,
                ...     num_inference_steps=100,
                ... ).images
        
                >>> image[0].save("cat.png")
                ```
        '''
    ),
    "src/diffusers/pipelines/kandinsky/pipeline_kandinsky_combined.py": (
        '''
        Examples:
                ```py
                from diffusers import AutoPipelineForText2Image
                import torch
        
                pipe = AutoPipelineForText2Image.from_pretrained(
                    "kandinsky-community/kandinsky-2-1", torch_dtype=torch.float16
                )
                pipe.enable_model_cpu_offload()
        
                prompt = "A lion in galaxies, spirals, nebulae, stars, smoke, iridescent, intricate detail, octane render, 8k"
        
                image = pipe(prompt=prompt, num_inference_steps=25).images[0]
                ```
        '''
    ),
    "src/diffusers/pipelines/kandinsky/pipeline_kandinsky_img2img.py": (
        '''
        Examples:
                ```py
                >>> from diffusers import KandinskyImg2ImgPipeline, KandinskyPriorPipeline
                >>> from diffusers.utils import load_image
                >>> import torch
        
                >>> pipe_prior = KandinskyPriorPipeline.from_pretrained(
                ...     "kandinsky-community/kandinsky-2-1-prior", torch_dtype=torch.float16
                ... )
                >>> pipe_prior.to("cuda")
        
                >>> prompt = "A red cartoon frog, 4k"
                >>> image_emb, zero_image_emb = pipe_prior(prompt, return_dict=False)
        
                >>> pipe = KandinskyImg2ImgPipeline.from_pretrained(
                ...     "kandinsky-community/kandinsky-2-1", torch_dtype=torch.float16
                ... )
                >>> pipe.to("cuda")
        
                >>> init_image = load_image(
                ...     "https://huggingface.co/datasets/hf-internal-testing/diffusers-images/resolve/main"
                ...     "/kandinsky/frog.png"
                ... )
        
                >>> image = pipe(
                ...     prompt,
                ...     image=init_image,
                ...     image_embeds=image_emb,
                ...     negative_image_embeds=zero_image_emb,
                ...     height=768,
                ...     width=768,
                ...     num_inference_steps=100,
                ...     strength=0.2,
                ... ).images
        
                >>> image[0].save("red_frog.png")
                ```
        '''
    ),
    "src/diffusers/pipelines/kandinsky/pipeline_kandinsky_inpaint.py": (
        '''
        Examples:
                ```py
                >>> from diffusers import KandinskyInpaintPipeline, KandinskyPriorPipeline
                >>> from diffusers.utils import load_image
                >>> import torch
                >>> import numpy as np
        
                >>> pipe_prior = KandinskyPriorPipeline.from_pretrained(
                ...     "kandinsky-community/kandinsky-2-1-prior", torch_dtype=torch.float16
                ... )
                >>> pipe_prior.to("cuda")
        
                >>> prompt = "a hat"
                >>> image_emb, zero_image_emb = pipe_prior(prompt, return_dict=False)
        
                >>> pipe = KandinskyInpaintPipeline.from_pretrained(
                ...     "kandinsky-community/kandinsky-2-1-inpaint", torch_dtype=torch.float16
                ... )
                >>> pipe.to("cuda")
        
                >>> init_image = load_image(
                ...     "https://huggingface.co/datasets/hf-internal-testing/diffusers-images/resolve/main"
                ...     "/kandinsky/cat.png"
                ... )
        
                >>> mask = np.zeros((768, 768), dtype=np.float32)
                >>> mask[:250, 250:-250] = 1
        
                >>> out = pipe(
                ...     prompt,
                ...     image=init_image,
                ...     mask_image=mask,
                ...     image_embeds=image_emb,
                ...     negative_image_embeds=zero_image_emb,
                ...     height=768,
                ...     width=768,
                ...     num_inference_steps=50,
                ... )
        
                >>> image = out.images[0]
                >>> image.save("cat_with_hat.png")
                ```
        '''
    ),
    "src/diffusers/pipelines/kandinsky/pipeline_kandinsky_prior.py": (
        '''
        Examples:
                ```py
                >>> from diffusers import KandinskyPipeline, KandinskyPriorPipeline
                >>> import torch
        
                >>> pipe_prior = KandinskyPriorPipeline.from_pretrained("kandinsky-community/kandinsky-2-1-prior")
                >>> pipe_prior.to("cuda")
        
                >>> prompt = "red cat, 4k photo"
                >>> out = pipe_prior(prompt)
                >>> image_emb = out.image_embeds
                >>> negative_image_emb = out.negative_image_embeds
        
                >>> pipe = KandinskyPipeline.from_pretrained("kandinsky-community/kandinsky-2-1")
                >>> pipe.to("cuda")
        
                >>> image = pipe(
                ...     prompt,
                ...     image_embeds=image_emb,
                ...     negative_image_embeds=negative_image_emb,
                ...     height=768,
                ...     width=768,
                ...     num_inference_steps=100,
                ... ).images
        
                >>> image[0].save("cat.png")
                ```
        '''
    ),
    "src/diffusers/pipelines/kandinsky2_2/pipeline_kandinsky2_2.py": (
        '''
        Examples:
                ```py
                >>> from diffusers import KandinskyV22Pipeline, KandinskyV22PriorPipeline
                >>> import torch
        
                >>> pipe_prior = KandinskyV22PriorPipeline.from_pretrained("kandinsky-community/kandinsky-2-2-prior")
                >>> pipe_prior.to("cuda")
                >>> prompt = "red cat, 4k photo"
                >>> out = pipe_prior(prompt)
                >>> image_emb = out.image_embeds
                >>> zero_image_emb = out.negative_image_embeds
                >>> pipe = KandinskyV22Pipeline.from_pretrained("kandinsky-community/kandinsky-2-2-decoder")
                >>> pipe.to("cuda")
                >>> image = pipe(
                ...     image_embeds=image_emb,
                ...     negative_image_embeds=zero_image_emb,
                ...     height=768,
                ...     width=768,
                ...     num_inference_steps=50,
                ... ).images
                >>> image[0].save("cat.png")
                ```
        '''
    ),
    "src/diffusers/pipelines/kandinsky2_2/pipeline_kandinsky2_2_combined.py": (
        '''
        Examples:
                ```py
                from diffusers import AutoPipelineForText2Image
                import torch
        
                pipe = AutoPipelineForText2Image.from_pretrained(
                    "kandinsky-community/kandinsky-2-2-decoder", torch_dtype=torch.float16
                )
                pipe.enable_model_cpu_offload()
        
                prompt = "A lion in galaxies, spirals, nebulae, stars, smoke, iridescent, intricate detail, octane render, 8k"
        
                image = pipe(prompt=prompt, num_inference_steps=25).images[0]
                ```
        '''
    ),
    "src/diffusers/pipelines/kandinsky2_2/pipeline_kandinsky2_2_controlnet.py": (
        '''
        Examples:
                ```py
                >>> import torch
                >>> import numpy as np
        
                >>> from diffusers import KandinskyV22PriorPipeline, KandinskyV22ControlnetPipeline
                >>> from transformers import pipeline
                >>> from diffusers.utils import load_image
        
        
                >>> def make_hint(image, depth_estimator):
                ...     image = depth_estimator(image)["depth"]
                ...     image = np.array(image)
                ...     image = image[:, :, None]
                ...     image = np.concatenate([image, image, image], axis=2)
                ...     detected_map = torch.from_numpy(image).float() / 255.0
                ...     hint = detected_map.permute(2, 0, 1)
                ...     return hint
        
        
                >>> depth_estimator = pipeline("depth-estimation")
        
                >>> pipe_prior = KandinskyV22PriorPipeline.from_pretrained(
                ...     "kandinsky-community/kandinsky-2-2-prior", torch_dtype=torch.float16
                ... )
                >>> pipe_prior = pipe_prior.to("cuda")
        
                >>> pipe = KandinskyV22ControlnetPipeline.from_pretrained(
                ...     "kandinsky-community/kandinsky-2-2-controlnet-depth", torch_dtype=torch.float16
                ... )
                >>> pipe = pipe.to("cuda")
        
        
                >>> img = load_image(
                ...     "https://huggingface.co/datasets/hf-internal-testing/diffusers-images/resolve/main"
                ...     "/kandinsky/cat.png"
                ... ).resize((768, 768))
        
                >>> hint = make_hint(img, depth_estimator).unsqueeze(0).half().to("cuda")
        
                >>> prompt = "A robot, 4k photo"
                >>> negative_prior_prompt = "lowres, text, error, cropped, worst quality, low quality, jpeg artifacts, ugly, duplicate, morbid, mutilated, out of frame, extra fingers, mutated hands, poorly drawn hands, poorly drawn face, mutation, deformed, blurry, dehydrated, bad anatomy, bad proportions, extra limbs, cloned face, disfigured, gross proportions, malformed limbs, missing arms, missing legs, extra arms, extra legs, fused fingers, too many fingers, long neck, username, watermark, signature"
        
                >>> generator = torch.Generator(device="cuda").manual_seed(43)
        
                >>> image_emb, zero_image_emb = pipe_prior(
                ...     prompt=prompt, negative_prompt=negative_prior_prompt, generator=generator
                ... ).to_tuple()
        
                >>> images = pipe(
                ...     image_embeds=image_emb,
                ...     negative_image_embeds=zero_image_emb,
                ...     hint=hint,
                ...     num_inference_steps=50,
                ...     generator=generator,
                ...     height=768,
                ...     width=768,
                ... ).images
        
                >>> images[0].save("robot_cat.png")
                ```
        '''
    ),
    "src/diffusers/pipelines/kandinsky2_2/pipeline_kandinsky2_2_controlnet_img2img.py": (
        '''
        Examples:
                ```py
                >>> import torch
                >>> import numpy as np
        
                >>> from diffusers import KandinskyV22PriorEmb2EmbPipeline, KandinskyV22ControlnetImg2ImgPipeline
                >>> from transformers import pipeline
                >>> from diffusers.utils import load_image
        
        
                >>> def make_hint(image, depth_estimator):
                ...     image = depth_estimator(image)["depth"]
                ...     image = np.array(image)
                ...     image = image[:, :, None]
                ...     image = np.concatenate([image, image, image], axis=2)
                ...     detected_map = torch.from_numpy(image).float() / 255.0
                ...     hint = detected_map.permute(2, 0, 1)
                ...     return hint
        
        
                >>> depth_estimator = pipeline("depth-estimation")
        
                >>> pipe_prior = KandinskyV22PriorEmb2EmbPipeline.from_pretrained(
                ...     "kandinsky-community/kandinsky-2-2-prior", torch_dtype=torch.float16
                ... )
                >>> pipe_prior = pipe_prior.to("cuda")
        
                >>> pipe = KandinskyV22ControlnetImg2ImgPipeline.from_pretrained(
                ...     "kandinsky-community/kandinsky-2-2-controlnet-depth", torch_dtype=torch.float16
                ... )
                >>> pipe = pipe.to("cuda")
        
                >>> img = load_image(
                ...     "https://huggingface.co/datasets/hf-internal-testing/diffusers-images/resolve/main"
                ...     "/kandinsky/cat.png"
                ... ).resize((768, 768))
        
        
                >>> hint = make_hint(img, depth_estimator).unsqueeze(0).half().to("cuda")
        
                >>> prompt = "A robot, 4k photo"
                >>> negative_prior_prompt = "lowres, text, error, cropped, worst quality, low quality, jpeg artifacts, ugly, duplicate, morbid, mutilated, out of frame, extra fingers, mutated hands, poorly drawn hands, poorly drawn face, mutation, deformed, blurry, dehydrated, bad anatomy, bad proportions, extra limbs, cloned face, disfigured, gross proportions, malformed limbs, missing arms, missing legs, extra arms, extra legs, fused fingers, too many fingers, long neck, username, watermark, signature"
        
                >>> generator = torch.Generator(device="cuda").manual_seed(43)
        
                >>> img_emb = pipe_prior(prompt=prompt, image=img, strength=0.85, generator=generator)
                >>> negative_emb = pipe_prior(prompt=negative_prior_prompt, image=img, strength=1, generator=generator)
        
                >>> images = pipe(
                ...     image=img,
                ...     strength=0.5,
                ...     image_embeds=img_emb.image_embeds,
                ...     negative_image_embeds=negative_emb.image_embeds,
                ...     hint=hint,
                ...     num_inference_steps=50,
                ...     generator=generator,
                ...     height=768,
                ...     width=768,
                ... ).images
        
                >>> images[0].save("robot_cat.png")
                ```
        '''
    ),
    "src/diffusers/pipelines/kandinsky2_2/pipeline_kandinsky2_2_img2img.py": (
        '''
        Examples:
                ```py
                >>> from diffusers import KandinskyV22Img2ImgPipeline, KandinskyV22PriorPipeline
                >>> from diffusers.utils import load_image
                >>> import torch
        
                >>> pipe_prior = KandinskyV22PriorPipeline.from_pretrained(
                ...     "kandinsky-community/kandinsky-2-2-prior", torch_dtype=torch.float16
                ... )
                >>> pipe_prior.to("cuda")
        
                >>> prompt = "A red cartoon frog, 4k"
                >>> image_emb, zero_image_emb = pipe_prior(prompt, return_dict=False)
        
                >>> pipe = KandinskyV22Img2ImgPipeline.from_pretrained(
                ...     "kandinsky-community/kandinsky-2-2-decoder", torch_dtype=torch.float16
                ... )
                >>> pipe.to("cuda")
        
                >>> init_image = load_image(
                ...     "https://huggingface.co/datasets/hf-internal-testing/diffusers-images/resolve/main"
                ...     "/kandinsky/frog.png"
                ... )
        
                >>> image = pipe(
                ...     image=init_image,
                ...     image_embeds=image_emb,
                ...     negative_image_embeds=zero_image_emb,
                ...     height=768,
                ...     width=768,
                ...     num_inference_steps=100,
                ...     strength=0.2,
                ... ).images
        
                >>> image[0].save("red_frog.png")
                ```
        '''
    ),
    "src/diffusers/pipelines/kandinsky2_2/pipeline_kandinsky2_2_inpainting.py": (
        '''
        Examples:
                ```py
                >>> from diffusers import KandinskyV22InpaintPipeline, KandinskyV22PriorPipeline
                >>> from diffusers.utils import load_image
                >>> import torch
                >>> import numpy as np
        
                >>> pipe_prior = KandinskyV22PriorPipeline.from_pretrained(
                ...     "kandinsky-community/kandinsky-2-2-prior", torch_dtype=torch.float16
                ... )
                >>> pipe_prior.to("cuda")
        
                >>> prompt = "a hat"
                >>> image_emb, zero_image_emb = pipe_prior(prompt, return_dict=False)
        
                >>> pipe = KandinskyV22InpaintPipeline.from_pretrained(
                ...     "kandinsky-community/kandinsky-2-2-decoder-inpaint", torch_dtype=torch.float16
                ... )
                >>> pipe.to("cuda")
        
                >>> init_image = load_image(
                ...     "https://huggingface.co/datasets/hf-internal-testing/diffusers-images/resolve/main"
                ...     "/kandinsky/cat.png"
                ... )
        
                >>> mask = np.zeros((768, 768), dtype=np.float32)
                >>> mask[:250, 250:-250] = 1
        
                >>> out = pipe(
                ...     image=init_image,
                ...     mask_image=mask,
                ...     image_embeds=image_emb,
                ...     negative_image_embeds=zero_image_emb,
                ...     height=768,
                ...     width=768,
                ...     num_inference_steps=50,
                ... )
        
                >>> image = out.images[0]
                >>> image.save("cat_with_hat.png")
                ```
        '''
    ),
    "src/diffusers/pipelines/kandinsky2_2/pipeline_kandinsky2_2_prior.py": (
        '''
        Examples:
                ```py
                >>> from diffusers import KandinskyV22Pipeline, KandinskyV22PriorPipeline
                >>> import torch
        
                >>> pipe_prior = KandinskyV22PriorPipeline.from_pretrained("kandinsky-community/kandinsky-2-2-prior")
                >>> pipe_prior.to("cuda")
                >>> prompt = "red cat, 4k photo"
                >>> image_emb, negative_image_emb = pipe_prior(prompt).to_tuple()
        
                >>> pipe = KandinskyV22Pipeline.from_pretrained("kandinsky-community/kandinsky-2-2-decoder")
                >>> pipe.to("cuda")
                >>> image = pipe(
                ...     image_embeds=image_emb,
                ...     negative_image_embeds=negative_image_emb,
                ...     height=768,
                ...     width=768,
                ...     num_inference_steps=50,
                ... ).images
                >>> image[0].save("cat.png")
                ```
        '''
    ),
    "src/diffusers/pipelines/kandinsky2_2/pipeline_kandinsky2_2_prior_emb2emb.py": (
        '''
        Examples:
                ```py
                >>> from diffusers import KandinskyV22Pipeline, KandinskyV22PriorEmb2EmbPipeline
                >>> import torch
        
                >>> pipe_prior = KandinskyPriorPipeline.from_pretrained(
                ...     "kandinsky-community/kandinsky-2-2-prior", torch_dtype=torch.float16
                ... )
                >>> pipe_prior.to("cuda")
        
                >>> prompt = "red cat, 4k photo"
                >>> img = load_image(
                ...     "https://huggingface.co/datasets/hf-internal-testing/diffusers-images/resolve/main"
                ...     "/kandinsky/cat.png"
                ... )
                >>> image_emb, nagative_image_emb = pipe_prior(prompt, image=img, strength=0.2).to_tuple()
        
                >>> pipe = KandinskyPipeline.from_pretrained(
                ...     "kandinsky-community/kandinsky-2-2-decoder, torch_dtype=torch.float16"
                ... )
                >>> pipe.to("cuda")
        
                >>> image = pipe(
                ...     image_embeds=image_emb,
                ...     negative_image_embeds=negative_image_emb,
                ...     height=768,
                ...     width=768,
                ...     num_inference_steps=100,
                ... ).images
        
                >>> image[0].save("cat.png")
                ```
        '''
    ),
    "src/diffusers/pipelines/kandinsky3/pipeline_kandinsky3.py": (
        '''
        Examples:
                ```py
                >>> from diffusers import AutoPipelineForText2Image
                >>> import torch
        
                >>> pipe = AutoPipelineForText2Image.from_pretrained(
                ...     "kandinsky-community/kandinsky-3", variant="fp16", torch_dtype=torch.float16
                ... )
                >>> pipe.enable_model_cpu_offload()
        
                >>> prompt = "A photograph of the inside of a subway train. There are raccoons sitting on the seats. One of them is reading a newspaper. The window shows the city in the background."
        
                >>> generator = torch.Generator(device="cpu").manual_seed(0)
                >>> image = pipe(prompt, num_inference_steps=25, generator=generator).images[0]
                ```
        '''
    ),
    "src/diffusers/pipelines/kandinsky3/pipeline_kandinsky3_img2img.py": (
        '''
        Examples:
                ```py
                >>> from diffusers import AutoPipelineForImage2Image
                >>> from diffusers.utils import load_image
                >>> import torch
        
                >>> pipe = AutoPipelineForImage2Image.from_pretrained(
                ...     "kandinsky-community/kandinsky-3", variant="fp16", torch_dtype=torch.float16
                ... )
                >>> pipe.enable_model_cpu_offload()
        
                >>> prompt = "A painting of the inside of a subway train with tiny raccoons."
                >>> image = load_image(
                ...     "https://huggingface.co/datasets/hf-internal-testing/diffusers-images/resolve/main/kandinsky3/t2i.png"
                ... )
        
                >>> generator = torch.Generator(device="cpu").manual_seed(0)
                >>> image = pipe(prompt, image=image, strength=0.75, num_inference_steps=25, generator=generator).images[0]
                ```
        '''
    ),
    "src/diffusers/pipelines/kolors/pipeline_kolors.py": (
        '''
        Examples:
                ```py
                >>> import torch
                >>> from diffusers import KolorsPipeline
        
                >>> pipe = KolorsPipeline.from_pretrained(
                ...     "Kwai-Kolors/Kolors-diffusers", variant="fp16", torch_dtype=torch.float16
                ... )
                >>> pipe = pipe.to("cuda")
        
                >>> prompt = (
                ...     "A photo of a ladybug, macro, zoom, high quality, film, holding a wooden sign with the text 'KOLORS'"
                ... )
                >>> image = pipe(prompt).images[0]
                ```
        '''
    ),
    "src/diffusers/pipelines/kolors/pipeline_kolors_img2img.py": (
        '''
        Examples:
                ```py
                >>> import torch
                >>> from diffusers import KolorsImg2ImgPipeline
                >>> from diffusers.utils import load_image
        
                >>> pipe = KolorsImg2ImgPipeline.from_pretrained(
                ...     "Kwai-Kolors/Kolors-diffusers", variant="fp16", torch_dtype=torch.float16
                ... )
                >>> pipe = pipe.to("cuda")
                >>> url = (
                ...     "https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/kolors/bunny_source.png"
                ... )
        
        
                >>> init_image = load_image(url)
                >>> prompt = "high quality image of a capybara wearing sunglasses. In the background of the image there are trees, poles, grass and other objects. At the bottom of the object there is the road., 8k, highly detailed."
                >>> image = pipe(prompt, image=init_image).images[0]
                ```
        '''
    ),
    "src/diffusers/pipelines/latent_consistency_models/pipeline_latent_consistency_img2img.py": (
        '''
        Examples:
                ```py
                >>> from diffusers import AutoPipelineForImage2Image
                >>> import torch
                >>> import PIL
        
                >>> pipe = AutoPipelineForImage2Image.from_pretrained("SimianLuo/LCM_Dreamshaper_v7")
                >>> # To save GPU memory, torch.float16 can be used, but it may compromise image quality.
                >>> pipe.to(torch_device="cuda", torch_dtype=torch.float32)
        
                >>> prompt = "High altitude snowy mountains"
                >>> image = PIL.Image.open("./snowy_mountains.png")
        
                >>> # Can be set to 1~50 steps. LCM support fast inference even <= 4 steps. Recommend: 1~8 steps.
                >>> num_inference_steps = 4
                >>> images = pipe(
                ...     prompt=prompt, image=image, num_inference_steps=num_inference_steps, guidance_scale=8.0
                ... ).images
        
                >>> images[0].save("image.png")
                ```
        '''
    ),
    "src/diffusers/pipelines/latent_consistency_models/pipeline_latent_consistency_text2img.py": (
        '''
        Examples:
                ```py
                >>> from diffusers import DiffusionPipeline
                >>> import torch
        
                >>> pipe = DiffusionPipeline.from_pretrained("SimianLuo/LCM_Dreamshaper_v7")
                >>> # To save GPU memory, torch.float16 can be used, but it may compromise image quality.
                >>> pipe.to(torch_device="cuda", torch_dtype=torch.float32)
        
                >>> prompt = "Self-portrait oil painting, a beautiful cyborg with golden hair, 8k"
        
                >>> # Can be set to 1~50 steps. LCM support fast inference even <= 4 steps. Recommend: 1~8 steps.
                >>> num_inference_steps = 4
                >>> images = pipe(prompt=prompt, num_inference_steps=num_inference_steps, guidance_scale=8.0).images
                >>> images[0].save("image.png")
                ```
        '''
    ),
    "src/diffusers/pipelines/latte/pipeline_latte.py": (
        '''
        Examples:
                ```py
                >>> import torch
                >>> from diffusers import LattePipeline
                >>> from diffusers.utils import export_to_gif
        
                >>> # You can replace the checkpoint id with "maxin-cn/Latte-1" too.
                >>> pipe = LattePipeline.from_pretrained("maxin-cn/Latte-1", torch_dtype=torch.float16)
                >>> # Enable memory optimizations.
                >>> pipe.enable_model_cpu_offload()
        
                >>> prompt = "A small cactus with a happy face in the Sahara desert."
                >>> videos = pipe(prompt).frames[0]
                >>> export_to_gif(videos, "latte.gif")
                ```
        '''
    ),
    "src/diffusers/pipelines/ledits_pp/pipeline_leditspp_stable_diffusion.py": (
        '''
        Examples:
                ```py
                >>> import torch
        
                >>> from diffusers import LEditsPPPipelineStableDiffusion
                >>> from diffusers.utils import load_image
        
                >>> pipe = LEditsPPPipelineStableDiffusion.from_pretrained(
                ...     "runwayml/stable-diffusion-v1-5", variant="fp16", torch_dtype=torch.float16
                ... )
                >>> pipe.enable_vae_tiling()
                >>> pipe = pipe.to("cuda")
        
                >>> img_url = "https://www.aiml.informatik.tu-darmstadt.de/people/mbrack/cherry_blossom.png"
                >>> image = load_image(img_url).resize((512, 512))
        
                >>> _ = pipe.invert(image=image, num_inversion_steps=50, skip=0.1)
        
                >>> edited_image = pipe(
                ...     editing_prompt=["cherry blossom"], edit_guidance_scale=10.0, edit_threshold=0.75
                ... ).images[0]
                ```
        '''
    ),
    "src/diffusers/pipelines/ledits_pp/pipeline_leditspp_stable_diffusion_xl.py": (
        '''
        Examples:
                ```py
                >>> import torch
        
                >>> from diffusers import LEditsPPPipelineStableDiffusionXL
                >>> from diffusers.utils import load_image
        
                >>> pipe = LEditsPPPipelineStableDiffusionXL.from_pretrained(
                ...     "stabilityai/stable-diffusion-xl-base-1.0", variant="fp16", torch_dtype=torch.float16
                ... )
                >>> pipe.enable_vae_tiling()
                >>> pipe = pipe.to("cuda")
        
                >>> img_url = "https://www.aiml.informatik.tu-darmstadt.de/people/mbrack/tennis.jpg"
                >>> image = load_image(img_url).resize((1024, 1024))
        
                >>> _ = pipe.invert(image=image, num_inversion_steps=50, skip=0.2)
        
                >>> edited_image = pipe(
                ...     editing_prompt=["tennis ball", "tomato"],
                ...     reverse_editing_direction=[True, False],
                ...     edit_guidance_scale=[5.0, 10.0],
                ...     edit_threshold=[0.9, 0.85],
                ... ).images[0]
                ```
        '''
    ),
    "src/diffusers/pipelines/ltx/pipeline_ltx.py": (
        '''
        Examples:
                ```py
                >>> import torch
                >>> from diffusers import LTXPipeline
                >>> from diffusers.utils import export_to_video
        
                >>> pipe = LTXPipeline.from_pretrained("Lightricks/LTX-Video", torch_dtype=torch.bfloat16)
                >>> pipe.to("cuda")
        
                >>> prompt = "A woman with long brown hair and light skin smiles at another woman with long blonde hair. The woman with brown hair wears a black jacket and has a small, barely noticeable mole on her right cheek. The camera angle is a close-up, focused on the woman with brown hair's face. The lighting is warm and natural, likely from the setting sun, casting a soft glow on the scene. The scene appears to be real-life footage"
                >>> negative_prompt = "worst quality, inconsistent motion, blurry, jittery, distorted"
        
                >>> video = pipe(
                ...     prompt=prompt,
                ...     negative_prompt=negative_prompt,
                ...     width=704,
                ...     height=480,
                ...     num_frames=161,
                ...     num_inference_steps=50,
                ... ).frames[0]
                >>> export_to_video(video, "output.mp4", fps=24)
                ```
        '''
    ),
    "src/diffusers/pipelines/ltx/pipeline_ltx_condition.py": (
        '''
        Examples:
                ```py
                >>> import torch
                >>> from diffusers.pipelines.ltx.pipeline_ltx_condition import LTXConditionPipeline, LTXVideoCondition
                >>> from diffusers.utils import export_to_video, load_video, load_image
        
                >>> pipe = LTXConditionPipeline.from_pretrained("Lightricks/LTX-Video-0.9.5", torch_dtype=torch.bfloat16)
                >>> pipe.to("cuda")
        
                >>> # Load input image and video
                >>> video = load_video(
                ...     "https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/cosmos/cosmos-video2world-input-vid.mp4"
                ... )
                >>> image = load_image(
                ...     "https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/cosmos/cosmos-video2world-input.jpg"
                ... )
        
                >>> # Create conditioning objects
                >>> condition1 = LTXVideoCondition(
                ...     image=image,
                ...     frame_index=0,
                ... )
                >>> condition2 = LTXVideoCondition(
                ...     video=video,
                ...     frame_index=80,
                ... )
        
                >>> prompt = "The video depicts a long, straight highway stretching into the distance, flanked by metal guardrails. The road is divided into multiple lanes, with a few vehicles visible in the far distance. The surrounding landscape features dry, grassy fields on one side and rolling hills on the other. The sky is mostly clear with a few scattered clouds, suggesting a bright, sunny day. And then the camera switch to a winding mountain road covered in snow, with a single vehicle traveling along it. The road is flanked by steep, rocky cliffs and sparse vegetation. The landscape is characterized by rugged terrain and a river visible in the distance. The scene captures the solitude and beauty of a winter drive through a mountainous region."
                >>> negative_prompt = "worst quality, inconsistent motion, blurry, jittery, distorted"
        
                >>> # Generate video
                >>> generator = torch.Generator("cuda").manual_seed(0)
                >>> # Text-only conditioning is also supported without the need to pass `conditions`
                >>> video = pipe(
                ...     conditions=[condition1, condition2],
                ...     prompt=prompt,
                ...     negative_prompt=negative_prompt,
                ...     width=768,
                ...     height=512,
                ...     num_frames=161,
                ...     num_inference_steps=40,
                ...     generator=generator,
                ... ).frames[0]
        
                >>> export_to_video(video, "output.mp4", fps=24)
                ```
        '''
    ),
    "src/diffusers/pipelines/ltx/pipeline_ltx_image2video.py": (
        '''
        Examples:
                ```py
                >>> import torch
                >>> from diffusers import LTXImageToVideoPipeline
                >>> from diffusers.utils import export_to_video, load_image
        
                >>> pipe = LTXImageToVideoPipeline.from_pretrained("Lightricks/LTX-Video", torch_dtype=torch.bfloat16)
                >>> pipe.to("cuda")
        
                >>> image = load_image(
                ...     "https://huggingface.co/datasets/a-r-r-o-w/tiny-meme-dataset-captioned/resolve/main/images/8.png"
                ... )
                >>> prompt = "A young girl stands calmly in the foreground, looking directly at the camera, as a house fire rages in the background. Flames engulf the structure, with smoke billowing into the air. Firefighters in protective gear rush to the scene, a fire truck labeled '38' visible behind them. The girl's neutral expression contrasts sharply with the chaos of the fire, creating a poignant and emotionally charged scene."
                >>> negative_prompt = "worst quality, inconsistent motion, blurry, jittery, distorted"
        
                >>> video = pipe(
                ...     image=image,
                ...     prompt=prompt,
                ...     negative_prompt=negative_prompt,
                ...     width=704,
                ...     height=480,
                ...     num_frames=161,
                ...     num_inference_steps=50,
                ... ).frames[0]
                >>> export_to_video(video, "output.mp4", fps=24)
                ```
        '''
    ),
    "src/diffusers/pipelines/lumina/pipeline_lumina.py": (
        '''
        Examples:
                ```py
                >>> import torch
                >>> from diffusers import LuminaPipeline
        
                >>> pipe = LuminaPipeline.from_pretrained("Alpha-VLLM/Lumina-Next-SFT-diffusers", torch_dtype=torch.bfloat16)
                >>> # Enable memory optimizations.
                >>> pipe.enable_model_cpu_offload()
        
                >>> prompt = "Upper body of a young woman in a Victorian-era outfit with brass goggles and leather straps. Background shows an industrial revolution cityscape with smoky skies and tall, metal structures"
                >>> image = pipe(prompt).images[0]
                ```
        '''
    ),
    "src/diffusers/pipelines/lumina2/pipeline_lumina2.py": (
        '''
        Examples:
                ```py
                >>> import torch
                >>> from diffusers import Lumina2Pipeline
        
                >>> pipe = Lumina2Pipeline.from_pretrained("Alpha-VLLM/Lumina-Image-2.0", torch_dtype=torch.bfloat16)
                >>> # Enable memory optimizations.
                >>> pipe.enable_model_cpu_offload()
        
                >>> prompt = "Upper body of a young woman in a Victorian-era outfit with brass goggles and leather straps. Background shows an industrial revolution cityscape with smoky skies and tall, metal structures"
                >>> image = pipe(prompt).images[0]
                ```
        '''
    ),
    "src/diffusers/pipelines/marigold/pipeline_marigold_depth.py": (
        '''
        Examples:
        ```py
        >>> import diffusers
        >>> import torch
        
        >>> pipe = diffusers.MarigoldDepthPipeline.from_pretrained(
        ...     "prs-eth/marigold-depth-v1-1", variant="fp16", torch_dtype=torch.float16
        ... ).to("cuda")
        
        >>> image = diffusers.utils.load_image("https://marigoldmonodepth.github.io/images/einstein.jpg")
        >>> depth = pipe(image)
        
        >>> vis = pipe.image_processor.visualize_depth(depth.prediction)
        >>> vis[0].save("einstein_depth.png")
        
        >>> depth_16bit = pipe.image_processor.export_depth_to_16bit_png(depth.prediction)
        >>> depth_16bit[0].save("einstein_depth_16bit.png")
        ```
        '''
    ),
    "src/diffusers/pipelines/marigold/pipeline_marigold_intrinsics.py": (
        '''
        Examples:
        ```py
        >>> import diffusers
        >>> import torch
        
        >>> pipe = diffusers.MarigoldIntrinsicsPipeline.from_pretrained(
        ...     "prs-eth/marigold-iid-appearance-v1-1", variant="fp16", torch_dtype=torch.float16
        ... ).to("cuda")
        
        >>> image = diffusers.utils.load_image("https://marigoldmonodepth.github.io/images/einstein.jpg")
        >>> intrinsics = pipe(image)
        
        >>> vis = pipe.image_processor.visualize_intrinsics(intrinsics.prediction, pipe.target_properties)
        >>> vis[0]["albedo"].save("einstein_albedo.png")
        >>> vis[0]["roughness"].save("einstein_roughness.png")
        >>> vis[0]["metallicity"].save("einstein_metallicity.png")
        ```
        ```py
        >>> import diffusers
        >>> import torch
        
        >>> pipe = diffusers.MarigoldIntrinsicsPipeline.from_pretrained(
        ...     "prs-eth/marigold-iid-lighting-v1-1", variant="fp16", torch_dtype=torch.float16
        ... ).to("cuda")
        
        >>> image = diffusers.utils.load_image("https://marigoldmonodepth.github.io/images/einstein.jpg")
        >>> intrinsics = pipe(image)
        
        >>> vis = pipe.image_processor.visualize_intrinsics(intrinsics.prediction, pipe.target_properties)
        >>> vis[0]["albedo"].save("einstein_albedo.png")
        >>> vis[0]["shading"].save("einstein_shading.png")
        >>> vis[0]["residual"].save("einstein_residual.png")
        ```
        '''
    ),
    "src/diffusers/pipelines/marigold/pipeline_marigold_normals.py": (
        '''
        Examples:
        ```py
        >>> import diffusers
        >>> import torch
        
        >>> pipe = diffusers.MarigoldNormalsPipeline.from_pretrained(
        ...     "prs-eth/marigold-normals-v1-1", variant="fp16", torch_dtype=torch.float16
        ... ).to("cuda")
        
        >>> image = diffusers.utils.load_image("https://marigoldmonodepth.github.io/images/einstein.jpg")
        >>> normals = pipe(image)
        
        >>> vis = pipe.image_processor.visualize_normals(normals.prediction)
        >>> vis[0].save("einstein_normals.png")
        ```
        '''
    ),
    "src/diffusers/pipelines/mochi/pipeline_mochi.py": (
        '''
        Examples:
                ```py
                >>> import torch
                >>> from diffusers import MochiPipeline
                >>> from diffusers.utils import export_to_video
        
                >>> pipe = MochiPipeline.from_pretrained("genmo/mochi-1-preview", torch_dtype=torch.bfloat16)
                >>> pipe.enable_model_cpu_offload()
                >>> pipe.enable_vae_tiling()
                >>> prompt = "Close-up of a chameleon's eye, with its scaly skin changing color. Ultra high resolution 4k."
                >>> frames = pipe(prompt, num_inference_steps=28, guidance_scale=3.5).frames[0]
                >>> export_to_video(frames, "mochi.mp4")
                ```
        '''
    ),
    "src/diffusers/pipelines/musicldm/pipeline_musicldm.py": (
        '''
        Examples:
                ```py
                >>> from diffusers import MusicLDMPipeline
                >>> import torch
                >>> import scipy
        
                >>> repo_id = "ucsd-reach/musicldm"
                >>> pipe = MusicLDMPipeline.from_pretrained(repo_id, torch_dtype=torch.float16)
                >>> pipe = pipe.to("cuda")
        
                >>> prompt = "Techno music with a strong, upbeat tempo and high melodic riffs"
                >>> audio = pipe(prompt, num_inference_steps=10, audio_length_in_s=5.0).audios[0]
        
                >>> # save the audio sample as a .wav file
                >>> scipy.io.wavfile.write("techno.wav", rate=16000, data=audio)
                ```
        '''
    ),
    "src/diffusers/pipelines/omnigen/pipeline_omnigen.py": (
        '''
        Examples:
                ```py
                >>> import torch
                >>> from diffusers import OmniGenPipeline
        
                >>> pipe = OmniGenPipeline.from_pretrained("Shitao/OmniGen-v1-diffusers", torch_dtype=torch.bfloat16)
                >>> pipe.to("cuda")
        
                >>> prompt = "A cat holding a sign that says hello world"
                >>> # Depending on the variant being used, the pipeline call will slightly vary.
                >>> # Refer to the pipeline documentation for more details.
                >>> image = pipe(prompt, num_inference_steps=50, guidance_scale=2.5).images[0]
                >>> image.save("output.png")
                ```
        '''
    ),
    "src/diffusers/pipelines/pag/pipeline_pag_controlnet_sd.py": (
        '''
        Examples:
                ```py
                >>> # !pip install opencv-python transformers accelerate
                >>> from diffusers import AutoPipelineForText2Image, ControlNetModel, UniPCMultistepScheduler
                >>> from diffusers.utils import load_image
                >>> import numpy as np
                >>> import torch
        
                >>> import cv2
                >>> from PIL import Image
        
                >>> # download an image
                >>> image = load_image(
                ...     "https://hf.co/datasets/hf-internal-testing/diffusers-images/resolve/main/sd_controlnet/hf-logo.png"
                ... )
                >>> image = np.array(image)
        
                >>> # get canny image
                >>> image = cv2.Canny(image, 100, 200)
                >>> image = image[:, :, None]
                >>> image = np.concatenate([image, image, image], axis=2)
                >>> canny_image = Image.fromarray(image)
        
                >>> # load control net and stable diffusion v1-5
                >>> controlnet = ControlNetModel.from_pretrained("lllyasviel/sd-controlnet-canny", torch_dtype=torch.float16)
                >>> pipe = AutoPipelineForText2Image.from_pretrained(
                ...     "runwayml/stable-diffusion-v1-5", controlnet=controlnet, torch_dtype=torch.float16, enable_pag=True
                ... )
        
                >>> # speed up diffusion process with faster scheduler and memory optimization
                >>> # remove following line if xformers is not installed
                >>> pipe.enable_xformers_memory_efficient_attention()
        
                >>> pipe.enable_model_cpu_offload()
        
                >>> # generate image
                >>> generator = torch.manual_seed(0)
                >>> image = pipe(
                ...     "aerial view, a futuristic research complex in a bright foggy jungle, hard lighting",
                ...     guidance_scale=7.5,
                ...     generator=generator,
                ...     image=canny_image,
                ...     pag_scale=10,
                ... ).images[0]
                ```
        '''
    ),
    "src/diffusers/pipelines/pag/pipeline_pag_controlnet_sd_inpaint.py": (
        '''
        Examples:
                ```py
                >>> # !pip install transformers accelerate
                >>> import cv2
                >>> from diffusers import AutoPipelineForInpainting, ControlNetModel, DDIMScheduler
                >>> from diffusers.utils import load_image
                >>> import numpy as np
                >>> from PIL import Image
                >>> import torch
        
                >>> init_image = load_image(
                ...     "https://huggingface.co/datasets/diffusers/test-arrays/resolve/main/stable_diffusion_inpaint/boy.png"
                ... )
                >>> init_image = init_image.resize((512, 512))
        
                >>> generator = torch.Generator(device="cpu").manual_seed(1)
        
                >>> mask_image = load_image(
                ...     "https://huggingface.co/datasets/diffusers/test-arrays/resolve/main/stable_diffusion_inpaint/boy_mask.png"
                ... )
                >>> mask_image = mask_image.resize((512, 512))
        
        
                >>> def make_canny_condition(image):
                ...     image = np.array(image)
                ...     image = cv2.Canny(image, 100, 200)
                ...     image = image[:, :, None]
                ...     image = np.concatenate([image, image, image], axis=2)
                ...     image = Image.fromarray(image)
                ...     return image
        
        
                >>> control_image = make_canny_condition(init_image)
        
                >>> controlnet = ControlNetModel.from_pretrained(
                ...     "lllyasviel/control_v11p_sd15_inpaint", torch_dtype=torch.float16
                ... )
                >>> pipe = AutoPipelineForInpainting.from_pretrained(
                ...     "runwayml/stable-diffusion-v1-5", controlnet=controlnet, torch_dtype=torch.float16, enable_pag=True
                ... )
        
                >>> pipe.scheduler = DDIMScheduler.from_config(pipe.scheduler.config)
                >>> pipe.enable_model_cpu_offload()
        
                >>> # generate image
                >>> image = pipe(
                ...     "a handsome man with ray-ban sunglasses",
                ...     num_inference_steps=20,
                ...     generator=generator,
                ...     eta=1.0,
                ...     image=init_image,
                ...     mask_image=mask_image,
                ...     control_image=control_image,
                ...     pag_scale=0.3,
                ... ).images[0]
                ```
        '''
    ),
    "src/diffusers/pipelines/pag/pipeline_pag_controlnet_sd_xl.py": (
        '''
        Examples:
                ```py
                >>> # !pip install opencv-python transformers accelerate
                >>> from diffusers import AutoPipelineForText2Image, ControlNetModel, AutoencoderKL
                >>> from diffusers.utils import load_image
                >>> import numpy as np
                >>> import torch
        
                >>> import cv2
                >>> from PIL import Image
        
                >>> prompt = "aerial view, a futuristic research complex in a bright foggy jungle, hard lighting"
                >>> negative_prompt = "low quality, bad quality, sketches"
        
                >>> # download an image
                >>> image = load_image(
                ...     "https://hf.co/datasets/hf-internal-testing/diffusers-images/resolve/main/sd_controlnet/hf-logo.png"
                ... )
        
                >>> # initialize the models and pipeline
                >>> controlnet_conditioning_scale = 0.5  # recommended for good generalization
                >>> controlnet = ControlNetModel.from_pretrained(
                ...     "diffusers/controlnet-canny-sdxl-1.0", torch_dtype=torch.float16
                ... )
                >>> vae = AutoencoderKL.from_pretrained("madebyollin/sdxl-vae-fp16-fix", torch_dtype=torch.float16)
                >>> pipe = AutoPipelineForText2Image.from_pretrained(
                ...     "stabilityai/stable-diffusion-xl-base-1.0",
                ...     controlnet=controlnet,
                ...     vae=vae,
                ...     torch_dtype=torch.float16,
                ...     enable_pag=True,
                ... )
                >>> pipe.enable_model_cpu_offload()
        
                >>> # get canny image
                >>> image = np.array(image)
                >>> image = cv2.Canny(image, 100, 200)
                >>> image = image[:, :, None]
                >>> image = np.concatenate([image, image, image], axis=2)
                >>> canny_image = Image.fromarray(image)
        
                >>> # generate image
                >>> image = pipe(
                ...     prompt, controlnet_conditioning_scale=controlnet_conditioning_scale, image=canny_image, pag_scale=0.3
                ... ).images[0]
                ```
        '''
    ),
    "src/diffusers/pipelines/pag/pipeline_pag_controlnet_sd_xl_img2img.py": (
        '''
        Examples:
                ```py
                >>> # pip install accelerate transformers safetensors diffusers
        
                >>> import torch
                >>> import numpy as np
                >>> from PIL import Image
        
                >>> from transformers import DPTFeatureExtractor, DPTForDepthEstimation
                >>> from diffusers import ControlNetModel, StableDiffusionXLControlNetPAGImg2ImgPipeline, AutoencoderKL
                >>> from diffusers.utils import load_image
        
        
                >>> depth_estimator = DPTForDepthEstimation.from_pretrained("Intel/dpt-hybrid-midas").to("cuda")
                >>> feature_extractor = DPTFeatureExtractor.from_pretrained("Intel/dpt-hybrid-midas")
                >>> controlnet = ControlNetModel.from_pretrained(
                ...     "diffusers/controlnet-depth-sdxl-1.0-small",
                ...     variant="fp16",
                ...     use_safetensors="True",
                ...     torch_dtype=torch.float16,
                ... )
                >>> vae = AutoencoderKL.from_pretrained("madebyollin/sdxl-vae-fp16-fix", torch_dtype=torch.float16)
                >>> pipe = StableDiffusionXLControlNetPAGImg2ImgPipeline.from_pretrained(
                ...     "stabilityai/stable-diffusion-xl-base-1.0",
                ...     controlnet=controlnet,
                ...     vae=vae,
                ...     variant="fp16",
                ...     use_safetensors=True,
                ...     torch_dtype=torch.float16,
                ...     enable_pag=True,
                ... )
                >>> pipe.enable_model_cpu_offload()
        
        
                >>> def get_depth_map(image):
                ...     image = feature_extractor(images=image, return_tensors="pt").pixel_values.to("cuda")
                ...     with torch.no_grad(), torch.autocast("cuda"):
                ...         depth_map = depth_estimator(image).predicted_depth
        
                ...     depth_map = torch.nn.functional.interpolate(
                ...         depth_map.unsqueeze(1),
                ...         size=(1024, 1024),
                ...         mode="bicubic",
                ...         align_corners=False,
                ...     )
                ...     depth_min = torch.amin(depth_map, dim=[1, 2, 3], keepdim=True)
                ...     depth_max = torch.amax(depth_map, dim=[1, 2, 3], keepdim=True)
                ...     depth_map = (depth_map - depth_min) / (depth_max - depth_min)
                ...     image = torch.cat([depth_map] * 3, dim=1)
                ...     image = image.permute(0, 2, 3, 1).cpu().numpy()[0]
                ...     image = Image.fromarray((image * 255.0).clip(0, 255).astype(np.uint8))
                ...     return image
        
        
                >>> prompt = "A robot, 4k photo"
                >>> image = load_image(
                ...     "https://huggingface.co/datasets/hf-internal-testing/diffusers-images/resolve/main"
                ...     "/kandinsky/cat.png"
                ... ).resize((1024, 1024))
                >>> controlnet_conditioning_scale = 0.5  # recommended for good generalization
                >>> depth_image = get_depth_map(image)
        
                >>> images = pipe(
                ...     prompt,
                ...     image=image,
                ...     control_image=depth_image,
                ...     strength=0.99,
                ...     num_inference_steps=50,
                ...     controlnet_conditioning_scale=controlnet_conditioning_scale,
                ... ).images
                >>> images[0].save(f"robot_cat.png")
                ```
        '''
    ),
    "src/diffusers/pipelines/pag/pipeline_pag_hunyuandit.py": (
        '''
        Examples:
                ```python
                >>> import torch
                >>> from diffusers import AutoPipelineForText2Image
        
                >>> pipe = AutoPipelineForText2Image.from_pretrained(
                ...     "Tencent-Hunyuan/HunyuanDiT-v1.2-Diffusers",
                ...     torch_dtype=torch.float16,
                ...     enable_pag=True,
                ...     pag_applied_layers=[14],
                ... ).to("cuda")
        
                >>> # prompt = "an astronaut riding a horse"
                >>> prompt = "一个宇航员在骑马"
                >>> image = pipe(prompt, guidance_scale=4, pag_scale=3).images[0]
                ```
        '''
    ),
    "src/diffusers/pipelines/pag/pipeline_pag_kolors.py": (
        '''
        Examples:
                ```py
                >>> import torch
                >>> from diffusers import AutoPipelineForText2Image
        
                >>> pipe = AutoPipelineForText2Image.from_pretrained(
                ...     "Kwai-Kolors/Kolors-diffusers",
                ...     variant="fp16",
                ...     torch_dtype=torch.float16,
                ...     enable_pag=True,
                ...     pag_applied_layers=["down.block_2.attentions_1", "up.block_0.attentions_1"],
                ... )
                >>> pipe = pipe.to("cuda")
        
                >>> prompt = (
                ...     "A photo of a ladybug, macro, zoom, high quality, film, holding a wooden sign with the text 'KOLORS'"
                ... )
                >>> image = pipe(prompt, guidance_scale=5.5, pag_scale=1.5).images[0]
                ```
        '''
    ),
    "src/diffusers/pipelines/pag/pipeline_pag_pixart_sigma.py": (
        '''
        Examples:
                ```py
                >>> import torch
                >>> from diffusers import AutoPipelineForText2Image
        
                >>> pipe = AutoPipelineForText2Image.from_pretrained(
                ...     "PixArt-alpha/PixArt-Sigma-XL-2-1024-MS",
                ...     torch_dtype=torch.float16,
                ...     pag_applied_layers=["blocks.14"],
                ...     enable_pag=True,
                ... )
                >>> pipe = pipe.to("cuda")
        
                >>> prompt = "A small cactus with a happy face in the Sahara desert"
                >>> image = pipe(prompt, pag_scale=4.0, guidance_scale=1.0).images[0]
                ```
        '''
    ),
    "src/diffusers/pipelines/pag/pipeline_pag_sana.py": (
        '''
        Examples:
                ```py
                >>> import torch
                >>> from diffusers import SanaPAGPipeline
        
                >>> pipe = SanaPAGPipeline.from_pretrained(
                ...     "Efficient-Large-Model/Sana_1600M_1024px_BF16_diffusers",
                ...     pag_applied_layers=["transformer_blocks.8"],
                ...     torch_dtype=torch.float32,
                ... )
                >>> pipe.to("cuda")
                >>> pipe.text_encoder.to(torch.bfloat16)
                >>> pipe.transformer = pipe.transformer.to(torch.bfloat16)
        
                >>> image = pipe(prompt='a cyberpunk cat with a neon sign that says "Sana"')[0]
                >>> image[0].save("output.png")
                ```
        '''
    ),
    "src/diffusers/pipelines/pag/pipeline_pag_sd.py": (
        '''
        Examples:
                ```py
                >>> import torch
                >>> from diffusers import AutoPipelineForText2Image
        
                >>> pipe = AutoPipelineForText2Image.from_pretrained(
                ...     "runwayml/stable-diffusion-v1-5", torch_dtype=torch.float16, enable_pag=True
                ... )
                >>> pipe = pipe.to("cuda")
        
                >>> prompt = "a photo of an astronaut riding a horse on mars"
                >>> image = pipe(prompt, pag_scale=0.3).images[0]
                ```
        '''
    ),
    "src/diffusers/pipelines/pag/pipeline_pag_sd_3.py": (
        '''
        Examples:
                ```py
                >>> import torch
                >>> from diffusers import AutoPipelineForText2Image
        
                >>> pipe = AutoPipelineForText2Image.from_pretrained(
                ...     "stabilityai/stable-diffusion-3-medium-diffusers",
                ...     torch_dtype=torch.float16,
                ...     enable_pag=True,
                ...     pag_applied_layers=["blocks.13"],
                ... )
                >>> pipe.to("cuda")
                >>> prompt = "A cat holding a sign that says hello world"
                >>> image = pipe(prompt, guidance_scale=5.0, pag_scale=0.7).images[0]
                >>> image.save("sd3_pag.png")
                ```
        '''
    ),
    "src/diffusers/pipelines/pag/pipeline_pag_sd_3_img2img.py": (
        '''
        Examples:
                ```py
                >>> import torch
                >>> from diffusers import StableDiffusion3PAGImg2ImgPipeline
                >>> from diffusers.utils import load_image
        
                >>> pipe = StableDiffusion3PAGImg2ImgPipeline.from_pretrained(
                ...     "stabilityai/stable-diffusion-3-medium-diffusers",
                ...     torch_dtype=torch.float16,
                ...     pag_applied_layers=["blocks.13"],
                ... )
                >>> pipe.to("cuda")
                >>> prompt = "a photo of an astronaut riding a horse on mars"
                >>> url = "https://huggingface.co/datasets/patrickvonplaten/images/resolve/main/aa_xl/000000009.png"
                >>> init_image = load_image(url).convert("RGB")
                >>> image = pipe(prompt, image=init_image, guidance_scale=5.0, pag_scale=0.7).images[0]
                ```
        '''
    ),
    "src/diffusers/pipelines/pag/pipeline_pag_sd_animatediff.py": (
        '''
        Examples:
                ```py
                >>> import torch
                >>> from diffusers import AnimateDiffPAGPipeline, MotionAdapter, DDIMScheduler
                >>> from diffusers.utils import export_to_gif
        
                >>> model_id = "SG161222/Realistic_Vision_V5.1_noVAE"
                >>> motion_adapter_id = "guoyww/animatediff-motion-adapter-v1-5-2"
                >>> motion_adapter = MotionAdapter.from_pretrained(motion_adapter_id)
                >>> scheduler = DDIMScheduler.from_pretrained(
                ...     model_id, subfolder="scheduler", beta_schedule="linear", steps_offset=1, clip_sample=False
                ... )
                >>> pipe = AnimateDiffPAGPipeline.from_pretrained(
                ...     model_id,
                ...     motion_adapter=motion_adapter,
                ...     scheduler=scheduler,
                ...     pag_applied_layers=["mid"],
                ...     torch_dtype=torch.float16,
                ... ).to("cuda")
        
                >>> video = pipe(
                ...     prompt="car, futuristic cityscape with neon lights, street, no human",
                ...     negative_prompt="low quality, bad quality",
                ...     num_inference_steps=25,
                ...     guidance_scale=6.0,
                ...     pag_scale=3.0,
                ...     generator=torch.Generator().manual_seed(42),
                ... ).frames[0]
        
                >>> export_to_gif(video, "animatediff_pag.gif")
                ```
        '''
    ),
    "src/diffusers/pipelines/pag/pipeline_pag_sd_img2img.py": (
        '''
        Examples:
                ```py
                >>> import torch
                >>> from diffusers import AutoPipelineForImage2Image
                >>> from diffusers.utils import load_image
        
                >>> pipe = AutoPipelineForImage2Image.from_pretrained(
                ...     "runwayml/stable-diffusion-v1-5",
                ...     torch_dtype=torch.float16,
                ...     enable_pag=True,
                ... )
                >>> pipe = pipe.to("cuda")
                >>> url = "https://huggingface.co/datasets/patrickvonplaten/images/resolve/main/aa_xl/000000009.png"
        
                >>> init_image = load_image(url).convert("RGB")
                >>> prompt = "a photo of an astronaut riding a horse on mars"
                >>> image = pipe(prompt, image=init_image, pag_scale=0.3).images[0]
                ```
        '''
    ),
    "src/diffusers/pipelines/pag/pipeline_pag_sd_inpaint.py": (
        '''
        Examples:
                ```py
                >>> import torch
                >>> from diffusers import AutoPipelineForInpainting
        
                >>> pipe = AutoPipelineForInpainting.from_pretrained(
                ...     "runwayml/stable-diffusion-v1-5", torch_dtype=torch.float16, enable_pag=True
                ... )
                >>> pipe = pipe.to("cuda")
                >>> img_url = "https://raw.githubusercontent.com/CompVis/latent-diffusion/main/data/inpainting_examples/overture-creations-5sI6fQgYIuo.png"
                >>> mask_url = "https://raw.githubusercontent.com/CompVis/latent-diffusion/main/data/inpainting_examples/overture-creations-5sI6fQgYIuo_mask.png"
                >>> init_image = load_image(img_url).convert("RGB")
                >>> mask_image = load_image(mask_url).convert("RGB")
                >>> prompt = "A majestic tiger sitting on a bench"
                >>> image = pipe(
                ...     prompt=prompt,
                ...     image=init_image,
                ...     mask_image=mask_image,
                ...     strength=0.8,
                ...     num_inference_steps=50,
                ...     guidance_scale=guidance_scale,
                ...     generator=generator,
                ...     pag_scale=pag_scale,
                ... ).images[0]
                ```
        '''
    ),
    "src/diffusers/pipelines/pag/pipeline_pag_sd_xl.py": (
        '''
        Examples:
                ```py
                >>> import torch
                >>> from diffusers import AutoPipelineForText2Image
        
                >>> pipe = AutoPipelineForText2Image.from_pretrained(
                ...     "stabilityai/stable-diffusion-xl-base-1.0",
                ...     torch_dtype=torch.float16,
                ...     enable_pag=True,
                ... )
                >>> pipe = pipe.to("cuda")
        
                >>> prompt = "a photo of an astronaut riding a horse on mars"
                >>> image = pipe(prompt, pag_scale=0.3).images[0]
                ```
        '''
    ),
    "src/diffusers/pipelines/pag/pipeline_pag_sd_xl_img2img.py": (
        '''
        Examples:
                ```py
                >>> import torch
                >>> from diffusers import AutoPipelineForImage2Image
                >>> from diffusers.utils import load_image
        
                >>> pipe = AutoPipelineForImage2Image.from_pretrained(
                ...     "stabilityai/stable-diffusion-xl-refiner-1.0",
                ...     torch_dtype=torch.float16,
                ...     enable_pag=True,
                ... )
                >>> pipe = pipe.to("cuda")
                >>> url = "https://huggingface.co/datasets/patrickvonplaten/images/resolve/main/aa_xl/000000009.png"
        
                >>> init_image = load_image(url).convert("RGB")
                >>> prompt = "a photo of an astronaut riding a horse on mars"
                >>> image = pipe(prompt, image=init_image, pag_scale=0.3).images[0]
                ```
        '''
    ),
    "src/diffusers/pipelines/pag/pipeline_pag_sd_xl_inpaint.py": (
        '''
        Examples:
                ```py
                >>> import torch
                >>> from diffusers import AutoPipelineForInpainting
                >>> from diffusers.utils import load_image
        
                >>> pipe = AutoPipelineForInpainting.from_pretrained(
                ...     "stabilityai/stable-diffusion-xl-base-1.0",
                ...     torch_dtype=torch.float16,
                ...     variant="fp16",
                ...     enable_pag=True,
                ... )
                >>> pipe.to("cuda")
        
                >>> img_url = "https://raw.githubusercontent.com/CompVis/latent-diffusion/main/data/inpainting_examples/overture-creations-5sI6fQgYIuo.png"
                >>> mask_url = "https://raw.githubusercontent.com/CompVis/latent-diffusion/main/data/inpainting_examples/overture-creations-5sI6fQgYIuo_mask.png"
        
                >>> init_image = load_image(img_url).convert("RGB")
                >>> mask_image = load_image(mask_url).convert("RGB")
        
                >>> prompt = "A majestic tiger sitting on a bench"
                >>> image = pipe(
                ...     prompt=prompt,
                ...     image=init_image,
                ...     mask_image=mask_image,
                ...     num_inference_steps=50,
                ...     strength=0.80,
                ...     pag_scale=0.3,
                ... ).images[0]
                ```
        '''
    ),
    "src/diffusers/pipelines/pia/pipeline_pia.py": (
        '''
        Examples:
                ```py
                >>> import torch
                >>> from diffusers import EulerDiscreteScheduler, MotionAdapter, PIAPipeline
                >>> from diffusers.utils import export_to_gif, load_image
        
                >>> adapter = MotionAdapter.from_pretrained("openmmlab/PIA-condition-adapter")
                >>> pipe = PIAPipeline.from_pretrained(
                ...     "SG161222/Realistic_Vision_V6.0_B1_noVAE", motion_adapter=adapter, torch_dtype=torch.float16
                ... )
        
                >>> pipe.scheduler = EulerDiscreteScheduler.from_config(pipe.scheduler.config)
                >>> image = load_image(
                ...     "https://huggingface.co/datasets/hf-internal-testing/diffusers-images/resolve/main/pix2pix/cat_6.png?download=true"
                ... )
                >>> image = image.resize((512, 512))
                >>> prompt = "cat in a hat"
                >>> negative_prompt = "wrong white balance, dark, sketches, worst quality, low quality, deformed, distorted"
                >>> generator = torch.Generator("cpu").manual_seed(0)
                >>> output = pipe(image=image, prompt=prompt, negative_prompt=negative_prompt, generator=generator)
                >>> frames = output.frames[0]
                >>> export_to_gif(frames, "pia-animation.gif")
                ```
        '''
    ),
    "src/diffusers/pipelines/pixart_alpha/pipeline_pixart_alpha.py": (
        '''
        Examples:
                ```py
                >>> import torch
                >>> from diffusers import PixArtAlphaPipeline
        
                >>> # You can replace the checkpoint id with "PixArt-alpha/PixArt-XL-2-512x512" too.
                >>> pipe = PixArtAlphaPipeline.from_pretrained("PixArt-alpha/PixArt-XL-2-1024-MS", torch_dtype=torch.float16)
                >>> # Enable memory optimizations.
                >>> pipe.enable_model_cpu_offload()
        
                >>> prompt = "A small cactus with a happy face in the Sahara desert."
                >>> image = pipe(prompt).images[0]
                ```
        '''
    ),
    "src/diffusers/pipelines/pixart_alpha/pipeline_pixart_sigma.py": (
        '''
        Examples:
                ```py
                >>> import torch
                >>> from diffusers import PixArtSigmaPipeline
        
                >>> # You can replace the checkpoint id with "PixArt-alpha/PixArt-Sigma-XL-2-512-MS" too.
                >>> pipe = PixArtSigmaPipeline.from_pretrained(
                ...     "PixArt-alpha/PixArt-Sigma-XL-2-1024-MS", torch_dtype=torch.float16
                ... )
                >>> # Enable memory optimizations.
                >>> # pipe.enable_model_cpu_offload()
        
                >>> prompt = "A small cactus with a happy face in the Sahara desert."
                >>> image = pipe(prompt).images[0]
                ```
        '''
    ),
    "src/diffusers/pipelines/qwenimage/pipeline_qwenimage.py": (
        '''
        Examples:
                ```py
                >>> import torch
                >>> from diffusers import QwenImagePipeline
        
                >>> pipe = QwenImagePipeline.from_pretrained("Qwen/Qwen-Image", torch_dtype=torch.bfloat16)
                >>> pipe.to("cuda")
                >>> prompt = "A cat holding a sign that says hello world"
                >>> # Depending on the variant being used, the pipeline call will slightly vary.
                >>> # Refer to the pipeline documentation for more details.
                >>> image = pipe(prompt, num_inference_steps=50).images[0]
                >>> image.save("qwenimage.png")
                ```
        '''
    ),
    "src/diffusers/pipelines/qwenimage/pipeline_qwenimage_controlnet.py": (
        '''
        Examples:
                ```py
                >>> import torch
                >>> from diffusers.utils import load_image
                >>> from diffusers import QwenImageControlNetModel, QwenImageMultiControlNetModel, QwenImageControlNetPipeline
        
                >>> # QwenImageControlNetModel
                >>> controlnet = QwenImageControlNetModel.from_pretrained(
                ...     "InstantX/Qwen-Image-ControlNet-Union", torch_dtype=torch.bfloat16
                ... )
                >>> pipe = QwenImageControlNetPipeline.from_pretrained(
                ...     "Qwen/Qwen-Image", controlnet=controlnet, torch_dtype=torch.bfloat16
                ... )
                >>> pipe.to("cuda")
                >>> prompt = "Aesthetics art, traditional asian pagoda, elaborate golden accents, sky blue and white color palette, swirling cloud pattern, digital illustration, east asian architecture, ornamental rooftop, intricate detailing on building, cultural representation."
                >>> negative_prompt = " "
                >>> control_image = load_image(
                ...     "https://huggingface.co/InstantX/Qwen-Image-ControlNet-Union/resolve/main/conds/canny.png"
                ... )
                >>> # Depending on the variant being used, the pipeline call will slightly vary.
                >>> # Refer to the pipeline documentation for more details.
                >>> image = pipe(
                ...     prompt,
                ...     negative_prompt=negative_prompt,
                ...     control_image=control_image,
                ...     controlnet_conditioning_scale=1.0,
                ...     num_inference_steps=30,
                ...     true_cfg_scale=4.0,
                ... ).images[0]
                >>> image.save("qwenimage_cn_union.png")
        
                >>> # QwenImageMultiControlNetModel
                >>> controlnet = QwenImageControlNetModel.from_pretrained(
                ...     "InstantX/Qwen-Image-ControlNet-Union", torch_dtype=torch.bfloat16
                ... )
                >>> controlnet = QwenImageMultiControlNetModel([controlnet])
                >>> pipe = QwenImageControlNetPipeline.from_pretrained(
                ...     "Qwen/Qwen-Image", controlnet=controlnet, torch_dtype=torch.bfloat16
                ... )
                >>> pipe.to("cuda")
                >>> prompt = "Aesthetics art, traditional asian pagoda, elaborate golden accents, sky blue and white color palette, swirling cloud pattern, digital illustration, east asian architecture, ornamental rooftop, intricate detailing on building, cultural representation."
                >>> negative_prompt = " "
                >>> control_image = load_image(
                ...     "https://huggingface.co/InstantX/Qwen-Image-ControlNet-Union/resolve/main/conds/canny.png"
                ... )
                >>> # Depending on the variant being used, the pipeline call will slightly vary.
                >>> # Refer to the pipeline documentation for more details.
                >>> image = pipe(
                ...     prompt,
                ...     negative_prompt=negative_prompt,
                ...     control_image=[control_image, control_image],
                ...     controlnet_conditioning_scale=[0.5, 0.5],
                ...     num_inference_steps=30,
                ...     true_cfg_scale=4.0,
                ... ).images[0]
                >>> image.save("qwenimage_cn_union_multi.png")
                ```
        '''
    ),
    "src/diffusers/pipelines/qwenimage/pipeline_qwenimage_edit.py": (
        '''
        Examples:
                ```py
                >>> import torch
                >>> from PIL import Image
                >>> from diffusers import QwenImageEditPipeline
                >>> from diffusers.utils import load_image
        
                >>> pipe = QwenImageEditPipeline.from_pretrained("Qwen/Qwen-Image-Edit", torch_dtype=torch.bfloat16)
                >>> pipe.to("cuda")
                >>> image = load_image(
                ...     "https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/yarn-art-pikachu.png"
                ... ).convert("RGB")
                >>> prompt = (
                ...     "Make Pikachu hold a sign that says 'Qwen Edit is awesome', yarn art style, detailed, vibrant colors"
                ... )
                >>> # Depending on the variant being used, the pipeline call will slightly vary.
                >>> # Refer to the pipeline documentation for more details.
                >>> image = pipe(image, prompt, num_inference_steps=50).images[0]
                >>> image.save("qwenimage_edit.png")
                ```
        '''
    ),
    "src/diffusers/pipelines/qwenimage/pipeline_qwenimage_img2img.py": (
        '''
        Examples:
                ```py
                >>> import torch
                >>> from diffusers import QwenImageImg2ImgPipeline
                >>> from diffusers.utils import load_image
        
                >>> pipe = QwenImageImg2ImgPipeline.from_pretrained("Qwen/Qwen-Image", torch_dtype=torch.bfloat16)
                >>> pipe = pipe.to("cuda")
                >>> url = "https://raw.githubusercontent.com/CompVis/stable-diffusion/main/assets/stable-samples/img2img/sketch-mountains-input.jpg"
                >>> init_image = load_image(url).resize((1024, 1024))
                >>> prompt = "cat wizard, gandalf, lord of the rings, detailed, fantasy, cute, adorable, Pixar, Disney"
                >>> images = pipe(prompt=prompt, negative_prompt=" ", image=init_image, strength=0.95).images[0]
                >>> images.save("qwenimage_img2img.png")
                ```
        '''
    ),
    "src/diffusers/pipelines/qwenimage/pipeline_qwenimage_inpaint.py": (
        '''
        Examples:
                ```py
                >>> import torch
                >>> from diffusers import QwenImageInpaintPipeline
                >>> from diffusers.utils import load_image
        
                >>> pipe = QwenImageInpaintPipeline.from_pretrained("Qwen/Qwen-Image", torch_dtype=torch.bfloat16)
                >>> pipe.to("cuda")
                >>> prompt = "Face of a yellow cat, high resolution, sitting on a park bench"
                >>> img_url = "https://raw.githubusercontent.com/CompVis/latent-diffusion/main/data/inpainting_examples/overture-creations-5sI6fQgYIuo.png"
                >>> mask_url = "https://raw.githubusercontent.com/CompVis/latent-diffusion/main/data/inpainting_examples/overture-creations-5sI6fQgYIuo_mask.png"
                >>> source = load_image(img_url)
                >>> mask = load_image(mask_url)
                >>> image = pipe(prompt=prompt, negative_prompt=" ", image=source, mask_image=mask, strength=0.85).images[0]
                >>> image.save("qwenimage_inpainting.png")
                ```
        '''
    ),
    "src/diffusers/pipelines/sana/pipeline_sana.py": (
        '''
        Examples:
                ```py
                >>> import torch
                >>> from diffusers import SanaPipeline
        
                >>> pipe = SanaPipeline.from_pretrained(
                ...     "Efficient-Large-Model/Sana_1600M_1024px_BF16_diffusers", torch_dtype=torch.float32
                ... )
                >>> pipe.to("cuda")
                >>> pipe.text_encoder.to(torch.bfloat16)
                >>> pipe.transformer = pipe.transformer.to(torch.bfloat16)
        
                >>> image = pipe(prompt='a cyberpunk cat with a neon sign that says "Sana"')[0]
                >>> image[0].save("output.png")
                ```
        '''
    ),
    "src/diffusers/pipelines/sana/pipeline_sana_controlnet.py": (
        '''
        Examples:
                ```py
                >>> import torch
                >>> from diffusers import SanaControlNetPipeline
                >>> from diffusers.utils import load_image
        
                >>> pipe = SanaControlNetPipeline.from_pretrained(
                ...     "ishan24/Sana_600M_1024px_ControlNetPlus_diffusers",
                ...     variant="fp16",
                ...     torch_dtype={"default": torch.bfloat16, "controlnet": torch.float16, "transformer": torch.float16},
                ...     device_map="balanced",
                ... )
                >>> cond_image = load_image(
                ...     "https://huggingface.co/ishan24/Sana_600M_1024px_ControlNet_diffusers/resolve/main/hed_example.png"
                ... )
                >>> prompt = 'a cat with a neon sign that says "Sana"'
                >>> image = pipe(
                ...     prompt,
                ...     control_image=cond_image,
                ... ).images[0]
                >>> image.save("output.png")
                ```
        '''
    ),
    "src/diffusers/pipelines/sana/pipeline_sana_sprint.py": (
        '''
        Examples:
                ```py
                >>> import torch
                >>> from diffusers import SanaSprintPipeline
        
                >>> pipe = SanaSprintPipeline.from_pretrained(
                ...     "Efficient-Large-Model/Sana_Sprint_1.6B_1024px_diffusers", torch_dtype=torch.bfloat16
                ... )
                >>> pipe.to("cuda")
        
                >>> image = pipe(prompt="a tiny astronaut hatching from an egg on the moon")[0]
                >>> image[0].save("output.png")
                ```
        '''
    ),
    "src/diffusers/pipelines/sana/pipeline_sana_sprint_img2img.py": (
        '''
        Examples:
                ```py
                >>> import torch
                >>> from diffusers import SanaSprintImg2ImgPipeline
                >>> from diffusers.utils.loading_utils import load_image
        
                >>> pipe = SanaSprintImg2ImgPipeline.from_pretrained(
                ...     "Efficient-Large-Model/Sana_Sprint_1.6B_1024px_diffusers", torch_dtype=torch.bfloat16
                ... )
                >>> pipe.to("cuda")
        
                >>> image = load_image(
                ...     "https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/penguin.png"
                ... )
        
        
                >>> image = pipe(prompt="a cute pink bear", image=image, strength=0.5, height=832, width=480).images[0]
                >>> image[0].save("output.png")
                ```
        '''
    ),
    "src/diffusers/pipelines/shap_e/pipeline_shap_e.py": (
        '''
        Examples:
                ```py
                >>> import torch
                >>> from diffusers import DiffusionPipeline
                >>> from diffusers.utils import export_to_gif
        
                >>> device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        
                >>> repo = "openai/shap-e"
                >>> pipe = DiffusionPipeline.from_pretrained(repo, torch_dtype=torch.float16)
                >>> pipe = pipe.to(device)
        
                >>> guidance_scale = 15.0
                >>> prompt = "a shark"
        
                >>> images = pipe(
                ...     prompt,
                ...     guidance_scale=guidance_scale,
                ...     num_inference_steps=64,
                ...     frame_size=256,
                ... ).images
        
                >>> gif_path = export_to_gif(images[0], "shark_3d.gif")
                ```
        '''
    ),
    "src/diffusers/pipelines/shap_e/pipeline_shap_e_img2img.py": (
        '''
        Examples:
                ```py
                >>> from PIL import Image
                >>> import torch
                >>> from diffusers import DiffusionPipeline
                >>> from diffusers.utils import export_to_gif, load_image
        
                >>> device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        
                >>> repo = "openai/shap-e-img2img"
                >>> pipe = DiffusionPipeline.from_pretrained(repo, torch_dtype=torch.float16)
                >>> pipe = pipe.to(device)
        
                >>> guidance_scale = 3.0
                >>> image_url = "https://hf.co/datasets/diffusers/docs-images/resolve/main/shap-e/corgi.png"
                >>> image = load_image(image_url).convert("RGB")
        
                >>> images = pipe(
                ...     image,
                ...     guidance_scale=guidance_scale,
                ...     num_inference_steps=64,
                ...     frame_size=256,
                ... ).images
        
                >>> gif_path = export_to_gif(images[0], "corgi_3d.gif")
                ```
        '''
    ),
    "src/diffusers/pipelines/skyreels_v2/pipeline_skyreels_v2.py": (
        '''
        \
            Examples:
                ```py
                >>> import torch
                >>> from diffusers import (
                ...     SkyReelsV2Pipeline,
                ...     UniPCMultistepScheduler,
                ...     AutoencoderKLWan,
                ... )
                >>> from diffusers.utils import export_to_video
        
                >>> # Load the pipeline
                >>> # Available models:
                >>> # - Skywork/SkyReels-V2-T2V-14B-540P-Diffusers
                >>> # - Skywork/SkyReels-V2-T2V-14B-720P-Diffusers
                >>> vae = AutoencoderKLWan.from_pretrained(
                ...     "Skywork/SkyReels-V2-T2V-14B-720P-Diffusers",
                ...     subfolder="vae",
                ...     torch_dtype=torch.float32,
                ... )
                >>> pipe = SkyReelsV2Pipeline.from_pretrained(
                ...     "Skywork/SkyReels-V2-T2V-14B-720P-Diffusers",
                ...     vae=vae,
                ...     torch_dtype=torch.bfloat16,
                ... )
                >>> flow_shift = 8.0  # 8.0 for T2V, 5.0 for I2V
                >>> pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config, flow_shift=flow_shift)
                >>> pipe = pipe.to("cuda")
        
                >>> prompt = "A cat and a dog baking a cake together in a kitchen. The cat is carefully measuring flour, while the dog is stirring the batter with a wooden spoon. The kitchen is cozy, with sunlight streaming through the window."
        
                >>> output = pipe(
                ...     prompt=prompt,
                ...     num_inference_steps=50,
                ...     height=544,
                ...     width=960,
                ...     guidance_scale=6.0,  # 6.0 for T2V, 5.0 for I2V
                ...     num_frames=97,
                ... ).frames[0]
                >>> export_to_video(output, "video.mp4", fps=24, quality=8)
                ```
        '''
    ),
    "src/diffusers/pipelines/skyreels_v2/pipeline_skyreels_v2_diffusion_forcing.py": (
        '''
        \
            Examples:
                ```py
                >>> import torch
                >>> from diffusers import (
                ...     SkyReelsV2DiffusionForcingPipeline,
                ...     UniPCMultistepScheduler,
                ...     AutoencoderKLWan,
                ... )
                >>> from diffusers.utils import export_to_video
        
                >>> # Load the pipeline
                >>> # Available models:
                >>> # - Skywork/SkyReels-V2-DF-1.3B-540P-Diffusers
                >>> # - Skywork/SkyReels-V2-DF-14B-540P-Diffusers
                >>> # - Skywork/SkyReels-V2-DF-14B-720P-Diffusers
                >>> vae = AutoencoderKLWan.from_pretrained(
                ...     "Skywork/SkyReels-V2-DF-14B-720P-Diffusers",
                ...     subfolder="vae",
                ...     torch_dtype=torch.float32,
                ... )
                >>> pipe = SkyReelsV2DiffusionForcingPipeline.from_pretrained(
                ...     "Skywork/SkyReels-V2-DF-14B-720P-Diffusers",
                ...     vae=vae,
                ...     torch_dtype=torch.bfloat16,
                ... )
                >>> flow_shift = 8.0  # 8.0 for T2V, 5.0 for I2V
                >>> pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config, flow_shift=flow_shift)
                >>> pipe = pipe.to("cuda")
        
                >>> prompt = "A cat and a dog baking a cake together in a kitchen. The cat is carefully measuring flour, while the dog is stirring the batter with a wooden spoon. The kitchen is cozy, with sunlight streaming through the window."
        
                >>> output = pipe(
                ...     prompt=prompt,
                ...     num_inference_steps=30,
                ...     height=544,
                ...     width=960,
                ...     guidance_scale=6.0,  # 6.0 for T2V, 5.0 for I2V
                ...     num_frames=97,
                ...     ar_step=5,  # Controls asynchronous inference (0 for synchronous mode)
                ...     causal_block_size=5,  # Number of frames processed together in a causal block
                ...     overlap_history=None,  # Number of frames to overlap for smooth transitions in long videos
                ...     addnoise_condition=20,  # Improves consistency in long video generation
                ... ).frames[0]
                >>> export_to_video(output, "video.mp4", fps=24, quality=8)
                ```
        '''
    ),
    "src/diffusers/pipelines/skyreels_v2/pipeline_skyreels_v2_diffusion_forcing_i2v.py": (
        '''
        \
            Examples:
                ```py
                >>> import torch
                >>> from diffusers import (
                ...     SkyReelsV2DiffusionForcingImageToVideoPipeline,
                ...     UniPCMultistepScheduler,
                ...     AutoencoderKLWan,
                ... )
                >>> from diffusers.utils import export_to_video
                >>> from PIL import Image
        
                >>> # Load the pipeline
                >>> # Available models:
                >>> # - Skywork/SkyReels-V2-DF-1.3B-540P-Diffusers
                >>> # - Skywork/SkyReels-V2-DF-14B-540P-Diffusers
                >>> # - Skywork/SkyReels-V2-DF-14B-720P-Diffusers
                >>> vae = AutoencoderKLWan.from_pretrained(
                ...     "Skywork/SkyReels-V2-DF-14B-720P-Diffusers",
                ...     subfolder="vae",
                ...     torch_dtype=torch.float32,
                ... )
                >>> pipe = SkyReelsV2DiffusionForcingImageToVideoPipeline.from_pretrained(
                ...     "Skywork/SkyReels-V2-DF-14B-720P-Diffusers",
                ...     vae=vae,
                ...     torch_dtype=torch.bfloat16,
                ... )
                >>> flow_shift = 5.0  # 8.0 for T2V, 5.0 for I2V
                >>> pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config, flow_shift=flow_shift)
                >>> pipe = pipe.to("cuda")
        
                >>> prompt = "A cat and a dog baking a cake together in a kitchen. The cat is carefully measuring flour, while the dog is stirring the batter with a wooden spoon. The kitchen is cozy, with sunlight streaming through the window."
                >>> image = Image.open("path/to/image.png")
        
                >>> output = pipe(
                ...     image=image,
                ...     prompt=prompt,
                ...     num_inference_steps=50,
                ...     height=544,
                ...     width=960,
                ...     guidance_scale=5.0,  # 6.0 for T2V, 5.0 for I2V
                ...     num_frames=97,
                ...     ar_step=0,  # Controls asynchronous inference (0 for synchronous mode)
                ...     overlap_history=None,  # Number of frames to overlap for smooth transitions in long videos
                ...     addnoise_condition=20,  # Improves consistency in long video generation
                ... ).frames[0]
                >>> export_to_video(output, "video.mp4", fps=24, quality=8)
                ```
        '''
    ),
    "src/diffusers/pipelines/skyreels_v2/pipeline_skyreels_v2_diffusion_forcing_v2v.py": (
        '''
        \
            Examples:
                ```py
                >>> import torch
                >>> from diffusers import (
                ...     SkyReelsV2DiffusionForcingVideoToVideoPipeline,
                ...     UniPCMultistepScheduler,
                ...     AutoencoderKLWan,
                ... )
                >>> from diffusers.utils import export_to_video
        
                >>> # Load the pipeline
                >>> # Available models:
                >>> # - Skywork/SkyReels-V2-DF-1.3B-540P-Diffusers
                >>> # - Skywork/SkyReels-V2-DF-14B-540P-Diffusers
                >>> # - Skywork/SkyReels-V2-DF-14B-720P-Diffusers
                >>> vae = AutoencoderKLWan.from_pretrained(
                ...     "Skywork/SkyReels-V2-DF-14B-720P-Diffusers",
                ...     subfolder="vae",
                ...     torch_dtype=torch.float32,
                ... )
                >>> pipe = SkyReelsV2DiffusionForcingVideoToVideoPipeline.from_pretrained(
                ...     "Skywork/SkyReels-V2-DF-14B-720P-Diffusers",
                ...     vae=vae,
                ...     torch_dtype=torch.bfloat16,
                ... )
                >>> flow_shift = 8.0  # 8.0 for T2V, 5.0 for I2V
                >>> pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config, flow_shift=flow_shift)
                >>> pipe = pipe.to("cuda")
        
                >>> prompt = "A cat and a dog baking a cake together in a kitchen. The cat is carefully measuring flour, while the dog is stirring the batter with a wooden spoon. The kitchen is cozy, with sunlight streaming through the window."
        
                >>> output = pipe(
                ...     prompt=prompt,
                ...     num_inference_steps=50,
                ...     height=544,
                ...     width=960,
                ...     guidance_scale=6.0,  # 6.0 for T2V, 5.0 for I2V
                ...     num_frames=97,
                ...     ar_step=0,  # Controls asynchronous inference (0 for synchronous mode)
                ...     overlap_history=None,  # Number of frames to overlap for smooth transitions in long videos
                ...     addnoise_condition=20,  # Improves consistency in long video generation
                ... ).frames[0]
                >>> export_to_video(output, "video.mp4", fps=24, quality=8)
                ```
        '''
    ),
    "src/diffusers/pipelines/skyreels_v2/pipeline_skyreels_v2_i2v.py": (
        '''
        \
            Examples:
                ```py
                >>> import torch
                >>> from diffusers import (
                ...     SkyReelsV2ImageToVideoPipeline,
                ...     UniPCMultistepScheduler,
                ...     AutoencoderKLWan,
                ... )
                >>> from diffusers.utils import export_to_video
                >>> from PIL import Image
        
                >>> # Load the pipeline
                >>> # Available models:
                >>> # - Skywork/SkyReels-V2-I2V-1.3B-540P-Diffusers
                >>> # - Skywork/SkyReels-V2-I2V-14B-540P-Diffusers
                >>> # - Skywork/SkyReels-V2-I2V-14B-720P-Diffusers
                >>> vae = AutoencoderKLWan.from_pretrained(
                ...     "Skywork/SkyReels-V2-I2V-14B-720P-Diffusers",
                ...     subfolder="vae",
                ...     torch_dtype=torch.float32,
                ... )
                >>> pipe = SkyReelsV2ImageToVideoPipeline.from_pretrained(
                ...     "Skywork/SkyReels-V2-I2V-14B-720P-Diffusers",
                ...     vae=vae,
                ...     torch_dtype=torch.bfloat16,
                ... )
                >>> flow_shift = 5.0  # 8.0 for T2V, 5.0 for I2V
                >>> pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config, flow_shift=flow_shift)
                >>> pipe = pipe.to("cuda")
        
                >>> prompt = "A cat and a dog baking a cake together in a kitchen. The cat is carefully measuring flour, while the dog is stirring the batter with a wooden spoon. The kitchen is cozy, with sunlight streaming through the window."
                >>> image = Image.open("path/to/image.png")
        
                >>> output = pipe(
                ...     image=image,
                ...     prompt=prompt,
                ...     num_inference_steps=50,
                ...     height=544,
                ...     width=960,
                ...     guidance_scale=5.0,  # 6.0 for T2V, 5.0 for I2V
                ...     num_frames=97,
                ... ).frames[0]
                >>> export_to_video(output, "video.mp4", fps=24, quality=8)
                ```
        '''
    ),
    "src/diffusers/pipelines/stable_audio/pipeline_stable_audio.py": (
        '''
        Examples:
                ```py
                >>> import scipy
                >>> import torch
                >>> import soundfile as sf
                >>> from diffusers import StableAudioPipeline
        
                >>> repo_id = "stabilityai/stable-audio-open-1.0"
                >>> pipe = StableAudioPipeline.from_pretrained(repo_id, torch_dtype=torch.float16)
                >>> pipe = pipe.to("cuda")
        
                >>> # define the prompts
                >>> prompt = "The sound of a hammer hitting a wooden surface."
                >>> negative_prompt = "Low quality."
        
                >>> # set the seed for generator
                >>> generator = torch.Generator("cuda").manual_seed(0)
        
                >>> # run the generation
                >>> audio = pipe(
                ...     prompt,
                ...     negative_prompt=negative_prompt,
                ...     num_inference_steps=200,
                ...     audio_end_in_s=10.0,
                ...     num_waveforms_per_prompt=3,
                ...     generator=generator,
                ... ).audios
        
                >>> output = audio[0].T.float().cpu().numpy()
                >>> sf.write("hammer.wav", output, pipe.vae.sampling_rate)
                ```
        '''
    ),
    "src/diffusers/pipelines/stable_cascade/pipeline_stable_cascade.py": (
        '''
        Examples:
                ```py
                >>> import torch
                >>> from diffusers import StableCascadePriorPipeline, StableCascadeDecoderPipeline
        
                >>> prior_pipe = StableCascadePriorPipeline.from_pretrained(
                ...     "stabilityai/stable-cascade-prior", torch_dtype=torch.bfloat16
                ... ).to("cuda")
                >>> gen_pipe = StableCascadeDecoderPipeline.from_pretrain(
                ...     "stabilityai/stable-cascade", torch_dtype=torch.float16
                ... ).to("cuda")
        
                >>> prompt = "an image of a shiba inu, donning a spacesuit and helmet"
                >>> prior_output = pipe(prompt)
                >>> images = gen_pipe(prior_output.image_embeddings, prompt=prompt)
                ```
        '''
    ),
    "src/diffusers/pipelines/stable_cascade/pipeline_stable_cascade_combined.py": (
        '''
        Examples:
                ```py
                >>> import torch
                >>> from diffusers import StableCascadeCombinedPipeline
        
                >>> pipe = StableCascadeCombinedPipeline.from_pretrained(
                ...     "stabilityai/stable-cascade", variant="bf16", torch_dtype=torch.bfloat16
                ... )
                >>> pipe.enable_model_cpu_offload()
                >>> prompt = "an image of a shiba inu, donning a spacesuit and helmet"
                >>> images = pipe(prompt=prompt)
                ```
        '''
    ),
    "src/diffusers/pipelines/stable_cascade/pipeline_stable_cascade_prior.py": (
        '''
        Examples:
                ```py
                >>> import torch
                >>> from diffusers import StableCascadePriorPipeline
        
                >>> prior_pipe = StableCascadePriorPipeline.from_pretrained(
                ...     "stabilityai/stable-cascade-prior", torch_dtype=torch.bfloat16
                ... ).to("cuda")
        
                >>> prompt = "an image of a shiba inu, donning a spacesuit and helmet"
                >>> prior_output = pipe(prompt)
                ```
        '''
    ),
    "src/diffusers/pipelines/stable_diffusion/pipeline_flax_stable_diffusion.py": (
        '''
        Examples:
                ```py
                >>> import jax
                >>> import numpy as np
                >>> from flax.jax_utils import replicate
                >>> from flax.training.common_utils import shard
        
                >>> from diffusers import FlaxStableDiffusionPipeline
        
                >>> pipeline, params = FlaxStableDiffusionPipeline.from_pretrained(
                ...     "stable-diffusion-v1-5/stable-diffusion-v1-5", variant="bf16", dtype=jax.numpy.bfloat16
                ... )
        
                >>> prompt = "a photo of an astronaut riding a horse on mars"
        
                >>> prng_seed = jax.random.PRNGKey(0)
                >>> num_inference_steps = 50
        
                >>> num_samples = jax.device_count()
                >>> prompt = num_samples * [prompt]
                >>> prompt_ids = pipeline.prepare_inputs(prompt)
                # shard inputs and rng
        
                >>> params = replicate(params)
                >>> prng_seed = jax.random.split(prng_seed, jax.device_count())
                >>> prompt_ids = shard(prompt_ids)
        
                >>> images = pipeline(prompt_ids, params, prng_seed, num_inference_steps, jit=True).images
                >>> images = pipeline.numpy_to_pil(np.asarray(images.reshape((num_samples,) + images.shape[-3:])))
                ```
        '''
    ),
    "src/diffusers/pipelines/stable_diffusion/pipeline_flax_stable_diffusion_img2img.py": (
        '''
        Examples:
                ```py
                >>> import jax
                >>> import numpy as np
                >>> import jax.numpy as jnp
                >>> from flax.jax_utils import replicate
                >>> from flax.training.common_utils import shard
                >>> import requests
                >>> from io import BytesIO
                >>> from PIL import Image
                >>> from diffusers import FlaxStableDiffusionImg2ImgPipeline
        
        
                >>> def create_key(seed=0):
                ...     return jax.random.PRNGKey(seed)
        
        
                >>> rng = create_key(0)
        
                >>> url = "https://raw.githubusercontent.com/CompVis/stable-diffusion/main/assets/stable-samples/img2img/sketch-mountains-input.jpg"
                >>> response = requests.get(url)
                >>> init_img = Image.open(BytesIO(response.content)).convert("RGB")
                >>> init_img = init_img.resize((768, 512))
        
                >>> prompts = "A fantasy landscape, trending on artstation"
        
                >>> pipeline, params = FlaxStableDiffusionImg2ImgPipeline.from_pretrained(
                ...     "CompVis/stable-diffusion-v1-4",
                ...     revision="flax",
                ...     dtype=jnp.bfloat16,
                ... )
        
                >>> num_samples = jax.device_count()
                >>> rng = jax.random.split(rng, jax.device_count())
                >>> prompt_ids, processed_image = pipeline.prepare_inputs(
                ...     prompt=[prompts] * num_samples, image=[init_img] * num_samples
                ... )
                >>> p_params = replicate(params)
                >>> prompt_ids = shard(prompt_ids)
                >>> processed_image = shard(processed_image)
        
                >>> output = pipeline(
                ...     prompt_ids=prompt_ids,
                ...     image=processed_image,
                ...     params=p_params,
                ...     prng_seed=rng,
                ...     strength=0.75,
                ...     num_inference_steps=50,
                ...     jit=True,
                ...     height=512,
                ...     width=768,
                ... ).images
        
                >>> output_images = pipeline.numpy_to_pil(np.asarray(output.reshape((num_samples,) + output.shape[-3:])))
                ```
        '''
    ),
    "src/diffusers/pipelines/stable_diffusion/pipeline_flax_stable_diffusion_inpaint.py": (
        '''
        Examples:
                ```py
                >>> import jax
                >>> import numpy as np
                >>> from flax.jax_utils import replicate
                >>> from flax.training.common_utils import shard
                >>> import PIL
                >>> import requests
                >>> from io import BytesIO
                >>> from diffusers import FlaxStableDiffusionInpaintPipeline
        
        
                >>> def download_image(url):
                ...     response = requests.get(url)
                ...     return PIL.Image.open(BytesIO(response.content)).convert("RGB")
        
        
                >>> img_url = "https://raw.githubusercontent.com/CompVis/latent-diffusion/main/data/inpainting_examples/overture-creations-5sI6fQgYIuo.png"
                >>> mask_url = "https://raw.githubusercontent.com/CompVis/latent-diffusion/main/data/inpainting_examples/overture-creations-5sI6fQgYIuo_mask.png"
        
                >>> init_image = download_image(img_url).resize((512, 512))
                >>> mask_image = download_image(mask_url).resize((512, 512))
        
                >>> pipeline, params = FlaxStableDiffusionInpaintPipeline.from_pretrained(
                ...     "xvjiarui/stable-diffusion-2-inpainting"
                ... )
        
                >>> prompt = "Face of a yellow cat, high resolution, sitting on a park bench"
                >>> prng_seed = jax.random.PRNGKey(0)
                >>> num_inference_steps = 50
        
                >>> num_samples = jax.device_count()
                >>> prompt = num_samples * [prompt]
                >>> init_image = num_samples * [init_image]
                >>> mask_image = num_samples * [mask_image]
                >>> prompt_ids, processed_masked_images, processed_masks = pipeline.prepare_inputs(
                ...     prompt, init_image, mask_image
                ... )
                # shard inputs and rng
        
                >>> params = replicate(params)
                >>> prng_seed = jax.random.split(prng_seed, jax.device_count())
                >>> prompt_ids = shard(prompt_ids)
                >>> processed_masked_images = shard(processed_masked_images)
                >>> processed_masks = shard(processed_masks)
        
                >>> images = pipeline(
                ...     prompt_ids, processed_masks, processed_masked_images, params, prng_seed, num_inference_steps, jit=True
                ... ).images
                >>> images = pipeline.numpy_to_pil(np.asarray(images.reshape((num_samples,) + images.shape[-3:])))
                ```
        '''
    ),
    "src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion.py": (
        '''
        Examples:
                ```py
                >>> import torch
                >>> from diffusers import StableDiffusionPipeline
        
                >>> pipe = StableDiffusionPipeline.from_pretrained(
                ...     "stable-diffusion-v1-5/stable-diffusion-v1-5", torch_dtype=torch.float16
                ... )
                >>> pipe = pipe.to("cuda")
        
                >>> prompt = "a photo of an astronaut riding a horse on mars"
                >>> image = pipe(prompt).images[0]
                ```
        '''
    ),
    "src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_img2img.py": (
        '''
        Examples:
                ```py
                >>> import requests
                >>> import torch
                >>> from PIL import Image
                >>> from io import BytesIO
        
                >>> from diffusers import StableDiffusionImg2ImgPipeline
        
                >>> device = "cuda"
                >>> model_id_or_path = "stable-diffusion-v1-5/stable-diffusion-v1-5"
                >>> pipe = StableDiffusionImg2ImgPipeline.from_pretrained(model_id_or_path, torch_dtype=torch.float16)
                >>> pipe = pipe.to(device)
        
                >>> url = "https://raw.githubusercontent.com/CompVis/stable-diffusion/main/assets/stable-samples/img2img/sketch-mountains-input.jpg"
        
                >>> response = requests.get(url)
                >>> init_image = Image.open(BytesIO(response.content)).convert("RGB")
                >>> init_image = init_image.resize((768, 512))
        
                >>> prompt = "A fantasy landscape, trending on artstation"
        
                >>> images = pipe(prompt=prompt, image=init_image, strength=0.75, guidance_scale=7.5).images
                >>> images[0].save("fantasy_landscape.png")
                ```
        '''
    ),
    "src/diffusers/pipelines/stable_diffusion/pipeline_stable_unclip.py": (
        '''
        Examples:
                ```py
                >>> import torch
                >>> from diffusers import StableUnCLIPPipeline
        
                >>> pipe = StableUnCLIPPipeline.from_pretrained(
                ...     "fusing/stable-unclip-2-1-l", torch_dtype=torch.float16
                ... )  # TODO update model path
                >>> pipe = pipe.to("cuda")
        
                >>> prompt = "a photo of an astronaut riding a horse on mars"
                >>> images = pipe(prompt).images
                >>> images[0].save("astronaut_horse.png")
                ```
        '''
    ),
    "src/diffusers/pipelines/stable_diffusion/pipeline_stable_unclip_img2img.py": (
        '''
        Examples:
                ```py
                >>> import requests
                >>> import torch
                >>> from PIL import Image
                >>> from io import BytesIO
        
                >>> from diffusers import StableUnCLIPImg2ImgPipeline
        
                >>> pipe = StableUnCLIPImg2ImgPipeline.from_pretrained(
                ...     "stabilityai/stable-diffusion-2-1-unclip-small", torch_dtype=torch.float16
                ... )
                >>> pipe = pipe.to("cuda")
        
                >>> url = "https://raw.githubusercontent.com/CompVis/stable-diffusion/main/assets/stable-samples/img2img/sketch-mountains-input.jpg"
        
                >>> response = requests.get(url)
                >>> init_image = Image.open(BytesIO(response.content)).convert("RGB")
                >>> init_image = init_image.resize((768, 512))
        
                >>> prompt = "A fantasy landscape, trending on artstation"
        
                >>> images = pipe(init_image, prompt).images
                >>> images[0].save("fantasy_landscape.png")
                ```
        '''
    ),
    "src/diffusers/pipelines/stable_diffusion_3/pipeline_stable_diffusion_3.py": (
        '''
        Examples:
                ```py
                >>> import torch
                >>> from diffusers import StableDiffusion3Pipeline
        
                >>> pipe = StableDiffusion3Pipeline.from_pretrained(
                ...     "stabilityai/stable-diffusion-3-medium-diffusers", torch_dtype=torch.float16
                ... )
                >>> pipe.to("cuda")
                >>> prompt = "A cat holding a sign that says hello world"
                >>> image = pipe(prompt).images[0]
                >>> image.save("sd3.png")
                ```
        '''
    ),
    "src/diffusers/pipelines/stable_diffusion_3/pipeline_stable_diffusion_3_img2img.py": (
        '''
        Examples:
                ```py
                >>> import torch
        
                >>> from diffusers import AutoPipelineForImage2Image
                >>> from diffusers.utils import load_image
        
                >>> device = "cuda"
                >>> model_id_or_path = "stabilityai/stable-diffusion-3-medium-diffusers"
                >>> pipe = AutoPipelineForImage2Image.from_pretrained(model_id_or_path, torch_dtype=torch.float16)
                >>> pipe = pipe.to(device)
        
                >>> url = "https://raw.githubusercontent.com/CompVis/stable-diffusion/main/assets/stable-samples/img2img/sketch-mountains-input.jpg"
                >>> init_image = load_image(url).resize((1024, 1024))
        
                >>> prompt = "cat wizard, gandalf, lord of the rings, detailed, fantasy, cute, adorable, Pixar, Disney, 8k"
        
                >>> images = pipe(prompt=prompt, image=init_image, strength=0.95, guidance_scale=7.5).images[0]
                ```
        '''
    ),
    "src/diffusers/pipelines/stable_diffusion_3/pipeline_stable_diffusion_3_inpaint.py": (
        '''
        Examples:
                ```py
                >>> import torch
                >>> from diffusers import StableDiffusion3InpaintPipeline
                >>> from diffusers.utils import load_image
        
                >>> pipe = StableDiffusion3InpaintPipeline.from_pretrained(
                ...     "stabilityai/stable-diffusion-3-medium-diffusers", torch_dtype=torch.float16
                ... )
                >>> pipe.to("cuda")
                >>> prompt = "Face of a yellow cat, high resolution, sitting on a park bench"
                >>> img_url = "https://raw.githubusercontent.com/CompVis/latent-diffusion/main/data/inpainting_examples/overture-creations-5sI6fQgYIuo.png"
                >>> mask_url = "https://raw.githubusercontent.com/CompVis/latent-diffusion/main/data/inpainting_examples/overture-creations-5sI6fQgYIuo_mask.png"
                >>> source = load_image(img_url)
                >>> mask = load_image(mask_url)
                >>> image = pipe(prompt=prompt, image=source, mask_image=mask).images[0]
                >>> image.save("sd3_inpainting.png")
                ```
        '''
    ),
    "src/diffusers/pipelines/stable_diffusion_attend_and_excite/pipeline_stable_diffusion_attend_and_excite.py": (
        '''
        Examples:
                ```py
                >>> import torch
                >>> from diffusers import StableDiffusionAttendAndExcitePipeline
        
                >>> pipe = StableDiffusionAttendAndExcitePipeline.from_pretrained(
                ...     "CompVis/stable-diffusion-v1-4", torch_dtype=torch.float16
                ... ).to("cuda")
        
        
                >>> prompt = "a cat and a frog"
        
                >>> # use get_indices function to find out indices of the tokens you want to alter
                >>> pipe.get_indices(prompt)
                {0: '<|startoftext|>', 1: 'a</w>', 2: 'cat</w>', 3: 'and</w>', 4: 'a</w>', 5: 'frog</w>', 6: '<|endoftext|>'}
        
                >>> token_indices = [2, 5]
                >>> seed = 6141
                >>> generator = torch.Generator("cuda").manual_seed(seed)
        
                >>> images = pipe(
                ...     prompt=prompt,
                ...     token_indices=token_indices,
                ...     guidance_scale=7.5,
                ...     generator=generator,
                ...     num_inference_steps=50,
                ...     max_iter_to_alter=25,
                ... ).images
        
                >>> image = images[0]
                >>> image.save(f"../images/{prompt}_{seed}.png")
                ```
        '''
    ),
    "src/diffusers/pipelines/stable_diffusion_diffedit/pipeline_stable_diffusion_diffedit.py": (
        '''
        ```py
                >>> import PIL
                >>> import requests
                >>> import torch
                >>> from io import BytesIO
        
                >>> from diffusers import StableDiffusionDiffEditPipeline
        
        
                >>> def download_image(url):
                ...     response = requests.get(url)
                ...     return PIL.Image.open(BytesIO(response.content)).convert("RGB")
        
        
                >>> img_url = "https://github.com/Xiang-cd/DiffEdit-stable-diffusion/raw/main/assets/origin.png"
        
                >>> init_image = download_image(img_url).resize((768, 768))
        
                >>> pipeline = StableDiffusionDiffEditPipeline.from_pretrained(
                ...     "stabilityai/stable-diffusion-2-1", torch_dtype=torch.float16
                ... )
        
                >>> pipeline.scheduler = DDIMScheduler.from_config(pipeline.scheduler.config)
                >>> pipeline.inverse_scheduler = DDIMInverseScheduler.from_config(pipeline.scheduler.config)
                >>> pipeline.enable_model_cpu_offload()
        
                >>> mask_prompt = "A bowl of fruits"
                >>> prompt = "A bowl of pears"
        
                >>> mask_image = pipeline.generate_mask(image=init_image, source_prompt=prompt, target_prompt=mask_prompt)
                >>> image_latents = pipeline.invert(image=init_image, prompt=mask_prompt).latents
                >>> image = pipeline(prompt=prompt, mask_image=mask_image, image_latents=image_latents).images[0]
                ```
        '''
    ),
    "src/diffusers/pipelines/stable_diffusion_gligen/pipeline_stable_diffusion_gligen.py": (
        '''
        Examples:
                ```py
                >>> import torch
                >>> from diffusers import StableDiffusionGLIGENPipeline
                >>> from diffusers.utils import load_image
        
                >>> # Insert objects described by text at the region defined by bounding boxes
                >>> pipe = StableDiffusionGLIGENPipeline.from_pretrained(
                ...     "masterful/gligen-1-4-inpainting-text-box", variant="fp16", torch_dtype=torch.float16
                ... )
                >>> pipe = pipe.to("cuda")
        
                >>> input_image = load_image(
                ...     "https://hf.co/datasets/huggingface/documentation-images/resolve/main/diffusers/gligen/livingroom_modern.png"
                ... )
                >>> prompt = "a birthday cake"
                >>> boxes = [[0.2676, 0.6088, 0.4773, 0.7183]]
                >>> phrases = ["a birthday cake"]
        
                >>> images = pipe(
                ...     prompt=prompt,
                ...     gligen_phrases=phrases,
                ...     gligen_inpaint_image=input_image,
                ...     gligen_boxes=boxes,
                ...     gligen_scheduled_sampling_beta=1,
                ...     output_type="pil",
                ...     num_inference_steps=50,
                ... ).images
        
                >>> images[0].save("./gligen-1-4-inpainting-text-box.jpg")
        
                >>> # Generate an image described by the prompt and
                >>> # insert objects described by text at the region defined by bounding boxes
                >>> pipe = StableDiffusionGLIGENPipeline.from_pretrained(
                ...     "masterful/gligen-1-4-generation-text-box", variant="fp16", torch_dtype=torch.float16
                ... )
                >>> pipe = pipe.to("cuda")
        
                >>> prompt = "a waterfall and a modern high speed train running through the tunnel in a beautiful forest with fall foliage"
                >>> boxes = [[0.1387, 0.2051, 0.4277, 0.7090], [0.4980, 0.4355, 0.8516, 0.7266]]
                >>> phrases = ["a waterfall", "a modern high speed train running through the tunnel"]
        
                >>> images = pipe(
                ...     prompt=prompt,
                ...     gligen_phrases=phrases,
                ...     gligen_boxes=boxes,
                ...     gligen_scheduled_sampling_beta=1,
                ...     output_type="pil",
                ...     num_inference_steps=50,
                ... ).images
        
                >>> images[0].save("./gligen-1-4-generation-text-box.jpg")
                ```
        '''
    ),
    "src/diffusers/pipelines/stable_diffusion_gligen/pipeline_stable_diffusion_gligen_text_image.py": (
        '''
        Examples:
                ```py
                >>> import torch
                >>> from diffusers import StableDiffusionGLIGENTextImagePipeline
                >>> from diffusers.utils import load_image
        
                >>> # Insert objects described by image at the region defined by bounding boxes
                >>> pipe = StableDiffusionGLIGENTextImagePipeline.from_pretrained(
                ...     "anhnct/Gligen_Inpainting_Text_Image", torch_dtype=torch.float16
                ... )
                >>> pipe = pipe.to("cuda")
        
                >>> input_image = load_image(
                ...     "https://hf.co/datasets/huggingface/documentation-images/resolve/main/diffusers/gligen/livingroom_modern.png"
                ... )
                >>> prompt = "a backpack"
                >>> boxes = [[0.2676, 0.4088, 0.4773, 0.7183]]
                >>> phrases = None
                >>> gligen_image = load_image(
                ...     "https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/gligen/backpack.jpeg"
                ... )
        
                >>> images = pipe(
                ...     prompt=prompt,
                ...     gligen_phrases=phrases,
                ...     gligen_inpaint_image=input_image,
                ...     gligen_boxes=boxes,
                ...     gligen_images=[gligen_image],
                ...     gligen_scheduled_sampling_beta=1,
                ...     output_type="pil",
                ...     num_inference_steps=50,
                ... ).images
        
                >>> images[0].save("./gligen-inpainting-text-image-box.jpg")
        
                >>> # Generate an image described by the prompt and
                >>> # insert objects described by text and image at the region defined by bounding boxes
                >>> pipe = StableDiffusionGLIGENTextImagePipeline.from_pretrained(
                ...     "anhnct/Gligen_Text_Image", torch_dtype=torch.float16
                ... )
                >>> pipe = pipe.to("cuda")
        
                >>> prompt = "a flower sitting on the beach"
                >>> boxes = [[0.0, 0.09, 0.53, 0.76]]
                >>> phrases = ["flower"]
                >>> gligen_image = load_image(
                ...     "https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/gligen/pexels-pixabay-60597.jpg"
                ... )
        
                >>> images = pipe(
                ...     prompt=prompt,
                ...     gligen_phrases=phrases,
                ...     gligen_images=[gligen_image],
                ...     gligen_boxes=boxes,
                ...     gligen_scheduled_sampling_beta=1,
                ...     output_type="pil",
                ...     num_inference_steps=50,
                ... ).images
        
                >>> images[0].save("./gligen-generation-text-image-box.jpg")
        
                >>> # Generate an image described by the prompt and
                >>> # transfer style described by image at the region defined by bounding boxes
                >>> pipe = StableDiffusionGLIGENTextImagePipeline.from_pretrained(
                ...     "anhnct/Gligen_Text_Image", torch_dtype=torch.float16
                ... )
                >>> pipe = pipe.to("cuda")
        
                >>> prompt = "a dragon flying on the sky"
                >>> boxes = [[0.4, 0.2, 1.0, 0.8], [0.0, 1.0, 0.0, 1.0]]  # Set `[0.0, 1.0, 0.0, 1.0]` for the style
        
                >>> gligen_image = load_image(
                ...     "https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/landscape.png"
                ... )
        
                >>> gligen_placeholder = load_image(
                ...     "https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/landscape.png"
                ... )
        
                >>> images = pipe(
                ...     prompt=prompt,
                ...     gligen_phrases=[
                ...         "dragon",
                ...         "placeholder",
                ...     ],  # Can use any text instead of `placeholder` token, because we will use mask here
                ...     gligen_images=[
                ...         gligen_placeholder,
                ...         gligen_image,
                ...     ],  # Can use any image in gligen_placeholder, because we will use mask here
                ...     input_phrases_mask=[1, 0],  # Set 0 for the placeholder token
                ...     input_images_mask=[0, 1],  # Set 0 for the placeholder image
                ...     gligen_boxes=boxes,
                ...     gligen_scheduled_sampling_beta=1,
                ...     output_type="pil",
                ...     num_inference_steps=50,
                ... ).images
        
                >>> images[0].save("./gligen-generation-text-image-box-style-transfer.jpg")
                ```
        '''
    ),
    "src/diffusers/pipelines/stable_diffusion_k_diffusion/pipeline_stable_diffusion_xl_k_diffusion.py": (
        '''
        Examples:
                ```py
                >>> import torch
                >>> from diffusers import StableDiffusionXLKDiffusionPipeline
        
                >>> pipe = StableDiffusionXLKDiffusionPipeline.from_pretrained(
                ...     "stabilityai/stable-diffusion-xl-base-1.0", torch_dtype=torch.float16
                ... )
                >>> pipe = pipe.to("cuda")
                >>> pipe.set_scheduler("sample_dpmpp_2m_sde")
        
                >>> prompt = "a photo of an astronaut riding a horse on mars"
                >>> image = pipe(prompt).images[0]
                ```
        '''
    ),
    "src/diffusers/pipelines/stable_diffusion_ldm3d/pipeline_stable_diffusion_ldm3d.py": (
        '''
        Examples:
                ```python
                >>> from diffusers import StableDiffusionLDM3DPipeline
        
                >>> pipe = StableDiffusionLDM3DPipeline.from_pretrained("Intel/ldm3d-4c")
                >>> pipe = pipe.to("cuda")
        
                >>> prompt = "a photo of an astronaut riding a horse on mars"
                >>> output = pipe(prompt)
                >>> rgb_image, depth_image = output.rgb, output.depth
                >>> rgb_image[0].save("astronaut_ldm3d_rgb.jpg")
                >>> depth_image[0].save("astronaut_ldm3d_depth.png")
                ```
        '''
    ),
    "src/diffusers/pipelines/stable_diffusion_panorama/pipeline_stable_diffusion_panorama.py": (
        '''
        Examples:
                ```py
                >>> import torch
                >>> from diffusers import StableDiffusionPanoramaPipeline, DDIMScheduler
        
                >>> model_ckpt = "stabilityai/stable-diffusion-2-base"
                >>> scheduler = DDIMScheduler.from_pretrained(model_ckpt, subfolder="scheduler")
                >>> pipe = StableDiffusionPanoramaPipeline.from_pretrained(
                ...     model_ckpt, scheduler=scheduler, torch_dtype=torch.float16
                ... )
        
                >>> pipe = pipe.to("cuda")
        
                >>> prompt = "a photo of the dolomites"
                >>> image = pipe(prompt).images[0]
                ```
        '''
    ),
    "src/diffusers/pipelines/stable_diffusion_sag/pipeline_stable_diffusion_sag.py": (
        '''
        Examples:
                ```py
                >>> import torch
                >>> from diffusers import StableDiffusionSAGPipeline
        
                >>> pipe = StableDiffusionSAGPipeline.from_pretrained(
                ...     "stable-diffusion-v1-5/stable-diffusion-v1-5", torch_dtype=torch.float16
                ... )
                >>> pipe = pipe.to("cuda")
        
                >>> prompt = "a photo of an astronaut riding a horse on mars"
                >>> image = pipe(prompt, sag_scale=0.75).images[0]
                ```
        '''
    ),
    "src/diffusers/pipelines/stable_diffusion_xl/pipeline_stable_diffusion_xl.py": (
        '''
        Examples:
                ```py
                >>> import torch
                >>> from diffusers import StableDiffusionXLPipeline
        
                >>> pipe = StableDiffusionXLPipeline.from_pretrained(
                ...     "stabilityai/stable-diffusion-xl-base-1.0", torch_dtype=torch.float16
                ... )
                >>> pipe = pipe.to("cuda")
        
                >>> prompt = "a photo of an astronaut riding a horse on mars"
                >>> image = pipe(prompt).images[0]
                ```
        '''
    ),
    "src/diffusers/pipelines/stable_diffusion_xl/pipeline_stable_diffusion_xl_img2img.py": (
        '''
        Examples:
                ```py
                >>> import torch
                >>> from diffusers import StableDiffusionXLImg2ImgPipeline
                >>> from diffusers.utils import load_image
        
                >>> pipe = StableDiffusionXLImg2ImgPipeline.from_pretrained(
                ...     "stabilityai/stable-diffusion-xl-refiner-1.0", torch_dtype=torch.float16
                ... )
                >>> pipe = pipe.to("cuda")
                >>> url = "https://huggingface.co/datasets/patrickvonplaten/images/resolve/main/aa_xl/000000009.png"
        
                >>> init_image = load_image(url).convert("RGB")
                >>> prompt = "a photo of an astronaut riding a horse on mars"
                >>> image = pipe(prompt, image=init_image).images[0]
                ```
        '''
    ),
    "src/diffusers/pipelines/stable_diffusion_xl/pipeline_stable_diffusion_xl_inpaint.py": (
        '''
        Examples:
                ```py
                >>> import torch
                >>> from diffusers import StableDiffusionXLInpaintPipeline
                >>> from diffusers.utils import load_image
        
                >>> pipe = StableDiffusionXLInpaintPipeline.from_pretrained(
                ...     "stabilityai/stable-diffusion-xl-base-1.0",
                ...     torch_dtype=torch.float16,
                ...     variant="fp16",
                ...     use_safetensors=True,
                ... )
                >>> pipe.to("cuda")
        
                >>> img_url = "https://raw.githubusercontent.com/CompVis/latent-diffusion/main/data/inpainting_examples/overture-creations-5sI6fQgYIuo.png"
                >>> mask_url = "https://raw.githubusercontent.com/CompVis/latent-diffusion/main/data/inpainting_examples/overture-creations-5sI6fQgYIuo_mask.png"
        
                >>> init_image = load_image(img_url).convert("RGB")
                >>> mask_image = load_image(mask_url).convert("RGB")
        
                >>> prompt = "A majestic tiger sitting on a bench"
                >>> image = pipe(
                ...     prompt=prompt, image=init_image, mask_image=mask_image, num_inference_steps=50, strength=0.80
                ... ).images[0]
                ```
        '''
    ),
    "src/diffusers/pipelines/stable_diffusion_xl/pipeline_stable_diffusion_xl_instruct_pix2pix.py": (
        '''
        Examples:
                ```py
                >>> import torch
                >>> from diffusers import StableDiffusionXLInstructPix2PixPipeline
                >>> from diffusers.utils import load_image
        
                >>> resolution = 768
                >>> image = load_image(
                ...     "https://hf.co/datasets/diffusers/diffusers-images-docs/resolve/main/mountain.png"
                ... ).resize((resolution, resolution))
                >>> edit_instruction = "Turn sky into a cloudy one"
        
                >>> pipe = StableDiffusionXLInstructPix2PixPipeline.from_pretrained(
                ...     "diffusers/sdxl-instructpix2pix-768", torch_dtype=torch.float16
                ... ).to("cuda")
        
                >>> edited_image = pipe(
                ...     prompt=edit_instruction,
                ...     image=image,
                ...     height=resolution,
                ...     width=resolution,
                ...     guidance_scale=3.0,
                ...     image_guidance_scale=1.5,
                ...     num_inference_steps=30,
                ... ).images[0]
                >>> edited_image
                ```
        '''
    ),
    "src/diffusers/pipelines/stable_video_diffusion/pipeline_stable_video_diffusion.py": (
        '''
        Examples:
                ```py
                >>> from diffusers import StableVideoDiffusionPipeline
                >>> from diffusers.utils import load_image, export_to_video
        
                >>> pipe = StableVideoDiffusionPipeline.from_pretrained(
                ...     "stabilityai/stable-video-diffusion-img2vid-xt", torch_dtype=torch.float16, variant="fp16"
                ... )
                >>> pipe.to("cuda")
        
                >>> image = load_image(
                ...     "https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/svd-docstring-example.jpeg"
                ... )
                >>> image = image.resize((1024, 576))
        
                >>> frames = pipe(image, num_frames=25, decode_chunk_size=8).frames[0]
                >>> export_to_video(frames, "generated.mp4", fps=7)
                ```
        '''
    ),
    "src/diffusers/pipelines/t2i_adapter/pipeline_stable_diffusion_adapter.py": (
        '''
        Examples:
                ```py
                >>> from PIL import Image
                >>> from diffusers.utils import load_image
                >>> import torch
                >>> from diffusers import StableDiffusionAdapterPipeline, T2IAdapter
        
                >>> image = load_image(
                ...     "https://huggingface.co/datasets/diffusers/docs-images/resolve/main/t2i-adapter/color_ref.png"
                ... )
        
                >>> color_palette = image.resize((8, 8))
                >>> color_palette = color_palette.resize((512, 512), resample=Image.Resampling.NEAREST)
        
                >>> adapter = T2IAdapter.from_pretrained("TencentARC/t2iadapter_color_sd14v1", torch_dtype=torch.float16)
                >>> pipe = StableDiffusionAdapterPipeline.from_pretrained(
                ...     "CompVis/stable-diffusion-v1-4",
                ...     adapter=adapter,
                ...     torch_dtype=torch.float16,
                ... )
        
                >>> pipe.to("cuda")
        
                >>> out_image = pipe(
                ...     "At night, glowing cubes in front of the beach",
                ...     image=color_palette,
                ... ).images[0]
                ```
        '''
    ),
    "src/diffusers/pipelines/t2i_adapter/pipeline_stable_diffusion_xl_adapter.py": (
        '''
        Examples:
                ```py
                >>> import torch
                >>> from diffusers import T2IAdapter, StableDiffusionXLAdapterPipeline, DDPMScheduler
                >>> from diffusers.utils import load_image
        
                >>> sketch_image = load_image("https://huggingface.co/Adapter/t2iadapter/resolve/main/sketch.png").convert("L")
        
                >>> model_id = "stabilityai/stable-diffusion-xl-base-1.0"
        
                >>> adapter = T2IAdapter.from_pretrained(
                ...     "Adapter/t2iadapter",
                ...     subfolder="sketch_sdxl_1.0",
                ...     torch_dtype=torch.float16,
                ...     adapter_type="full_adapter_xl",
                ... )
                >>> scheduler = DDPMScheduler.from_pretrained(model_id, subfolder="scheduler")
        
                >>> pipe = StableDiffusionXLAdapterPipeline.from_pretrained(
                ...     model_id, adapter=adapter, torch_dtype=torch.float16, variant="fp16", scheduler=scheduler
                ... ).to("cuda")
        
                >>> generator = torch.manual_seed(42)
                >>> sketch_image_out = pipe(
                ...     prompt="a photo of a dog in real world, high quality",
                ...     negative_prompt="extra digit, fewer digits, cropped, worst quality, low quality",
                ...     image=sketch_image,
                ...     generator=generator,
                ...     guidance_scale=7.5,
                ... ).images[0]
                ```
        '''
    ),
    "src/diffusers/pipelines/text_to_video_synthesis/pipeline_text_to_video_synth.py": (
        '''
        Examples:
                ```py
                >>> import torch
                >>> from diffusers import TextToVideoSDPipeline
                >>> from diffusers.utils import export_to_video
        
                >>> pipe = TextToVideoSDPipeline.from_pretrained(
                ...     "damo-vilab/text-to-video-ms-1.7b", torch_dtype=torch.float16, variant="fp16"
                ... )
                >>> pipe.enable_model_cpu_offload()
        
                >>> prompt = "Spiderman is surfing"
                >>> video_frames = pipe(prompt).frames[0]
                >>> video_path = export_to_video(video_frames)
                >>> video_path
                ```
        '''
    ),
    "src/diffusers/pipelines/text_to_video_synthesis/pipeline_text_to_video_synth_img2img.py": (
        '''
        Examples:
                ```py
                >>> import torch
                >>> from diffusers import DiffusionPipeline, DPMSolverMultistepScheduler
                >>> from diffusers.utils import export_to_video
        
                >>> pipe = DiffusionPipeline.from_pretrained("cerspense/zeroscope_v2_576w", torch_dtype=torch.float16)
                >>> pipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)
                >>> pipe.to("cuda")
        
                >>> prompt = "spiderman running in the desert"
                >>> video_frames = pipe(prompt, num_inference_steps=40, height=320, width=576, num_frames=24).frames[0]
                >>> # safe low-res video
                >>> video_path = export_to_video(video_frames, output_video_path="./video_576_spiderman.mp4")
        
                >>> # let's offload the text-to-image model
                >>> pipe.to("cpu")
        
                >>> # and load the image-to-image model
                >>> pipe = DiffusionPipeline.from_pretrained(
                ...     "cerspense/zeroscope_v2_XL", torch_dtype=torch.float16, revision="refs/pr/15"
                ... )
                >>> pipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)
                >>> pipe.enable_model_cpu_offload()
        
                >>> # The VAE consumes A LOT of memory, let's make sure we run it in sliced mode
                >>> pipe.vae.enable_slicing()
        
                >>> # now let's upscale it
                >>> video = [Image.fromarray(frame).resize((1024, 576)) for frame in video_frames]
        
                >>> # and denoise it
                >>> video_frames = pipe(prompt, video=video, strength=0.6).frames[0]
                >>> video_path = export_to_video(video_frames, output_video_path="./video_1024_spiderman.mp4")
                >>> video_path
                ```
        '''
    ),
    "src/diffusers/pipelines/visualcloze/pipeline_visualcloze_combined.py": (
        '''
        Examples:
                ```python
                >>> import torch
                >>> from diffusers import VisualClozePipeline
                >>> from diffusers.utils import load_image
        
                >>> image_paths = [
                ...     # in-context examples
                ...     [
                ...         load_image(
                ...             "https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/visualcloze/visualcloze_mask2image_incontext-example-1_mask.jpg"
                ...         ),
                ...         load_image(
                ...             "https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/visualcloze/visualcloze_mask2image_incontext-example-1_image.jpg"
                ...         ),
                ...     ],
                ...     # query with the target image
                ...     [
                ...         load_image(
                ...             "https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/visualcloze/visualcloze_mask2image_query_mask.jpg"
                ...         ),
                ...         None,  # No image needed for the target image
                ...     ],
                ... ]
                >>> task_prompt = "In each row, a logical task is demonstrated to achieve [IMAGE2] an aesthetically pleasing photograph based on [IMAGE1] sam 2-generated masks with rich color coding."
                >>> content_prompt = "Majestic photo of a golden eagle perched on a rocky outcrop in a mountainous landscape. The eagle is positioned in the right foreground, facing left, with its sharp beak and keen eyes prominently visible. Its plumage is a mix of dark brown and golden hues, with intricate feather details. The background features a soft-focus view of snow-capped mountains under a cloudy sky, creating a serene and grandiose atmosphere. The foreground includes rugged rocks and patches of green moss. Photorealistic, medium depth of field, soft natural lighting, cool color palette, high contrast, sharp focus on the eagle, blurred background, tranquil, majestic, wildlife photography."
                >>> pipe = VisualClozePipeline.from_pretrained(
                ...     "VisualCloze/VisualClozePipeline-384", resolution=384, torch_dtype=torch.bfloat16
                ... )
                >>> pipe.to("cuda")
        
                >>> image = pipe(
                ...     task_prompt=task_prompt,
                ...     content_prompt=content_prompt,
                ...     image=image_paths,
                ...     upsampling_width=1344,
                ...     upsampling_height=768,
                ...     upsampling_strength=0.4,
                ...     guidance_scale=30,
                ...     num_inference_steps=30,
                ...     max_sequence_length=512,
                ...     generator=torch.Generator("cpu").manual_seed(0),
                ... ).images[0][0]
                >>> image.save("visualcloze.png")
                ```
        '''
    ),
    "src/diffusers/pipelines/visualcloze/pipeline_visualcloze_generation.py": (
        '''
        Examples:
                ```python
                >>> import torch
                >>> from diffusers import VisualClozeGenerationPipeline, FluxFillPipeline as VisualClozeUpsamplingPipeline
                >>> from diffusers.utils import load_image
                >>> from PIL import Image
        
                >>> image_paths = [
                ...     # in-context examples
                ...     [
                ...         load_image(
                ...             "https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/visualcloze/visualcloze_mask2image_incontext-example-1_mask.jpg"
                ...         ),
                ...         load_image(
                ...             "https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/visualcloze/visualcloze_mask2image_incontext-example-1_image.jpg"
                ...         ),
                ...     ],
                ...     # query with the target image
                ...     [
                ...         load_image(
                ...             "https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/visualcloze/visualcloze_mask2image_query_mask.jpg"
                ...         ),
                ...         None,  # No image needed for the target image
                ...     ],
                ... ]
                >>> task_prompt = "In each row, a logical task is demonstrated to achieve [IMAGE2] an aesthetically pleasing photograph based on [IMAGE1] sam 2-generated masks with rich color coding."
                >>> content_prompt = "Majestic photo of a golden eagle perched on a rocky outcrop in a mountainous landscape. The eagle is positioned in the right foreground, facing left, with its sharp beak and keen eyes prominently visible. Its plumage is a mix of dark brown and golden hues, with intricate feather details. The background features a soft-focus view of snow-capped mountains under a cloudy sky, creating a serene and grandiose atmosphere. The foreground includes rugged rocks and patches of green moss. Photorealistic, medium depth of field, soft natural lighting, cool color palette, high contrast, sharp focus on the eagle, blurred background, tranquil, majestic, wildlife photography."
                >>> pipe = VisualClozeGenerationPipeline.from_pretrained(
                ...     "VisualCloze/VisualClozePipeline-384", resolution=384, torch_dtype=torch.bfloat16
                ... )
                >>> pipe.to("cuda")
        
                >>> image = pipe(
                ...     task_prompt=task_prompt,
                ...     content_prompt=content_prompt,
                ...     image=image_paths,
                ...     guidance_scale=30,
                ...     num_inference_steps=30,
                ...     max_sequence_length=512,
                ...     generator=torch.Generator("cpu").manual_seed(0),
                ... ).images[0][0]
        
                >>> # optional, upsampling the generated image
                >>> pipe_upsample = VisualClozeUpsamplingPipeline.from_pipe(pipe)
                >>> pipe_upsample.to("cuda")
        
                >>> mask_image = Image.new("RGB", image.size, (255, 255, 255))
        
                >>> image = pipe_upsample(
                ...     image=image,
                ...     mask_image=mask_image,
                ...     prompt=content_prompt,
                ...     width=1344,
                ...     height=768,
                ...     strength=0.4,
                ...     guidance_scale=30,
                ...     num_inference_steps=30,
                ...     max_sequence_length=512,
                ...     generator=torch.Generator("cpu").manual_seed(0),
                ... ).images[0]
        
                >>> image.save("visualcloze.png")
                ```
        '''
    ),
    "src/diffusers/pipelines/wan/pipeline_wan.py": (
        '''
        Examples:
                ```python
                >>> import torch
                >>> from diffusers.utils import export_to_video
                >>> from diffusers import AutoencoderKLWan, WanPipeline
                >>> from diffusers.schedulers.scheduling_unipc_multistep import UniPCMultistepScheduler
        
                >>> # Available models: Wan-AI/Wan2.1-T2V-14B-Diffusers, Wan-AI/Wan2.1-T2V-1.3B-Diffusers
                >>> model_id = "Wan-AI/Wan2.1-T2V-14B-Diffusers"
                >>> vae = AutoencoderKLWan.from_pretrained(model_id, subfolder="vae", torch_dtype=torch.float32)
                >>> pipe = WanPipeline.from_pretrained(model_id, vae=vae, torch_dtype=torch.bfloat16)
                >>> flow_shift = 5.0  # 5.0 for 720P, 3.0 for 480P
                >>> pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config, flow_shift=flow_shift)
                >>> pipe.to("cuda")
        
                >>> prompt = "A cat and a dog baking a cake together in a kitchen. The cat is carefully measuring flour, while the dog is stirring the batter with a wooden spoon. The kitchen is cozy, with sunlight streaming through the window."
                >>> negative_prompt = "Bright tones, overexposed, static, blurred details, subtitles, style, works, paintings, images, static, overall gray, worst quality, low quality, JPEG compression residue, ugly, incomplete, extra fingers, poorly drawn hands, poorly drawn faces, deformed, disfigured, misshapen limbs, fused fingers, still picture, messy background, three legs, many people in the background, walking backwards"
        
                >>> output = pipe(
                ...     prompt=prompt,
                ...     negative_prompt=negative_prompt,
                ...     height=720,
                ...     width=1280,
                ...     num_frames=81,
                ...     guidance_scale=5.0,
                ... ).frames[0]
                >>> export_to_video(output, "output.mp4", fps=16)
                ```
        '''
    ),
    "src/diffusers/pipelines/wan/pipeline_wan_i2v.py": (
        '''
        Examples:
                ```python
                >>> import torch
                >>> import numpy as np
                >>> from diffusers import AutoencoderKLWan, WanImageToVideoPipeline
                >>> from diffusers.utils import export_to_video, load_image
                >>> from transformers import CLIPVisionModel
        
                >>> # Available models: Wan-AI/Wan2.1-I2V-14B-480P-Diffusers, Wan-AI/Wan2.1-I2V-14B-720P-Diffusers
                >>> model_id = "Wan-AI/Wan2.1-I2V-14B-480P-Diffusers"
                >>> image_encoder = CLIPVisionModel.from_pretrained(
                ...     model_id, subfolder="image_encoder", torch_dtype=torch.float32
                ... )
                >>> vae = AutoencoderKLWan.from_pretrained(model_id, subfolder="vae", torch_dtype=torch.float32)
                >>> pipe = WanImageToVideoPipeline.from_pretrained(
                ...     model_id, vae=vae, image_encoder=image_encoder, torch_dtype=torch.bfloat16
                ... )
                >>> pipe.to("cuda")
        
                >>> image = load_image(
                ...     "https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/astronaut.jpg"
                ... )
                >>> max_area = 480 * 832
                >>> aspect_ratio = image.height / image.width
                >>> mod_value = pipe.vae_scale_factor_spatial * pipe.transformer.config.patch_size[1]
                >>> height = round(np.sqrt(max_area * aspect_ratio)) // mod_value * mod_value
                >>> width = round(np.sqrt(max_area / aspect_ratio)) // mod_value * mod_value
                >>> image = image.resize((width, height))
                >>> prompt = (
                ...     "An astronaut hatching from an egg, on the surface of the moon, the darkness and depth of space realised in "
                ...     "the background. High quality, ultrarealistic detail and breath-taking movie-like camera shot."
                ... )
                >>> negative_prompt = "Bright tones, overexposed, static, blurred details, subtitles, style, works, paintings, images, static, overall gray, worst quality, low quality, JPEG compression residue, ugly, incomplete, extra fingers, poorly drawn hands, poorly drawn faces, deformed, disfigured, misshapen limbs, fused fingers, still picture, messy background, three legs, many people in the background, walking backwards"
        
                >>> output = pipe(
                ...     image=image,
                ...     prompt=prompt,
                ...     negative_prompt=negative_prompt,
                ...     height=height,
                ...     width=width,
                ...     num_frames=81,
                ...     guidance_scale=5.0,
                ... ).frames[0]
                >>> export_to_video(output, "output.mp4", fps=16)
                ```
        '''
    ),
    "src/diffusers/pipelines/wan/pipeline_wan_vace.py": (
        '''
        Examples:
                ```python
                >>> import torch
                >>> import PIL.Image
                >>> from diffusers import AutoencoderKLWan, WanVACEPipeline
                >>> from diffusers.schedulers.scheduling_unipc_multistep import UniPCMultistepScheduler
                >>> from diffusers.utils import export_to_video, load_image
                def prepare_video_and_mask(first_img: PIL.Image.Image, last_img: PIL.Image.Image, height: int, width: int, num_frames: int):
                    first_img = first_img.resize((width, height))
                    last_img = last_img.resize((width, height))
                    frames = []
                    frames.append(first_img)
                    # Ideally, this should be 127.5 to match original code, but they perform computation on numpy arrays
                    # whereas we are passing PIL images. If you choose to pass numpy arrays, you can set it to 127.5 to
                    # match the original code.
                    frames.extend([PIL.Image.new("RGB", (width, height), (128, 128, 128))] * (num_frames - 2))
                    frames.append(last_img)
                    mask_black = PIL.Image.new("L", (width, height), 0)
                    mask_white = PIL.Image.new("L", (width, height), 255)
                    mask = [mask_black, *[mask_white] * (num_frames - 2), mask_black]
                    return frames, mask
        
                >>> # Available checkpoints: Wan-AI/Wan2.1-VACE-1.3B-diffusers, Wan-AI/Wan2.1-VACE-14B-diffusers
                >>> model_id = "Wan-AI/Wan2.1-VACE-1.3B-diffusers"
                >>> vae = AutoencoderKLWan.from_pretrained(model_id, subfolder="vae", torch_dtype=torch.float32)
                >>> pipe = WanVACEPipeline.from_pretrained(model_id, vae=vae, torch_dtype=torch.bfloat16)
                >>> flow_shift = 3.0  # 5.0 for 720P, 3.0 for 480P
                >>> pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config, flow_shift=flow_shift)
                >>> pipe.to("cuda")
        
                >>> prompt = "CG animation style, a small blue bird takes off from the ground, flapping its wings. The bird's feathers are delicate, with a unique pattern on its chest. The background shows a blue sky with white clouds under bright sunshine. The camera follows the bird upward, capturing its flight and the vastness of the sky from a close-up, low-angle perspective."
                >>> negative_prompt = "Bright tones, overexposed, static, blurred details, subtitles, style, works, paintings, images, static, overall gray, worst quality, low quality, JPEG compression residue, ugly, incomplete, extra fingers, poorly drawn hands, poorly drawn faces, deformed, disfigured, misshapen limbs, fused fingers, still picture, messy background, three legs, many people in the background, walking backwards"
                >>> first_frame = load_image(
                ...     "https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/flf2v_input_first_frame.png"
                ... )
                >>> last_frame = load_image(
                ...     "https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/flf2v_input_last_frame.png>>> "
                ... )
        
                >>> height = 512
                >>> width = 512
                >>> num_frames = 81
                >>> video, mask = prepare_video_and_mask(first_frame, last_frame, height, width, num_frames)
        
                >>> output = pipe(
                ...     video=video,
                ...     mask=mask,
                ...     prompt=prompt,
                ...     negative_prompt=negative_prompt,
                ...     height=height,
                ...     width=width,
                ...     num_frames=num_frames,
                ...     num_inference_steps=30,
                ...     guidance_scale=5.0,
                ...     generator=torch.Generator().manual_seed(42),
                ... ).frames[0]
                >>> export_to_video(output, "output.mp4", fps=16)
                ```
        '''
    ),
    "src/diffusers/pipelines/wan/pipeline_wan_video2video.py": (
        '''
        Examples:
                ```python
                >>> import torch
                >>> from diffusers.utils import export_to_video
                >>> from diffusers import AutoencoderKLWan, WanVideoToVideoPipeline
                >>> from diffusers.schedulers.scheduling_unipc_multistep import UniPCMultistepScheduler
        
                >>> # Available models: Wan-AI/Wan2.1-T2V-14B-Diffusers, Wan-AI/Wan2.1-T2V-1.3B-Diffusers
                >>> model_id = "Wan-AI/Wan2.1-T2V-1.3B-Diffusers"
                >>> vae = AutoencoderKLWan.from_pretrained(model_id, subfolder="vae", torch_dtype=torch.float32)
                >>> pipe = WanVideoToVideoPipeline.from_pretrained(model_id, vae=vae, torch_dtype=torch.bfloat16)
                >>> flow_shift = 3.0  # 5.0 for 720P, 3.0 for 480P
                >>> pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config, flow_shift=flow_shift)
                >>> pipe.to("cuda")
        
                >>> prompt = "A robot standing on a mountain top. The sun is setting in the background"
                >>> negative_prompt = "Bright tones, overexposed, static, blurred details, subtitles, style, works, paintings, images, static, overall gray, worst quality, low quality, JPEG compression residue, ugly, incomplete, extra fingers, poorly drawn hands, poorly drawn faces, deformed, disfigured, misshapen limbs, fused fingers, still picture, messy background, three legs, many people in the background, walking backwards"
                >>> video = load_video(
                ...     "https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/hiker.mp4"
                ... )
                >>> output = pipe(
                ...     video=video,
                ...     prompt=prompt,
                ...     negative_prompt=negative_prompt,
                ...     height=480,
                ...     width=720,
                ...     guidance_scale=5.0,
                ...     strength=0.7,
                ... ).frames[0]
                >>> export_to_video(output, "output.mp4", fps=16)
                ```
        '''
    ),
    "src/diffusers/pipelines/wuerstchen/pipeline_wuerstchen.py": (
        '''
        Examples:
                ```py
                >>> import torch
                >>> from diffusers import WuerstchenPriorPipeline, WuerstchenDecoderPipeline
        
                >>> prior_pipe = WuerstchenPriorPipeline.from_pretrained(
                ...     "warp-ai/wuerstchen-prior", torch_dtype=torch.float16
                ... ).to("cuda")
                >>> gen_pipe = WuerstchenDecoderPipeline.from_pretrain("warp-ai/wuerstchen", torch_dtype=torch.float16).to(
                ...     "cuda"
                ... )
        
                >>> prompt = "an image of a shiba inu, donning a spacesuit and helmet"
                >>> prior_output = pipe(prompt)
                >>> images = gen_pipe(prior_output.image_embeddings, prompt=prompt)
                ```
        '''
    ),
    "src/diffusers/pipelines/wuerstchen/pipeline_wuerstchen_combined.py": (
        '''
        Examples:
                ```py
                >>> from diffusions import WuerstchenCombinedPipeline
        
                >>> pipe = WuerstchenCombinedPipeline.from_pretrained("warp-ai/Wuerstchen", torch_dtype=torch.float16).to(
                ...     "cuda"
                ... )
                >>> prompt = "an image of a shiba inu, donning a spacesuit and helmet"
                >>> images = pipe(prompt=prompt)
                ```
        '''
    ),
    "src/diffusers/pipelines/wuerstchen/pipeline_wuerstchen_prior.py": (
        '''
        Examples:
                ```py
                >>> import torch
                >>> from diffusers import WuerstchenPriorPipeline
        
                >>> prior_pipe = WuerstchenPriorPipeline.from_pretrained(
                ...     "warp-ai/wuerstchen-prior", torch_dtype=torch.float16
                ... ).to("cuda")
        
                >>> prompt = "an image of a shiba inu, donning a spacesuit and helmet"
                >>> prior_output = pipe(prompt)
                ```
        '''
    ),
}
